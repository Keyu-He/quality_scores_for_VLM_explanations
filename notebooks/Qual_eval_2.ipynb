{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support / Contrastivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temp_analysis.xlsx\n",
    "# Then do the simulatability analysis\n",
    "# Load the xlsx sheet\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "file_path = 'temp_analysis.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Refactor image paths and ensure column types\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            \n",
    "    # Make Alternative Hypotheses a list, using ast to convert string to list\n",
    "    if 'Alternative Hypotheses' in df.columns:\n",
    "        df['Alternative Hypotheses'] = df['Alternative Hypotheses'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Print sample rows to verify\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "    \n",
    "# Extract a small sheet for testing\n",
    "mini_sheet = sheets_dict['LLaVA-1.5 with image'].head()\n",
    "demo_sheets_dict = {\n",
    "    \"mini_sheet\": mini_sheet,\n",
    "}\n",
    "mini_sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "if 'nli_tokenizer' not in globals():\n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "if 'nli_model' not in globals():\n",
    "    nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "def get_longest_rationale(rationale_list):\n",
    "    rationales = eval(rationale_list)\n",
    "    return max(rationales, key=len) if isinstance(rationales, list) else ''\n",
    "\n",
    "def calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            no_entail_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, no_entail_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[0]\n",
    "\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    predicted_answer = str(predicted_answer)\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if use_pieces:\n",
    "            premise = row['concat_rationale_pieces_mask'] if use_mask else row['concat_rationale_pieces']\n",
    "        else: \n",
    "            premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        \n",
    "        hypothesis = row[hypothesis_col]\n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        support = entail_prob > threshold \n",
    "        if entail_prob < threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {entail_prob}\")\n",
    "        support_scores.append({\n",
    "            'entail_prob': entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "    return support_scores\n",
    "\n",
    "def compute_file_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mask for each rationale\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    df['rationale_mask'] = df.apply(lambda x: generate_mask(x['generated_rationale'], x['predicted_answer']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strict_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', alt_hypotheses_col='alternative_hypotheses', threshold=0.5):\n",
    "    strict_supports = []\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['rationale_mask']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        alt_hypotheses = row[alt_hypotheses_col]\n",
    "        \n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        alt_entail_probs = []\n",
    "        for alt_hypothesis in alt_hypotheses:\n",
    "            alt_entail_probs.append(calc_support_prob(premise, alt_hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer))\n",
    "        \n",
    "        strict_support = entail_prob > threshold and all(alt_entail_prob < threshold for alt_entail_prob in alt_entail_probs)\n",
    "#         if strict_support == False:\n",
    "        print(f\"Premise: {premise}\")\n",
    "        print(f\"Hypothesis: {hypothesis}\")\n",
    "        print(f\"Probability: {entail_prob}\")\n",
    "        print(f\"Alternative hypotheses: {alt_hypotheses}\")\n",
    "        print(f\"Probability: {alt_entail_probs}\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        strict_supports.append(strict_support)\n",
    "        data.loc[idx, 'entail_prob'] = entail_prob\n",
    "        data.loc[idx, 'alt_ent_prob'] = str(alt_entail_probs)\n",
    "        data.loc[idx, 'support'] = entail_prob > threshold\n",
    "        data.loc[idx, 'strict_sim'] = strict_support\n",
    "    return\n",
    "    \n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    evaluate_strict_support(df, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='Hypothesis', alt_hypotheses_col='Alternative Hypotheses', threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the updated sheets here, as a temp, in case of any errors\n",
    "temp_file_path = 'temp_analysis_forked_v2.xlsx'\n",
    "with pd.ExcelWriter(temp_file_path) as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the temp file to verify\n",
    "sheets_dict_temp = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "for sheet_name, df in sheets_dict_temp.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
