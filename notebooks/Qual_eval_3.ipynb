{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file separates the dataset into multiple groups to calculate their average scores in different qualities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../results/rationales_analysis_scores.xlsx'\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "sheets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store correlation results\n",
    "correlations = {}\n",
    "\n",
    "# Iterate through each sheet\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Ensure the required columns exist\n",
    "    if 'is_correct' in df.columns and 'visual_fidelity' in df.columns:\n",
    "        # Compute the Pearson correlation\n",
    "        corr = df['is_correct'].corr(df['visual_fidelity'])\n",
    "        correlations[sheet_name] = corr\n",
    "        print(f\"Correlation for sheet {sheet_name}: {corr}\")\n",
    "    else:\n",
    "        # Handle cases where columns might be missing\n",
    "        correlations[sheet_name] = None\n",
    "        print(f\"Columns missing in sheet: {sheet_name}\")\n",
    "\n",
    "print(\"Mean correlation:\", sum(correlations.values()) / len(correlations))\n",
    "\n",
    "\n",
    "# List to hold DataFrames that have the required columns\n",
    "valid_dfs = []\n",
    "\n",
    "# Iterate through each sheet and collect valid DataFrames\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    if 'is_correct' in df.columns and 'visual_fidelity' in df.columns:\n",
    "        # Select only the relevant columns\n",
    "        valid_dfs.append(df[['is_correct', 'visual_fidelity']])\n",
    "    else:\n",
    "        print(f\"Skipping sheet '{sheet_name}' due to missing columns.\")\n",
    "\n",
    "# Check if there are valid DataFrames to concatenate\n",
    "if valid_dfs:\n",
    "    # Concatenate all valid DataFrames\n",
    "    combined_df = pd.concat(valid_dfs, ignore_index=True)\n",
    "    print(f\"Total records after concatenation: {combined_df.shape[0]}\")\n",
    "else:\n",
    "    print(\"No valid DataFrames found to concatenate.\")\n",
    "    combined_df = None\n",
    "    \n",
    "if combined_df is not None:\n",
    "    # Compute the Pearson correlation on the combined data\n",
    "    overall_corr = combined_df['is_correct'].corr(combined_df['visual_fidelity'])\n",
    "    print(f\"Overall correlation between 'is_correct' and 'Visual Fidelity' across all sheets: {overall_corr}\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust pandas settings\n",
    "pd.set_option('display.width', 1000)  # Increase display width\n",
    "pd.set_option('display.max_colwidth', None)  # Don't wrap text in columns\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    print(sheet_name)\n",
    "    \n",
    "    # Compute averages grouped by 'is_correct'\n",
    "    averages = df.groupby('is_correct')[\n",
    "        'support', \n",
    "        'informativeness', \n",
    "        'commonsense_plausibility', \n",
    "        'strict_sim', \n",
    "        'visual_fidelity'\n",
    "    ].mean()\n",
    "\n",
    "    # Count instances in each group\n",
    "    counts = df['is_correct'].value_counts()\n",
    "\n",
    "    # Print in the required order\n",
    "    print(\"Averages (grouped by is_correct):\")\n",
    "    print(averages.reindex([1, 0]))  # Order vertically by 1 (correct) and 0 (incorrect)\n",
    "    print(\"\\nCounts (grouped by is_correct):\")\n",
    "    print(counts.reindex([1, 0], fill_value=0))  # Ensure the order 1, 0 even if one group is missing\n",
    "    print('*' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to your Excel file\n",
    "file_path = '../results/rationales_analysis_scores.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary of DataFrames\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# 1. Compute Per-Sheet Correlations\n",
    "correlations = {}\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Compute the Pearson correlation between 'is_correct' and 'visual_fidelity'\n",
    "    corr = df['is_correct'].corr(df['visual_fidelity'])\n",
    "    correlations[sheet_name] = corr\n",
    "    print(f\"Correlation for sheet '{sheet_name}': {corr}\")\n",
    "\n",
    "# Calculate the mean correlation across all sheets\n",
    "mean_correlation = sum(correlations.values()) / len(correlations)\n",
    "print(f\"\\nMean correlation across all sheets: {mean_correlation:.2f}\")\n",
    "\n",
    "# 2. Concatenate All Data and Compute Overall Correlation\n",
    "# Select only the relevant columns and concatenate\n",
    "combined_df = pd.concat([df[['is_correct', 'visual_fidelity']] for df in sheets_dict.values()], ignore_index=True)\n",
    "print(f\"\\nTotal records after concatenation: {combined_df.shape[0]}\")\n",
    "\n",
    "# Compute the overall Pearson correlation\n",
    "overall_corr = combined_df['is_correct'].corr(combined_df['visual_fidelity'])\n",
    "print(f\"Overall correlation between 'is_correct' and 'Visual Fidelity' across all sheets: {overall_corr:.2f}\")\n",
    "\n",
    "# 3. Plotting the Correlation Heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(combined_df[['is_correct', 'visual_fidelity']].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 4. Analyzing Accurate Instances (is_correct == 1)\n",
    "# Filter rows where 'is_correct' is 1\n",
    "combined_accurate_df = combined_df[combined_df['is_correct'] == 1]\n",
    "print(f\"\\nTotal accurate records: {combined_accurate_df.shape[0]}\")\n",
    "\n",
    "if combined_accurate_df is not None:\n",
    "    # Compute the Pearson correlation on the combined data\n",
    "    overall_corr = combined_accurate_df['is_correct'].corr(combined_accurate_df['visual_fidelity'])\n",
    "    print(f\"Overall correlation between 'is_correct' and 'Visual Fidelity' across all sheets: {overall_corr}\")\n",
    "\n",
    "# **Note**: Computing correlation between 'is_correct' and 'visual_fidelity' within accurate instances is meaningless\n",
    "# because 'is_correct' is constant (always 1) in this subset.\n",
    "\n",
    "# Instead, let's analyze the distribution of 'visual_fidelity' among accurate instances\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='visual_fidelity', data=combined_accurate_df)\n",
    "plt.title('Distribution of Visual Fidelity for Accurate Instances')\n",
    "plt.xlabel('Visual Fidelity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_accurate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name, df in sheets_dict.items():\n",
    "    print(sheet_name)\n",
    "\n",
    "    # Compute averages grouped by 'is_correct'\n",
    "    averages = df.groupby('is_correct')[\n",
    "        'support', \n",
    "        'informativeness', \n",
    "        'commonsense_plausibility', \n",
    "        'strict_sim', \n",
    "        'visual_fidelity'\n",
    "    ].mean()\n",
    "\n",
    "    # Calculate percentage drops from is_correct=1 to is_correct=0\n",
    "    drop_percentages = (averages.loc[1] - averages.loc[0]) / averages.loc[1] * 100\n",
    "\n",
    "    # Highlight the last two columns\n",
    "    significant_drops = drop_percentages[['strict_sim', 'visual_fidelity']]\n",
    "    other_drops = drop_percentages.drop(['strict_sim', 'visual_fidelity'])\n",
    "\n",
    "    print(\"Drop Percentages:\")\n",
    "    print(drop_percentages)\n",
    "    print(\"\\nSignificant Drops (strict_sim and visual_fidelity):\")\n",
    "    print(significant_drops)\n",
    "    print(\"\\nOther Metric Drops:\")\n",
    "    print(other_drops)\n",
    "    print('*' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the Excel file with all sheets\n",
    "file_path = '../results/rationales_analysis_full_details.xlsx'\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "def parse_column(column):\n",
    "    \"\"\"\n",
    "    Parse the string representations of lists into actual lists.\n",
    "    \"\"\"\n",
    "    return column.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "def convert_yes_no(series):\n",
    "    \"\"\"\n",
    "    Convert 'Yes' to 1 and 'No' to 0 in each sublist.\n",
    "    \"\"\"\n",
    "    def convert_list(lst):\n",
    "        return [1 if isinstance(item, str) and item.strip().lower() == 'yes' \n",
    "                else 0 if isinstance(item, str) and item.strip().lower() == 'no' \n",
    "                else None for item in lst]\n",
    "    \n",
    "    return series.apply(convert_list)\n",
    "\n",
    "def calculate_micro_average(series):\n",
    "    \"\"\"\n",
    "    Calculate the micro average: sum of all elements divided by the total number of elements.\n",
    "    \"\"\"\n",
    "    flattened = [item for sublist in series for item in sublist if item is not None]\n",
    "    return sum(flattened) / len(flattened) if flattened else 0\n",
    "\n",
    "def calculate_macro_average(series):\n",
    "    \"\"\"\n",
    "    Calculate the macro average: average of the mean of each sublist.\n",
    "    \"\"\"\n",
    "    means = [sum(sublist) / len(sublist) if sublist else 0 for sublist in series]\n",
    "    return sum(means) / len(means) if means else 0\n",
    "\n",
    "summary = []\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    if 'vf_answers_GPT_converted' not in df.columns:\n",
    "        print(f\"Sheet '{sheet_name}' does not contain 'vf_answers_GPT_converted' column.\")\n",
    "        continue\n",
    "    \n",
    "    if 'is_correct' not in df.columns:\n",
    "        print(f\"Sheet '{sheet_name}' does not contain 'is_correct' column. Skipping additional grouping analysis.\")\n",
    "        # Optionally, you can choose to process the sheet without grouping\n",
    "        # For now, we'll skip additional grouping if 'is_correct' is missing\n",
    "        process_grouping = False\n",
    "    else:\n",
    "        process_grouping = True\n",
    "\n",
    "    # Parse the column\n",
    "    df['parsed_scores'] = parse_column(df['vf_answers_GPT_converted'])\n",
    "    \n",
    "    # Ensure that all parsed entries are lists\n",
    "    if not df['parsed_scores'].apply(lambda x: isinstance(x, list)).all():\n",
    "        print(f\"Sheet '{sheet_name}' has non-list entries in 'vf_answers_GPT_converted'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert 'Yes'/'No' to 1/0\n",
    "    df['numeric_scores'] = convert_yes_no(df['parsed_scores'])\n",
    "    \n",
    "    # Check for None values indicating unexpected entries\n",
    "    if df['numeric_scores'].apply(lambda x: any(item is None for item in x)).any():\n",
    "        print(f\"Sheet '{sheet_name}' contains entries other than 'Yes'/'No'. These will be treated as 0.\")\n",
    "        # Optionally, you can handle these cases differently\n",
    "        df['numeric_scores'] = df['numeric_scores'].apply(lambda lst: [item if item is not None else 0 for item in lst])\n",
    "    \n",
    "    # Calculate overall micro and macro averages\n",
    "    micro_avg = calculate_micro_average(df['numeric_scores'])\n",
    "    macro_avg = calculate_macro_average(df['numeric_scores'])\n",
    "    \n",
    "    # Initialize summary entry for the sheet\n",
    "    summary_entry = {\n",
    "        'Sheet': sheet_name,\n",
    "        'Group': 'All',\n",
    "        'Micro Average': micro_avg,\n",
    "        'Macro Average': macro_avg\n",
    "    }\n",
    "    summary.append(summary_entry)\n",
    "    \n",
    "    # If grouping by 'is_correct' is applicable\n",
    "    if process_grouping:\n",
    "        grouped = df.groupby('is_correct')\n",
    "        for group_value, group_df in grouped:\n",
    "            group_label = 'Correct' if group_value == 1 else 'Incorrect'\n",
    "            \n",
    "            # Calculate micro and macro averages within the group\n",
    "            group_micro_avg = calculate_micro_average(group_df['numeric_scores'])\n",
    "            group_macro_avg = calculate_macro_average(group_df['numeric_scores'])\n",
    "            \n",
    "            # Append to summary\n",
    "            summary.append({\n",
    "                'Sheet': sheet_name,\n",
    "                'Group': group_label,\n",
    "                'Micro Average': group_micro_avg,\n",
    "                'Macro Average': group_macro_avg\n",
    "            })\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Optional: Order the summary for better readability\n",
    "summary_df = summary_df.sort_values(by=['Sheet', 'Group']).reset_index(drop=True)\n",
    "\n",
    "# Display the summary\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
