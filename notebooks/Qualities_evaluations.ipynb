{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we analyze the rationales in text-only metrics and our proposed text / vision metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the xlsx sheet\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "file_path = '../results/rationales.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Refactor image paths and ensure column types\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Update image paths\n",
    "    df['image_path'] = df['image_path'].str.replace('./images/', './results/img/validation/')\n",
    "    \n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Print sample rows to verify\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "\n",
    "# Extract a small sheet for testing\n",
    "mini_sheet = sheets_dict['LLaVA-1.5 with image'].head()\n",
    "mini_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Informativeness, Simulatability, Contrastivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Hypothesis, Alternative Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "import requests\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the OpenAI API key, read from ../OPENAI_key.txt first line\n",
    "with open('../OPENAI_key.txt', 'r') as file:\n",
    "    api_key = file.readline().strip()\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"../total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "\n",
    "def calculate_cost(usage, model, verbose=0):\n",
    "    if model == \"gpt-4o-2024-05-13\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    elif model == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_token = 0.0025 / 1000\n",
    "        output_cost_per_token = 0.010 / 1000\n",
    "    elif model == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_token = 0.00015 / 1000\n",
    "        output_cost_per_token = 0.00060 / 1000\n",
    "\n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)\n",
    "\n",
    "def gpt_gen_hypothesis(question, predicted_ans, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-mini-2024-07-18\"   \n",
    "    user_input = f\"\"\"Integrate the question and the answer into one sentence.\n",
    "For example, given the question \"What is the man waiting for?\" and the answer \"taxi\", you should output \"The man is waiting for taxi.\"\n",
    "\n",
    "Question: {question}\n",
    "Answer: {predicted_ans}\n",
    "\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "        return content\n",
    "    \n",
    "def generate_alternative_hypotheses(question, other_answers):\n",
    "    # Generate hypotheses for each alternative answer\n",
    "    return [gpt_gen_hypothesis(question, answer) for answer in other_answers]\n",
    "\n",
    "def extract_distinct_rationale_pieces(hypothesis, rationale):\n",
    "    prompt=f\"\"\"Please break the following rationale into distinct pieces, and keep only the ones that are not semantically equivalent to the hypothesis. Output the final answer in a Python list format.\n",
    "\n",
    "Example:\n",
    "Hypothesis: The man by the bags is waiting for a delivery.\n",
    "Rationale: The man by the bags is waiting for a delivery, as indicated by the presence of the suitcases and the fact that he is standing on the side of the road. The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\n",
    "Output: [\"Suitcases are present in the image.\", \"The man is standing on the side of the road.\", \"The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\"]\n",
    "\n",
    "Task:\n",
    "Hypothesis: {hypothesis}\n",
    "Rationale: {rationale}\"\"\"\n",
    "    \n",
    "    model_name = \"gpt-4o-2024-08-06\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    calculate_cost(response['usage'], model_name)\n",
    "    message = response.choices[0].message.content\n",
    "    start_index = message.find('[')\n",
    "    end_index = message.rfind(']')\n",
    "    R_list = message[start_index:end_index+1]\n",
    "    R_list = ast.literal_eval(R_list)\n",
    "    return R_list\n",
    "\n",
    "def process_sheet(sheet_df):\n",
    "    # Initialize columns\n",
    "    hypotheses = []\n",
    "    alternative_hypotheses = []\n",
    "    all_answers_list = []\n",
    "    other_answers_list = []\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for _, row in tqdm(sheet_df.iterrows(), total=sheet_df.shape[0], desc=\"Processing Rows\"):\n",
    "        # Generate hypothesis\n",
    "        hypothesis = gpt_gen_hypothesis(row['question'], row['predicted_answer'])\n",
    "        hypotheses.append(hypothesis)\n",
    "\n",
    "        # Extract all answers\n",
    "        all_answers = row['question'].split(\"Choices: \")[-1].split(\", \") if \"Choices: \" in row['question'] else []\n",
    "        all_answers_list.append(all_answers)\n",
    "\n",
    "        # Filter out the predicted answer to get other answers\n",
    "        other_answers = [ans for ans in all_answers if ans.lower() != row['predicted_answer'].lower()]\n",
    "        other_answers_list.append(other_answers)\n",
    "\n",
    "        # Generate alternative hypotheses\n",
    "        alt_hypotheses = [gpt_gen_hypothesis(row['question'], ans) for ans in other_answers]\n",
    "        alternative_hypotheses.append(alt_hypotheses)\n",
    "\n",
    "    # Assign new columns\n",
    "    sheet_df.loc[:,'Hypothesis'] = hypotheses\n",
    "    sheet_df.loc[:,'Alternative Hypotheses'] = alternative_hypotheses\n",
    "\n",
    "    return sheet_df\n",
    "\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    print(f\"Processing sheet: {sheet_name}\")\n",
    "    process_sheet(df)\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the updated sheets here, as a temp, in case of any errors\n",
    "temp_file_path = 'temp_analysis.xlsx'\n",
    "with pd.ExcelWriter(temp_file_path) as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the temp file to verify\n",
    "sheets_dict_temp = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "for sheet_name, df in sheets_dict_temp.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Simulatability and Contrastivity (done in Qual_eval_2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the xlsx sheet\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load temp_analysis_v3.xlsx\n",
    "file_path = 'temp_analysis_v3.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Refactor image paths and ensure column types\n",
    "for sheet_name, df in sheets_dict.items():    \n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Print sample rows to verify\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "\n",
    "# Extract a small sheet for testing\n",
    "mini_sheet = sheets_dict['LLaVA-1.5 with image'].head()\n",
    "demo_sheets_dict = {\n",
    "    'mini_sheet': mini_sheet\n",
    "}\n",
    "mini_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the OpenAI API key, read from ../OPENAI_key.txt first line\n",
    "with open('../OPENAI_key.txt', 'r') as file:\n",
    "    api_key = file.readline().strip()\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"../total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "\n",
    "def calculate_cost(usage, model, verbose=0):\n",
    "    if model == \"gpt-4o-2024-05-13\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    elif model == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_token = 0.0025 / 1000\n",
    "        output_cost_per_token = 0.010 / 1000\n",
    "    elif model == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_token = 0.00015 / 1000\n",
    "        output_cost_per_token = 0.00060 / 1000\n",
    "\n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import ast\n",
    "\n",
    "client = openai.Client(api_key=api_key)\n",
    "\n",
    "# Generate set R which contains rationale pieces for every instance\n",
    "\n",
    "def extract_distinct_rationale_pieces(hypothesis, rationale, max_retries=5):\n",
    "    prompt=f\"\"\"Please break the following rationale into distinct pieces, and keep only the ones that are not semantically equivalent to the hypothesis. Output the final answer in a Python list format.\n",
    "\n",
    "Example:\n",
    "Hypothesis: The man by the bags is waiting for a delivery.\n",
    "Rationale: The man by the bags is waiting for a delivery, as indicated by the presence of the suitcases and the fact that he is standing on the side of the road. The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\n",
    "Output: [\"Suitcases are present in the image.\", \"The man is standing on the side of the road.\", \"The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\"]\n",
    "\n",
    "Task:\n",
    "Hypothesis: {hypothesis}\n",
    "Rationale: {rationale}\"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-2024-08-06\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            # usage = response.usage\n",
    "            # calculate_cost(usage, \"gpt-4o-2024-08-06\")\n",
    "            message = response.choices[0].message.content\n",
    "            start_index = message.find('[')\n",
    "            end_index = message.rfind(']')\n",
    "            R_list_str = message[start_index:end_index+1]\n",
    "            R_list = ast.literal_eval(R_list_str)\n",
    "            return R_list\n",
    "        \n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            print(f\"Attempt {retries + 1} failed with error: {e}\")\n",
    "            retries += 1\n",
    "    \n",
    "    # If all attempts fail, return an empty list or handle it as needed\n",
    "    print(\"All attempts failed. Returning an empty list.\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress the specific SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "for sheet_name, df in tqdm(sheets_dict.items(), desc=\"Processing Sheets\"):\n",
    "    # Apply the function to each row and store the result in a new column\n",
    "    df.loc[:,'extracted_rationale_pieces'] = df.apply(\n",
    "        lambda row: extract_distinct_rationale_pieces(row['Hypothesis'], row['generated_rationale']),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the updated sheets here, as a temp, in case of any errors\n",
    "temp_file_path = 'temp_analysis_v4.xlsx'\n",
    "with pd.ExcelWriter(temp_file_path) as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the temp file to verify\n",
    "sheets_dict_temp = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "for sheet_name, df in sheets_dict_temp.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Commonsense Plausibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Global variables for model components\n",
    "_vera_model = None\n",
    "_vera_tokenizer = None\n",
    "_vera_linear = None\n",
    "_vera_temperature = None\n",
    "\n",
    "def get_vera_score(statements):\n",
    "    \"\"\"\n",
    "    Get plausibility scores for statements using the VERA model. Loads the model only once.\n",
    "\n",
    "    Parameters:\n",
    "    - statements (str or list of str): A single statement or a list of statements to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - scores (list of float): Calibrated plausibility scores for each input statement.\n",
    "    \"\"\"\n",
    "    global _vera_model, _vera_tokenizer, _vera_linear, _vera_temperature\n",
    "\n",
    "    # Lazy loading of the model\n",
    "    if _vera_model is None:\n",
    "        print(\"Loading VERA model...\")\n",
    "        model_name = 'liujch1998/vera'\n",
    "        _vera_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        _vera_model = transformers.T5EncoderModel.from_pretrained(model_name).to(device)\n",
    "        _vera_model.D = _vera_model.shared.embedding_dim\n",
    "\n",
    "        # Define the linear layer\n",
    "        _vera_linear = torch.nn.Linear(_vera_model.D, 1, dtype=_vera_model.dtype)\n",
    "        _vera_linear.weight = torch.nn.Parameter(_vera_model.shared.weight[32099, :].unsqueeze(0))\n",
    "        _vera_linear.bias = torch.nn.Parameter(_vera_model.shared.weight[32098, 0].unsqueeze(0))\n",
    "        _vera_model.eval()\n",
    "\n",
    "        # Get temperature for calibration\n",
    "        _vera_temperature = _vera_model.shared.weight[32097, 0].item()\n",
    "\n",
    "    # Ensure input is a list\n",
    "    if isinstance(statements, str):\n",
    "        statements = [statements]\n",
    "\n",
    "    # Tokenize the input and move to device\n",
    "    inputs = _vera_tokenizer.batch_encode_plus(\n",
    "        statements,\n",
    "        return_tensors='pt',\n",
    "        padding='longest',\n",
    "        truncation='longest_first',\n",
    "        max_length=128\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = _vera_model(input_ids)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        hidden = last_hidden_state[:, -1, :]  # Extract hidden state for the last token\n",
    "        logits = _vera_linear(hidden).squeeze(-1)  # Calculate logits\n",
    "        logits_calibrated = logits / _vera_temperature  # Apply temperature calibration\n",
    "        scores_calibrated = logits_calibrated.sigmoid()  # Convert to probabilities\n",
    "\n",
    "    # Return scores as a list\n",
    "    return scores_calibrated.tolist()\n",
    "\n",
    "\n",
    "statements = [\n",
    "    \"Water freezes at 0 degrees Celsius under normal atmospheric pressure.\",\n",
    "    \"The sun rises in the west.\",\n",
    "    \"The sun rises in the east.\",\n",
    "    \"Since the density of a marble is much less than the density of mercury, the marble would sink to the bottom of the bowl if placed in it.\",\n",
    "    \"Since the density of a marble is much more than the density of water, the marble would sink to the bottom of the bowl if placed in it.\",\n",
    "    \"Since the density of water is much less than the density of a marble, the marble would sink to the bottom of the bowl if placed in it.\"\n",
    "]\n",
    "\n",
    "# Get plausibility scores\n",
    "scores = get_vera_score(statements)\n",
    "\n",
    "# Print the results\n",
    "for statement, score in zip(statements, scores):\n",
    "    print(f\"Statement: {statement}\")\n",
    "    print(f\"Plausibility score: {score}\")\n",
    "\n",
    "\n",
    "file_path = 'results.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in tqdm(sheets_dict.items(), desc='Processing sheets'):\n",
    "    # Update image paths\n",
    "    df['image_path'] = df['image_path'].str.replace('./images/', '../results/img/validation/', regex=False)\n",
    "    \n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Get the generated rationales\n",
    "    if 'generated_rationale' in df.columns:\n",
    "        statements = df['generated_rationale'].to_list()\n",
    "        \n",
    "        # Get plausibility scores\n",
    "        scores = get_vera_score(statements)\n",
    "        \n",
    "        # Add the scores as a new column\n",
    "        df['commonsense_plausibility_score'] = scores\n",
    "    else:\n",
    "        # Handle cases where the column might be missing\n",
    "        df['commonsense_plausibility_score'] = np.nan\n",
    "    \n",
    "    # Update the sheet in the dictionary\n",
    "    sheets_dict[sheet_name] = df\n",
    "\n",
    "# # Save the updated sheets to a new Excel file\n",
    "# output_file_path = 'results_cp.xlsx'\n",
    "# with pd.ExcelWriter(output_file_path) as writer:\n",
    "#     for sheet_name, df in sheets_dict.items():\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(f\"Updated Excel file saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visual Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_gen_vf_questions(row, rationale_column_name, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-2024-08-06\"\n",
    "    # # Read the image and convert it to base64 format\n",
    "    # with open(row['image_path'], \"rb\") as image_file:\n",
    "    #     encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    system_prompt = f\"\"\"You will be shown a question about an image, along with an answer, and a rationale that explains the answer based on details from the image. Your task is to generate a list of yes/no questions that verify the details about the image that are **explicitly** mentioned in the rationale. Your questions should be phrased such that the answer to that question being yes means that the detail in the rationale is correct. Focus on creating questions that can be visually verified or refuted based on the details provided in the rationale. Ensure the questions are specific and directly pertain to aspects that are visually relevant and mentioned in the rationale. Avoid generating questions about elements that are not mentioned in the rationale, or the rationale explicitly states are not relevant or present. Also avoid generating multiple questions that check for the same visual detail.\n",
    "\n",
    "Here is one example:\n",
    "Input: \n",
    "Question: Why is the person wearing a helmet?\n",
    "Answer: For safety\n",
    "Rationale: The person is wearing a helmet because they are riding a bicycle on a busy city street. Helmets are commonly used to protect against head injuries in case of accidents, especially in areas with heavy traffic.\n",
    "\n",
    "Good Questions:\n",
    "1. Is the person wearing a helmet while riding a bicycle?\n",
    "Reason: This question is directly answerable by observing whether the person on the bicycle is wearing a helmet in the image. \n",
    "2. Is the street in the image busy with traffic?\n",
    "Reason: This question can be visually verified by looking at the amount of traffic on the street in the image.\n",
    "\n",
    "Bad Questions:\n",
    "1. Is the person wearing the helmet because they are concerned about head injuries?\n",
    "Reason: This question is not good because it assumes the person’s intentions or concerns, which cannot be visually verified from the image.\n",
    "2. Does wearing a helmet suggest that the person is highly safety-conscious?\n",
    "Reason: This question relies on inference and external knowledge about the person’s mindset, rather than on observable details from the image.\n",
    "3. Is there any indication that the person is wearing a helmet for safety reasons?\n",
    "Reason: This question verifies the answer to the original question, rather than verifying a detail about the image that's mentioned in the rationale.\n",
    "4. Is the person wearing a safety vest?\n",
    "Reason: This question is not good because it tries to verify details about the image that are not explicitly mentioned in the rationale.\n",
    "5. Is the person not wearing sunglasses?\n",
    "Reason: This question is not good because it asks for verification by absence and can only be answered with a \"no,\" which is not the preferred type of question.\n",
    "\n",
    "Respond with a list of (good) questions (without the reasons), starting from '1. '\"\"\"\n",
    "    \n",
    "\n",
    "    user_input = f\"\"\"Question: {row['question']}\n",
    "Answer: {row['predicted_answer']}\n",
    "Rationale: {row[rationale_column_name]}\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # content is a list of questions, separated by a newline character\n",
    "        # 1. ... \\n 2. ... \\n 3. ...\n",
    "\n",
    "        try:\n",
    "            # Split the content into individual questions and return a python list\n",
    "            if '\\n' in content:\n",
    "                parts = content.split('\\n')\n",
    "            else:\n",
    "                # parts = content.split('. ')  # Split by \". \" as a fallback\n",
    "                parts = [content]\n",
    "            questions = []\n",
    "\n",
    "            for part in parts:\n",
    "                try:\n",
    "                    # Attempt to split and take the second part\n",
    "                    question = part.split('. ')[1]\n",
    "                    questions.append(question)\n",
    "                except IndexError:\n",
    "                    # If there's an issue with splitting, add the entire part or handle as needed\n",
    "                    questions.append(part)\n",
    "        except Exception as e:\n",
    "            questions = [content]\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        return questions\n",
    "    \n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in tqdm(sheets_dict.items(), desc='Processing sheets'):\n",
    "    vf_questions = []\n",
    "    # Initialize the 'vf_questions' column if it doesn't exist\n",
    "    # if 'vf_questions' not in df.columns:\n",
    "    #     df['vf_questions'] = None  # Or use an empty list []\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\"):\n",
    "        # Generate visual verification questions\n",
    "        questions = gpt_gen_vf_questions(row, 'generated_rationale')\n",
    "        # store the questions as a string (but appear like a list)\n",
    "        questions = str(questions)\n",
    "        vf_questions.append(questions)\n",
    "        \n",
    "    # Assign new columns\n",
    "    df.loc[:,'vf_questions'] = vf_questions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the updated sheets here, as a temp, in case of any errors\n",
    "temp_file_path = 'temp_analysis_v2.xlsx'\n",
    "with pd.ExcelWriter(temp_file_path) as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the temp file to verify\n",
    "sheets_dict_temp = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "for sheet_name, df in sheets_dict_temp.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read temp_analysis_v2.xlsx\n",
    "\n",
    "# Load the xlsx sheet\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ast\n",
    "\n",
    "file_path = 'temp_analysis_v2.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Refactor image paths and ensure column types\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            \n",
    "    # Ensure vf_questions is a list\n",
    "    if 'vf_questions' in df.columns:\n",
    "        df['vf_questions'] = df['vf_questions'].apply(ast.literal_eval)\n",
    "    \n",
    "    print(type(df['vf_questions'][0]))\n",
    "    \n",
    "    # Print sample rows to verify\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "\n",
    "# Extract a small sheet for testing\n",
    "mini_sheet = sheets_dict['LLaVA-1.5 with image'].head()\n",
    "demo_sheets_dict = {\n",
    "    \"mini_sheet\": mini_sheet\n",
    "}\n",
    "mini_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "import requests\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the OpenAI API key, read from ../OPENAI_key.txt first line\n",
    "with open('../OPENAI_key.txt', 'r') as file:\n",
    "    api_key = file.readline().strip()\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"../total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "\n",
    "def calculate_cost(usage, model, verbose=0):\n",
    "    if model == \"gpt-4o-2024-05-13\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    elif model == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_token = 0.0025 / 1000\n",
    "        output_cost_per_token = 0.010 / 1000\n",
    "    elif model == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_token = 0.00015 / 1000\n",
    "        output_cost_per_token = 0.00060 / 1000\n",
    "\n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_answer_vf_questions(question, image_path, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-2024-08-06\"\n",
    "    # Read the image and convert it to base64 format\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    user_input = f\"\"\"Question: {question}. Based on the information provided in the image, answer with 'yes' or 'no'. Provide one-word answer only.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\n",
    "            #             \"type\": \"text\",\n",
    "            #             \"text\": system_prompt\n",
    "            #         }\n",
    "            #     ]\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        content = response.json()['choices'][0]['message']['content'].strip().strip('.')\n",
    "        return content\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def answer_vf_questions(data):\n",
    "    if 'vf_answers_GPT' not in data.columns:\n",
    "        data['vf_answers_GPT'] = None\n",
    "    # if 'vf_answers_GPT_GPT_r' not in data.columns:\n",
    "    #     data['vf_answers_GPT_GPT_r'] = None\n",
    "    data['vf_answers_GPT'] = data['vf_answers_GPT'].astype(object)\n",
    "    for idx, row in data.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        answer_list = []\n",
    "        # Image\n",
    "#         display(image)\n",
    "        for i, question in enumerate(row['vf_questions']):\n",
    "            answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "            answer_list.append(answer)\n",
    "            # Print image, question, and answer\n",
    "            print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        data.at[idx, 'vf_answers_GPT'] = answer_list\n",
    "        # for i, question in enumerate(row['vf_questions_GPT_r']):\n",
    "        #     answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "        #     answer_list.append(answer)\n",
    "        #     # Print image, question, and answer\n",
    "        #     print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        # data.at[idx, 'vf_answers_GPT_GPT_r'] = answer_list\n",
    "    return data\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    print(f\"Processing sheet: {sheet_name}\")\n",
    "    df = answer_vf_questions(df)\n",
    "    print(df.head())\n",
    "    print()\n",
    "\n",
    "# Store the updated sheets here, as a temp, in case of any errors\n",
    "temp_file_path = 'temp_analysis_v3.xlsx'\n",
    "with pd.ExcelWriter(temp_file_path) as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Load the temp file to verify\n",
    "sheets_dict_temp = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "for sheet_name, df in sheets_dict_temp.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel files into dictionaries of DataFrames\n",
    "file_path1 = 'temp_analysis_v4.xlsx'\n",
    "file_path2 = 'temp_analysis_forked_v2.xlsx'\n",
    "file_path3 = 'temp_analysis_forked2_cp.xlsx'\n",
    "\n",
    "sheets_dict1 = pd.read_excel(file_path1, sheet_name=None)\n",
    "sheets_dict2 = pd.read_excel(file_path2, sheet_name=None)\n",
    "sheets_dict3 = pd.read_excel(file_path3, sheet_name=None)\n",
    "\n",
    "# Combine the sheets, appending unique columns from forked_v2 to v4\n",
    "combined_sheets = {}\n",
    "\n",
    "for sheet_name, df1 in sheets_dict1.items():\n",
    "    if sheet_name in sheets_dict2:\n",
    "        df2 = sheets_dict2[sheet_name]\n",
    "        \n",
    "        # Find unique columns in df2 not present in df1\n",
    "        unique_cols = df2.columns.difference(df1.columns)\n",
    "        \n",
    "        # Append these unique columns to df1\n",
    "        df_combined = pd.concat([df1, df2[unique_cols]], axis=1)\n",
    "    else:\n",
    "        # If the sheet is not in the second file, keep the original\n",
    "        df_combined = df1\n",
    "        \n",
    "    if sheet_name in sheets_dict3:\n",
    "        df3 = sheets_dict3[sheet_name]\n",
    "        \n",
    "        # Find unique columns in df3 not present in df_combined\n",
    "        unique_cols = df3.columns.difference(df_combined.columns)\n",
    "        \n",
    "        # Append these unique columns to df_combined\n",
    "        df_combined = pd.concat([df_combined, df3[unique_cols]], axis=1)\n",
    "    \n",
    "    combined_sheets[sheet_name] = df_combined\n",
    "\n",
    "# Save the combined sheets to a new Excel file\n",
    "output_file_path = 'temp_analysis_combined.xlsx'\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in combined_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read temp_analyis_combined.xlsx\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "file_path = 'temp_analysis_combined.xlsx'\n",
    "\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# We process by converting every cell in the column to a list of strings (using ast.literal_eval)\n",
    "# And we convery every answer other than 'Yes' or 'No' to 'No'.\n",
    "# Add a column called vf_answers_GPT_processed\n",
    "# Then, add a column Visual_Fidelity, which checks if cell in column \"vf_answers_GPT\" contains 'No', outputs 0 if it does, and 1 otherwise.\n",
    "\n",
    "# Add a column called informativeness, which is a binary value.\n",
    "# It is 1 if extracted_rationale_pieces is not \"[]\" and 0 otherwise.\n",
    "\n",
    "# For columns strict_sim and support, convert them from TRUE FALSE to 1 and 0 respectively.\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Convert every cell in the column \"vf_answers_GPT\" to a list of strings\n",
    "    if \"vf_answers_GPT\" in df.columns:\n",
    "        df[\"vf_answers_GPT_converted\"] = df[\"vf_answers_GPT\"].apply(\n",
    "            lambda x: str([\"No\" if ans not in [\"Yes\", \"No\"] else ans for ans in ast.literal_eval(x)])\n",
    "            if isinstance(x, str) else x\n",
    "        )\n",
    "    \n",
    "    # Add a column \"visual_fidelity\"\n",
    "    if \"vf_answers_GPT\" in df.columns:\n",
    "        df[\"visual_fidelity\"] = df[\"vf_answers_GPT_converted\"].apply(\n",
    "            lambda x: 0 if isinstance(x, str) and \"No\" in ast.literal_eval(x) else 1\n",
    "        )\n",
    "    \n",
    "    # Add a column \"informativeness\"\n",
    "    if \"extracted_rationale_pieces\" in df.columns:\n",
    "        df[\"informativeness\"] = df[\"extracted_rationale_pieces\"].apply(\n",
    "            lambda x: 1 if x != \"[]\" else 0\n",
    "        )\n",
    "    \n",
    "    # Convert \"strict_sim\" from TRUE/FALSE to 1/0\n",
    "    if \"strict_sim\" in df.columns:\n",
    "        df[\"strict_sim\"] = df[\"strict_sim\"].apply(lambda x: 1 if x is True else 0)\n",
    "    \n",
    "    # Convert \"support\" from TRUE/FALSE to 1/0\n",
    "    if \"support\" in df.columns:\n",
    "        df[\"support\"] = df[\"support\"].apply(lambda x: 1 if x is True else 0)\n",
    "        \n",
    "    if \"commonsense_plausibility_score\" in df.columns:\n",
    "        # Rename to commonsense_plausibility\n",
    "        df.rename(columns={\"commonsense_plausibility_score\": \"commonsense_plausibility\"}, inplace=True)\n",
    "\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "\n",
    "def add_images_to_excel(file_path, sheets_dict, column='V'):\n",
    "    workbook = load_workbook(file_path)\n",
    "    \n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        if \"image_path\" in df.columns:\n",
    "            worksheet = workbook[sheet_name]\n",
    "            \n",
    "            # Iterate over the rows in the DataFrame\n",
    "            for index, row in df.iterrows():\n",
    "                image_path = row.get(\"image_path\")\n",
    "                \n",
    "                # Check if the image_path exists and is valid\n",
    "                if isinstance(image_path, str) and os.path.exists(image_path):\n",
    "                    # Add the image to the corresponding row\n",
    "                    img = Image(image_path)\n",
    "                    cell_address = f\"{column}{index + 2}\"  # Adjust row index to match Excel's 1-based indexing\n",
    "                    worksheet.add_image(img, cell_address)\n",
    "    \n",
    "    # Save the workbook with images added\n",
    "    workbook.save(file_path)\n",
    "    print(f\"Images added and saved to {file_path}\")\n",
    "\n",
    "# Save full details Excel file\n",
    "output_file_path1 = '../results/rationales_analysis_full_details.xlsx'\n",
    "with pd.ExcelWriter(output_file_path1, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "print(f\"Processed data saved to {output_file_path1}\")\n",
    "\n",
    "# Add images to the full details Excel file\n",
    "add_images_to_excel(output_file_path1, sheets_dict, column='V')\n",
    "\n",
    "# Save scores-only Excel file\n",
    "output_file_path2 = '../results/rationales_analysis_scores.xlsx'\n",
    "sheets_score_only = {\n",
    "    sheet_name: df[['question', 'predicted_answer', 'correct_answer', 'is_correct', \n",
    "                    'generated_rationale', 'image_path', 'visual_fidelity', \n",
    "                    'informativeness', 'strict_sim', 'support', 'commonsense_plausibility']]\n",
    "    for sheet_name, df in sheets_dict.items()\n",
    "}\n",
    "with pd.ExcelWriter(output_file_path2, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in sheets_score_only.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "print(f\"Processed data saved to {output_file_path2}\")\n",
    "\n",
    "# Add images to the scores-only Excel file\n",
    "add_images_to_excel(output_file_path2, sheets_score_only, column='M')\n",
    "\n",
    "\n",
    "# # Save the processed data to a new Excel file\n",
    "# # Add the images back, using the column image_path\n",
    "# output_file_path1 = '../results/rationales_analysis_full_details.xlsx'\n",
    "# with pd.ExcelWriter(output_file_path1, engine='openpyxl') as writer:\n",
    "#     for sheet_name, df in sheets_dict.items():\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "# print(f\"Processed data saved to {output_file_path1}\")\n",
    "\n",
    "# output_file_path2 = '../results/rationales_analysis_scores.xlsx'\n",
    "# sheets_score_only = {sheet_name: df[['question', 'predicted_answer', 'correct_answer', 'is_correct', \\\n",
    "#     'generated_rationale', 'image_path', 'visual_fidelity', 'informativeness', 'strict_sim', 'support', \\\n",
    "#     'commonsense_plausibility']] for sheet_name, df in sheets_dict.items()}\n",
    "# with pd.ExcelWriter(output_file_path2, engine='openpyxl') as writer:\n",
    "#     for sheet_name, df in sheets_score_only.items():\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "# print(f\"Processed data saved to {output_file_path2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate non-binary scores for visual fidelity and contrastiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ../results/rationales_analysis_full_details.xlsx\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "file_path = '../results/rationales_analysis_full_details.xlsx'\n",
    "\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    for col in ['vf_questions', 'vf_answers_GPT_converted', 'alt_ent_prob']:\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "    # Check the percentage of 'Yes' in the column vf_answers_GPT_converted\n",
    "    df['visual_fidelity_converted_score'] = df['vf_answers_GPT_converted'].apply(lambda x: x.count('Yes') / len(x))\n",
    "    # Check p(entail_prob) / (p(entail_prob) + sum(p(alt_ent_prob)))\n",
    "    df['contrastiveness_converted_score'] = df.apply(lambda row: row['entail_prob'] / (row['entail_prob'] + sum(row['alt_ent_prob'])), axis=1)\n",
    "    df['question_id'] = df['image_path'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "sheets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the sheets back\n",
    "# output_file_path = '../results/rationales_analysis_full_details_v2.xlsx'\n",
    "# with pd.ExcelWriter(output_file_path) as writer:\n",
    "#     for sheet_name, df in sheets_dict.items():\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "# from openpyxl import load_workbook\n",
    "# from openpyxl.drawing.image import Image\n",
    "\n",
    "# def add_images_to_excel(file_path, sheets_dict, column='V'):\n",
    "#     workbook = load_workbook(file_path)\n",
    "    \n",
    "#     for sheet_name, df in sheets_dict.items():\n",
    "#         if \"image_path\" in df.columns:\n",
    "#             worksheet = workbook[sheet_name]\n",
    "            \n",
    "#             # Iterate over the rows in the DataFrame\n",
    "#             for index, row in df.iterrows():\n",
    "#                 image_path = row.get(\"image_path\")\n",
    "                \n",
    "#                 # Check if the image_path exists and is valid\n",
    "#                 if isinstance(image_path, str) and os.path.exists(image_path):\n",
    "#                     # Add the image to the corresponding row\n",
    "#                     img = Image(image_path)\n",
    "#                     cell_address = f\"{column}{index + 2}\"  # Adjust row index to match Excel's 1-based indexing\n",
    "#                     worksheet.add_image(img, cell_address)\n",
    "    \n",
    "#     # Save the workbook with images added\n",
    "#     workbook.save(file_path)\n",
    "#     print(f\"Images added and saved to {file_path}\")\n",
    "    \n",
    "# # Add images\n",
    "# add_images_to_excel(output_file_path, sheets_dict, column='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    # Only use ast.literal_eval if the value is a string.\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {val}: {e}\")\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "def format_vf_sentence(question, answer):\n",
    "    # Clean the inputs\n",
    "    question = question.strip()\n",
    "    answer_clean = answer.strip().lower()\n",
    "    # if answer_clean == 'no':\n",
    "    #     return f\"The visual presentation did not clearly address '{question}'.\"\n",
    "    # elif answer_clean == 'yes':\n",
    "    #     return f\"The visual presentation clearly addressed '{question}'.\"\n",
    "    # else:\n",
    "    #     print(f\"Invalid answer: {answer}\")\n",
    "    #     return f\"For '{question}', the response was '{answer}'.\"\n",
    "    return \"<br> - \" + question\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    print(f\"Sheet Name: {sheet_name}\")\n",
    "    print(df.head())\n",
    "    print()\n",
    "    \n",
    "    reason_vf_list = []\n",
    "    reason_contr_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # reason of high/low visual fidelity\n",
    "        # show two visual questions and their answers\n",
    "        # priority to show incorrect answered questions\n",
    "        visual_questions = safe_literal_eval(row['vf_questions'])\n",
    "        visual_answers = safe_literal_eval(row['vf_answers_GPT_converted'])\n",
    "        \n",
    "        # Pair up questions with answers\n",
    "        qa_pairs = list(zip(visual_questions, visual_answers))\n",
    "        \n",
    "        # Prioritize pairs with incorrect answers (\"No\")\n",
    "        incorrect_pairs = [pair for pair in qa_pairs if pair[1].strip().lower() == 'no']\n",
    "        correct_pairs = [pair for pair in qa_pairs if pair[1].strip().lower() != 'no']\n",
    "        \n",
    "        # Choose up to two pairs, prioritizing incorrect answers\n",
    "        selected_pairs = incorrect_pairs[:2]\n",
    "        if len(selected_pairs) < 2:\n",
    "            needed = 2 - len(selected_pairs)\n",
    "            selected_pairs.extend(correct_pairs[:needed])\n",
    "        \n",
    "       # Create natural language sentences for each question-answer pair\n",
    "        sentences = [format_vf_sentence(q, a) for q, a in zip(visual_questions, visual_answers)]\n",
    "        # Combine the sentences into a single natural language paragraph\n",
    "        paragraph = \" \".join(sentences)\n",
    "        \n",
    "        reason_vf_list.append(paragraph)\n",
    "        \n",
    "        # Contrastiveness Reasoning ---------------------------------------\n",
    "        # Parse alternative scores and answer choices\n",
    "        contr_scores = safe_literal_eval(row['alt_ent_prob'])\n",
    "        # minus the predict answer\n",
    "        predicted_answer = row['predicted_answer']\n",
    "        alt_answers = row['question'].split('Choices: ')[1].split(', ')\n",
    "        if predicted_answer in alt_answers:\n",
    "            alt_answers.remove(predicted_answer)\n",
    "        \n",
    "        \n",
    "        correct_score = row['entail_prob']\n",
    "        threshold = correct_score / 2\n",
    "        \n",
    "        # Select answers with a score at least half of the correct answer's score\n",
    "        selected_options = []\n",
    "        for ans, score in zip(alt_answers, contr_scores):\n",
    "            if score >= threshold:\n",
    "                selected_options.append(f\"{ans.strip()} (score: {score:.2f})\")\n",
    "        \n",
    "        if selected_options:\n",
    "            contr_reason = (\n",
    "                \", \".join(selected_options)\n",
    "            )\n",
    "        else:\n",
    "            contr_reason = \"\"\n",
    "        \n",
    "        reason_contr_list.append(contr_reason)\n",
    "        \n",
    "    df['reason_vf'] = reason_vf_list\n",
    "    df['reason_contr'] = reason_contr_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the print setting to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "sheets_dict['LLaVA-1.5 with image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_dict['LLaVA-1.5 with image']['vf_questions'][9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample json file \n",
    "# Which includes question, predicted answer, generated_rationale, visual_fidelity_converted_score (named as visual fidelity), contrastiveness_converted_score (named as contrastiveness)\n",
    "# first 10 lines of the first df in the sheets_dict\n",
    "# Save it as sample_data.json\n",
    "import json\n",
    "\n",
    "# Initialize the sample_data dictionary\n",
    "sample_data = {}\n",
    "\n",
    "# Iterate through all sheets in the sheets_dict\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "\n",
    "    # Define the columns to include and their desired names in JSON\n",
    "    selected_columns = {\n",
    "        'question_id': 'question_id',\n",
    "        'question': 'question',\n",
    "        'predicted_answer': 'predicted_answer',\n",
    "        'is_correct': 'prediction_is_correct',\n",
    "        'generated_rationale': 'generated_rationale',\n",
    "        'visual_fidelity_converted_score': 'visual_fidelity',\n",
    "        'contrastiveness_converted_score': 'contrastiveness',\n",
    "        'reason_vf': 'reason_vf',\n",
    "        'reason_contr': 'reason_contr'\n",
    "    }\n",
    "\n",
    "    # Check if all required columns exist\n",
    "    missing_columns = [col for col in selected_columns.keys() if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"The following required columns are missing in the DataFrame of sheet '{sheet_name}': {missing_columns}\")\n",
    "\n",
    "    # Rename columns as per the desired JSON structure\n",
    "    renamed_df = df[list(selected_columns.keys())].rename(columns=selected_columns)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    sheet_data = renamed_df.to_dict(orient='records')\n",
    "\n",
    "    # Generate a valid file name by replacing spaces with underscores\n",
    "    file_name = f\"{sheet_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "    # Save to a separate JSON file\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(sheet_data, file, indent=4)  # Added indent for better readability\n",
    "\n",
    "    print(f\"Data from sheet '{sheet_name}' has been successfully saved to '{file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# file_path = '../results/rationales_analysis_full_details_v2_selected.xlsx'\n",
    "\n",
    "# sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# for sheet_name, df in sheets_dict.items():\n",
    "#     # Ensure specific columns are strings\n",
    "#     for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = df[col].astype(str)\n",
    "#     for col in ['vf_questions', 'vf_answers_GPT_converted', 'alt_ent_prob']:\n",
    "#         df[col] = df[col].apply(ast.literal_eval)\n",
    "#     # Check the percentage of 'Yes' in the column vf_answers_GPT_converted\n",
    "#     df['visual_fidelity_converted_score'] = df['vf_answers_GPT_converted'].apply(lambda x: x.count('Yes') / len(x))\n",
    "#     # Check p(entail_prob) / (p(entail_prob) + sum(p(alt_ent_prob)))\n",
    "#     df['contrastiveness_converted_score'] = df.apply(lambda row: row['entail_prob'] / (row['entail_prob'] + sum(row['alt_ent_prob'])), axis=1)\n",
    "#     df['question_id'] = df['image_path'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# sheets_dict\n",
    "\n",
    "def save_selected_rows_to_json(sheet_name, df):\n",
    "    # Take the first 10 rows of the DataFrame\n",
    "\n",
    "    # Define the columns to include and their desired names in JSON\n",
    "    selected_columns = {\n",
    "        'question_id': 'question_id',\n",
    "        'question': 'question',\n",
    "        'predicted_answer': 'predicted_answer',\n",
    "        'is_correct': 'prediction_is_correct',\n",
    "        'generated_rationale': 'generated_rationale',\n",
    "        'visual_fidelity_converted_score': 'visual_fidelity',\n",
    "        'contrastiveness_converted_score': 'contrastiveness',\n",
    "        'reason_vf': 'reason_vf',\n",
    "        'reason_contr': 'reason_contr'\n",
    "    }\n",
    "\n",
    "    # Check if all required columns exist\n",
    "    missing_columns = [col for col in selected_columns.keys() if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"The following required columns are missing in the DataFrame of sheet '{sheet_name}': {missing_columns}\")\n",
    "\n",
    "    # Rename columns as per the desired JSON structure\n",
    "    renamed_df = df[list(selected_columns.keys())].rename(columns=selected_columns)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    sheet_data = renamed_df.to_dict(orient='records')\n",
    "\n",
    "    # Generate a valid file name by replacing spaces with underscores\n",
    "    file_name = f\"{sheet_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "    # Save to a separate JSON file\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(sheet_data, file, indent=4)  # Added indent for better readability\n",
    "\n",
    "    print(f\"Data from sheet '{sheet_name}' has been successfully saved to '{file_name}'.\")\n",
    "\n",
    "# For the first in sheets_dict, randomly select 200 rows with 100 correct and 100 incorrect\n",
    "# For the second in sheets_dict, randomly select equal number of rows of correct with the incorrect ones\n",
    "import random\n",
    "import json\n",
    "\n",
    "for idx, (sheet_name, df) in enumerate(sheets_dict.items()):\n",
    "    if sheet_name.lower().startswith('llava'):\n",
    "        # Randomly select 200 rows with 100 correct and 100 incorrect\n",
    "        correct_rows = df[df['is_correct'] == 1].sample(n=100, random_state=42)\n",
    "        incorrect_rows = df[df['is_correct'] == 0].sample(n=100, random_state=42)\n",
    "        selected_rows = pd.concat([correct_rows, incorrect_rows])\n",
    "        # Save the selected rows to a new json file\n",
    "        save_selected_rows_to_json(sheet_name, selected_rows)\n",
    "    elif sheet_name.lower().startswith('gpt'):\n",
    "        # filter out the rows contained GPT refusal explanations: I'm sorry / I am sorry / I can't / I cannot / I'm unable / I am unable / I apologize\n",
    "        refused_expls = ['I\\'m sorry', 'I am sorry', 'I can\\'t', 'I cannot', 'I\\'m unable', 'I am unable', 'I apologize']\n",
    "        df['predicted_answer'] = df['predicted_answer'].str.lower().str.strip('*\\n ')\n",
    "        df['is_correct'] = (df['is_correct'] | (df['predicted_answer'] == df['correct_answer'].str.lower())).astype(int)\n",
    "        \n",
    "        df = df[~df['generated_rationale'].str.contains('|'.join(refused_expls), case=False)]\n",
    "        min_row_num = df['is_correct'].value_counts().min()\n",
    "        print(f\"Number of incorrect rows: {min_row_num}\")\n",
    "        correct_rows = df[df['is_correct'] == 1].sample(n=min_row_num, random_state=42)\n",
    "        incorrect_rows = df[df['is_correct'] == 0].sample(n=min_row_num, random_state=42)\n",
    "        selected_rows = pd.concat([correct_rows, incorrect_rows])\n",
    "        # Save the selected rows to a new json file\n",
    "        save_selected_rows_to_json(sheet_name, selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Improve the contrastiveness metric\n",
    "## Can we try convert scores back to logits and compute the confidence from there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ../results/rationales_analysis_full_details.xlsx\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "file_path = '../results/rationales_analysis_full_details_v2.xlsx'\n",
    "\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    # Ensure specific columns are strings\n",
    "    for col in ['question', 'predicted_answer', 'correct_answer', 'generated_rationale']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    for col in ['vf_questions', 'vf_answers_GPT_converted', 'alt_ent_prob']:\n",
    "        df[col] = df[col].apply(ast.literal_eval)\n",
    "    # Check the percentage of 'Yes' in the column vf_answers_GPT_converted\n",
    "    df['visual_fidelity_converted_score'] = df['vf_answers_GPT_converted'].apply(lambda x: x.count('Yes') / len(x))\n",
    "    # Check p(entail_prob) / (p(entail_prob) + sum(p(alt_ent_prob)))\n",
    "    df['contrastiveness_converted_score'] = df.apply(lambda row: row['entail_prob'] / (row['entail_prob'] + sum(row['alt_ent_prob'])), axis=1)\n",
    "    df['question_id'] = df['image_path'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "sheet1 = sheets_dict['LLaVA-1.5 with image']\n",
    "sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of contrastiveness scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sheet1['contrastiveness_converted_score'], bins=25, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Contrastiveness Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Contrastiveness Scores')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_prob_back_to_logits(prob):\n",
    "    \"\"\"_summary_\n",
    "        Usually, probs cannot be converted back to logits because we don't know the temperature and the potential tokens.\n",
    "        But here, this NLI entailment model only generates 2 labels, with temperature set to 1.\n",
    "        p = e^x / (e^x + e^y) => e^x = p * (e^x + e^y) => e^x = p / (1 - p) * e^y => x = log(p / (1 - p)) + y\n",
    "        So there is a group of logits that can generate the same probability:\n",
    "        x = log(p / (1 - p)) + c, y = c.\n",
    "        So by setting c = 0, we can convert the probability back to logits.\n",
    "    \"\"\"\n",
    "    return np.log(prob / (1 - prob))\n",
    "\n",
    "# Convert the contrastiveness scores back to logits\n",
    "def compute_cont_score_new(pred_ent_prob, alt_ent_prob):\n",
    "    pred_logit = convert_prob_back_to_logits(pred_ent_prob)\n",
    "    alt_logits = [convert_prob_back_to_logits(prob) for prob in alt_ent_prob]\n",
    "    # return e^x / (e^x + sum(e^y))\n",
    "    return np.exp(pred_logit) / (np.exp(pred_logit) + sum(np.exp(alt_logits)))\n",
    "\n",
    "# Compute the new contrastiveness scores\n",
    "for i, row in sheet1.iterrows():\n",
    "    sheet1.at[i, 'contrastiveness_converted_score_new'] = compute_cont_score_new(row['entail_prob'], row['alt_ent_prob'])\n",
    "    \n",
    "sheet1[['entail_prob', 'alt_ent_prob', 'contrastiveness_converted_score', 'contrastiveness_converted_score_new']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of contrastiveness scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(sheet1['contrastiveness_converted_score'], bins=25, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Contrastiveness Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Contrastiveness Scores')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of contrastiveness scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(sheet1['contrastiveness_converted_score_new'], bins=25, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Contrastiveness Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Contrastiveness Scores (New, converted back to logits and calculated softmax)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Extract the previous llava-1.5_with_image_old.json question ids (make sure we did the same questions on the user study)\n",
    "llava_15_old = pd.read_json('llava-1.5_with_image_old.json')\n",
    "llava_15_old_question_ids = llava_15_old['question_id'].tolist()\n",
    "# save the question ids to a new json file\n",
    "with open('llava-1.5_with_image_question_ids.json', 'w') as f:\n",
    "    json.dump(llava_15_old_question_ids, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
