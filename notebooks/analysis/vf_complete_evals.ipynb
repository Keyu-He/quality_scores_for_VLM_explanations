{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "columns_to_read = [\n",
    "    'question',\n",
    "    'correct_answer',\n",
    "    'predicted_answer',\n",
    "    'is_correct',\n",
    "    'image_path',\n",
    "    'vf_questions',\n",
    "    'vf_answers_GPT',\n",
    "    'strict_sim',\n",
    "    'Human Annotated VF Questions Qualities',\n",
    "    'Human Annotated VF Answers Correctness',\n",
    "    'Human Annotated completeness'\n",
    "]\n",
    "\n",
    "data = pd.read_excel('../results/Human Annotation of LLaVA+ Rationales.xlsx', sheet_name=\"LLAVA-1.5 Rationales-val 500 se\", usecols=columns_to_read).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of all annotated instances up to data == 71 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Number of all Human Annotated VF Questions Qualities\n",
    "# Flatten them, and count the numbers of -1, 0, and 1\n",
    "\n",
    "question_qualities = []\n",
    "for i in data['Human Annotated VF Questions Qualities']:\n",
    "    question_qualities.extend(ast.literal_eval(i))\n",
    "question_qualities = np.array(question_qualities)\n",
    "print(question_qualities, question_qualities.shape)\n",
    "# print number of -1, 0, 1\n",
    "print(f\"Number of -1: {np.sum(question_qualities == -1)}\")\n",
    "print(f\"Number of 0: {np.sum(question_qualities == 0)}\")\n",
    "print(f\"Number of 1: {np.sum(question_qualities == 1)}\")\n",
    "print(f\"Relevant questions percentage: {np.sum(question_qualities == 1) / question_qualities.shape[0]}\")\n",
    "\n",
    "vf_answers_GPT = []\n",
    "for i in data['vf_answers_GPT']:\n",
    "    vf_answers_GPT.extend(ast.literal_eval(i))\n",
    "# convert yes to 1, no to 0\n",
    "vf_answers_GPT = [1 if i == 'Yes' else 0 for i in vf_answers_GPT]\n",
    "vf_answers_GPT = np.array(vf_answers_GPT)\n",
    "print(vf_answers_GPT, vf_answers_GPT.shape)\n",
    "\n",
    "vf_answers_human = []\n",
    "for i in data['Human Annotated VF Answers Correctness']:\n",
    "    vf_answers_human.extend(ast.literal_eval(i))\n",
    "vf_answers_human = np.array(vf_answers_human)\n",
    "print(vf_answers_human, vf_answers_human.shape)\n",
    "\n",
    "# Merge these three arrays into a 2D dataframe\n",
    "# 1st column: question quality\n",
    "# 2nd column: GPT answer\n",
    "# 3rd column: human answer\n",
    "df = pd.DataFrame({\n",
    "    'question_quality': question_qualities,\n",
    "    'GPT_answer': vf_answers_GPT,\n",
    "    'human_answer': vf_answers_human\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# for the relevant questions (question_quality == 1), calculate the accuracy/correlation/agreement of GPT and human\n",
    "relevant_questions = df[df['question_quality'] == 1]\n",
    "print(relevant_questions['human_answer'].value_counts())\n",
    "\n",
    "GPT_answers, human_answers = relevant_questions['GPT_answer'], relevant_questions['human_answer']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.sum(GPT_answers == human_answers) / relevant_questions.shape[0]\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Bootstrapping for error bars (confidence intervals)\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    sample = resample(relevant_questions)\n",
    "    sample_accuracy = np.sum(sample['GPT_answer'] == sample['human_answer']) / sample.shape[0]\n",
    "    bootstrap_accuracies.append(sample_accuracy)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_accuracies, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_accuracies, 97.5)\n",
    "print(f\"95% Confidence Interval for Accuracy: [{lower_bound}, {upper_bound}]\")\n",
    "\n",
    "# Correlation (Pearson and Spearman)\n",
    "# pearson_corr, _ = pearsonr(relevant_questions['GPT_answer'], relevant_questions['human_answer'])\n",
    "spearman_corr, _ = spearmanr(GPT_answers, human_answers)\n",
    "# print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "\n",
    "# Agreement: Cohen's kappa\n",
    "kappa = cohen_kappa_score(GPT_answers, human_answers)\n",
    "print(f\"Cohen's Kappa: {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoevaled completeness\n",
    "autoeval_completeness = []\n",
    "for i in data['strict_sim']:\n",
    "    # True --> 1, False --> 0\n",
    "    autoeval_completeness.append(int(i))\n",
    "print(autoeval_completeness)\n",
    "\n",
    "\n",
    "# Human annotated completeness\n",
    "human_completeness = []\n",
    "for i in data['Human Annotated completeness']:\n",
    "    human_completeness.append(int(i))\n",
    "print(human_completeness)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.sum(np.array(human_completeness) == np.array(autoeval_completeness)) / len(human_completeness)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Bootstrapping for error bars (confidence intervals)\n",
    "n_iterations = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    sample = resample(relevant_questions)\n",
    "    sample_accuracy = np.sum(sample['GPT_answer'] == sample['human_answer']) / sample.shape[0]\n",
    "    bootstrap_accuracies.append(sample_accuracy)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_accuracies, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_accuracies, 97.5)\n",
    "print(f\"95% Confidence Interval for Accuracy: [{lower_bound}, {upper_bound}]\")\n",
    "\n",
    "# Correlation and agreement\n",
    "spearman_corr, _ = spearmanr(human_completeness, autoeval_completeness)\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "kappa = cohen_kappa_score(human_completeness, autoeval_completeness)\n",
    "print(f\"Cohen's Kappa: {kappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
