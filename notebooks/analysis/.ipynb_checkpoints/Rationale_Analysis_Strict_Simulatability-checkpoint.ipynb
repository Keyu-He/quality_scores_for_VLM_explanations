{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nli_tokenizer' not in globals():\n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "if 'nli_model' not in globals():\n",
    "    nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]\n",
    "    \n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Record the cost\n",
    "COST_FILE = \"total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "        \n",
    "def calculate_cost(usage, model, verbose=0):\n",
    "    if model == \"gpt-4o-2024-05-13\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    elif model == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_token = 0.0025 / 1000\n",
    "        output_cost_per_token = 0.010 / 1000\n",
    "    elif model == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_token = 0.00015 / 1000\n",
    "        output_cost_per_token = 0.00060 / 1000\n",
    "    \n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_rationale(rationale_list):\n",
    "    rationales = eval(rationale_list)\n",
    "    return max(rationales, key=len) if isinstance(rationales, list) else ''\n",
    "\n",
    "def calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            no_entail_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, no_entail_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[0]\n",
    "\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    predicted_answer = str(predicted_answer)\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if use_pieces:\n",
    "            premise = row['concat_rationale_pieces_mask'] if use_mask else row['concat_rationale_pieces']\n",
    "        else: \n",
    "            premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        \n",
    "        hypothesis = row[hypothesis_col]\n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        support = entail_prob > threshold \n",
    "        if entail_prob < threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {entail_prob}\")\n",
    "        support_scores.append({\n",
    "            'entail_prob': entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "    return support_scores\n",
    "\n",
    "def compute_file_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('<link_hidden>/notebooks/analysis/data_vf_q_val_GPT_r_dense_captions_support_concat.csv')\n",
    "df = data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_gen_hypothesis(question, predicted_ans, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-mini-2024-07-18\"   \n",
    "    user_input = f\"\"\"Integrate the question and the answer into one sentence.\n",
    "For example, given the question \"What is the man waiting for?\" and the answer \"taxi\", you should output \"The man is waiting for taxi.\"\n",
    "\n",
    "Question: {question}\n",
    "Answer: {predicted_ans}\n",
    "\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\n",
    "            #             \"type\": \"text\",\n",
    "            #             \"text\": system_prompt\n",
    "            #         }\n",
    "            #     ]\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alternative_hypotheses(row):\n",
    "    question_text, choices_text = row['question'].split(\"Choices: \")\n",
    "    predicted_answer = row['one_step_answer']\n",
    "    # Extract the possible answers from the question text\n",
    "    choices = choices_text.split(', ')\n",
    "    choices = [choice.strip('. ') for choice in choices]\n",
    "    \n",
    "    alternative_hypotheses = []\n",
    "    for choice in choices:\n",
    "        if choice.lower() != predicted_answer.lower():\n",
    "            alternative_hypotheses.append(gpt_gen_hypothesis(question_text, choice))\n",
    "            \n",
    "    return alternative_hypotheses\n",
    "\n",
    "data['alternative_hypotheses'] = data.apply(generate_alternative_hypotheses, axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strict_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', alt_hypotheses_col='alternative_hypotheses', threshold=0.5):\n",
    "    strict_supports = []\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['rationale_mask']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        alt_hypotheses = row[alt_hypotheses_col]\n",
    "        \n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        alt_entail_probs = []\n",
    "        for alt_hypothesis in alt_hypotheses:\n",
    "            alt_entail_probs.append(calc_support_prob(premise, alt_hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer))\n",
    "        \n",
    "        strict_support = entail_prob > threshold and all(alt_entail_prob < threshold for alt_entail_prob in alt_entail_probs)\n",
    "#         if strict_support == False:\n",
    "        print(f\"Premise: {premise}\")\n",
    "        print(f\"Hypothesis: {hypothesis}\")\n",
    "        print(f\"Probability: {entail_prob}\")\n",
    "        print(f\"Alternative hypotheses: {alt_hypotheses}\")\n",
    "        print(f\"Probability: {alt_entail_probs}\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        strict_supports.append(strict_support)\n",
    "        data.loc[idx, 'alt_ent_prob'] = str(alt_entail_probs)\n",
    "        data.loc[idx, 'strict_sim'] = strict_support\n",
    "    \n",
    "evaluate_strict_support(data, nli_model, nli_tokenizer)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_path = \"val_strict_sim.xlsx\"\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "import os\n",
    "\n",
    "# First, save the dataframe to an excel file (without images) to manipulate it with openpyxl\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Load the saved excel file\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Add images to the new column in the excel file\n",
    "for index, row in df.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    # Check if the image file exists before adding\n",
    "    if os.path.exists(img_path):\n",
    "        img = ExcelImage(img_path)\n",
    "        img_cell = f\"V{index + 2}\"  # Placing the image starting from row 2, column R\n",
    "        ws.add_image(img, img_cell)\n",
    "\n",
    "# Save the updated excel file with images\n",
    "output_path_with_images = output_path\n",
    "wb.save(output_path_with_images)\n",
    "\n",
    "output_path_with_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
