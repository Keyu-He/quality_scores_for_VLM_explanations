{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5874d0",
   "metadata": {},
   "source": [
    "# Section 1: Explanation Entailment (Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install nltk rouge-score sacrebleu openpyxl openai torch transformers datasets matplotlib bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9886e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3879e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the spreadsheet\n",
    "file_path = '../results/Human Annotation of LLaVA+ Rationales.xlsx'\n",
    "\n",
    "file_path = \"../results/gpt-4o_inference_two_steps_50.xlsx\"\n",
    "spreadsheet = pd.ExcelFile(file_path)\n",
    "\n",
    "# # Display sheet names\n",
    "# spreadsheet.sheet_names\n",
    "\n",
    "# Read the specified columns from the sheet\n",
    "columns_to_read = [\n",
    "    'question',\n",
    "    'correct_answer',\n",
    "    'predicted_answer',\n",
    "    'is_correct',\n",
    "    'groundtruth_rationale',\n",
    "    'generated_rationale'\n",
    "]\n",
    "\n",
    "if file_path== '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "else:\n",
    "    data = pd.read_excel(file_path, usecols=columns_to_read)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c092a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_rationale(rationale_list):\n",
    "    rationales = eval(rationale_list)\n",
    "    return max(rationales, key=len) if isinstance(rationales, list) else ''\n",
    "\n",
    "# Apply the function to create the new column\n",
    "data['longest_groundtruth_rationale'] = data['groundtruth_rationale'].apply(get_longest_rationale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30800189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "def calculate_meteor(reference, candidate):\n",
    "    reference_tokens = reference.split()\n",
    "    candidate_tokens = candidate.split()\n",
    "    score = meteor_score([reference_tokens], candidate_tokens)\n",
    "    return score\n",
    "\n",
    "def display_scores(reference, candidate):\n",
    "    bleu_score = calculate_bleu(reference, candidate)\n",
    "    rouge_scores = calculate_rouge(reference, candidate)\n",
    "    meteor_score = calculate_meteor(reference, candidate)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(\"ROUGE Scores:\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"METEOR Score: {meteor_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bab113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores for each row and store in the dataframe\n",
    "data['BLEU_score'] = data.apply(lambda row: calculate_bleu(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "data['ROUGE_scores'] = data.apply(lambda row: calculate_rouge(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "data['METEOR_score'] = data.apply(lambda row: calculate_meteor(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a6959",
   "metadata": {},
   "source": [
    "We can see that BLEU scores are generally very low. \n",
    "\n",
    "BLEU scores are generally more suited to tasks like machine translation where there is a high degree of overlap between the reference and generated text. \n",
    "For tasks like rationale generation, where the generated text might be quite different in wording from the reference but still correct, BLEU scores might be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f426f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the NLI model and tokenizer\n",
    "# nli_tokenizer = AutoTokenizer.from_pretrained('roberta-large-mnli')\n",
    "# nli_model = AutoModelForSequenceClassification.from_pretrained('roberta-large-mnli')\n",
    "\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_low_support_score(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    \n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            contra_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, contra_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[1]\n",
    "\n",
    "premise = \"A fossil fuel is a kind of natural resource. Coal is a kind of fossil fuel.\"\n",
    "hypothesis = \"Coal is a kind of natural resource.\"\n",
    "\n",
    "low_support_score = calc_low_support_score(premise, hypothesis)\n",
    "print(f'Hypothesis does not entail the premise: {bool(low_support_score >= 0.5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# def create_hypothesis(question, answer):\n",
    "#     question = question.split(' Choices:')[0]\n",
    "    \n",
    "#     QA_concat_str = f\"Q: {question} A: {answer}\"\n",
    "#     prompt = f\"Rephrase the following question and answer into a descriptive sentence:\\n{QA_concat_str}\"\n",
    "\n",
    "#     # Get the response from GPT-3.5-turbo\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         max_tokens=50,\n",
    "#     )\n",
    "\n",
    "#     # Extract the rephrased sentence\n",
    "#     rephrased_sentence = response.choices[0].message.content.strip()\n",
    "\n",
    "#     return rephrased_sentence\n",
    "\n",
    "# # Apply the function to each row in the DataFrame\n",
    "# data['hypothesis'] = data.apply(lambda row: create_hypothesis(row['question'], row['predicted_answer']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read hypothesis from hypothesis column from data_with_LAS.csv\n",
    "# # Load the new data from CSV file\n",
    "# data_with_LAS = pd.read_csv('data_with_LAS.csv')\n",
    "\n",
    "# # Combine the 'hypothesis' column from the new data with the current DataFrame\n",
    "# data['hypothesis'] = data_with_LAS['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hypothesis using run_question_converter.sh from https://github.com/HanjieChen/REV\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_data = data[['question', 'predicted_answer']].copy()\n",
    "\n",
    "# Keep only the part of the question before 'Choices:'\n",
    "input_data['question'] = input_data['question'].apply(lambda x: x.split(' Choices:')[0])\n",
    "\n",
    "input_data.rename(columns={'question': 'question_text', 'predicted_answer': 'answer_text'}, inplace=True)\n",
    "\n",
    "input_jsonl = 'input_data.jsonl'\n",
    "\n",
    "# Write to JSONL file\n",
    "with open(input_jsonl, 'w') as f:\n",
    "    for index, row in input_data.iterrows():\n",
    "        json.dump(row.to_dict(), f)\n",
    "        f.write('\\n')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5115",
   "metadata": {},
   "source": [
    "(Run the conversion script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full path to the script\n",
    "script_path = '<link_hidden>/REV/run_question_converter.sh'\n",
    "\n",
    "# Set PYTHONPATH and run the script\n",
    "os.system(f'export PYTHONPATH=/home/<link_hidden>/REV/:$PYTHONPATH && bash {script_path} cqa {input_jsonl} cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_jsonl = 'input_data.jsonl.predictions'\n",
    "\n",
    "# Read the predictions\n",
    "with open(output_jsonl, 'r') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert to DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.rename(columns={'question_text': 'question', 'question_statement_text': 'hypothesis'}, inplace=True)\n",
    "\n",
    "data['question_no_choice'] = data.apply(lambda row: row['question'].split(' Choices:')[0], axis=1)\n",
    "\n",
    "# Merge datasets based on the 'question' column\n",
    "data = pd.merge(data, predictions_df[['question', 'hypothesis']], left_on='question_no_choice', right_on='question', how='left')\n",
    "\n",
    "data.drop(columns=['question_y'], inplace=True)\n",
    "data.rename(columns={'question_x': 'question'}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757579c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "data['gen_rationale_mask'] = data.apply(lambda row: generate_mask(row['generated_rationale'], row['predicted_answer']), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305631eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'hypothesis_y': 'hypothesis'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the full content of the 'gen_rationale_mask' column\n",
    "print(data['gen_rationale_mask'])\n",
    "\n",
    "# Reset the display option back\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba117ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_nli(model, tokenizer, premise, hypothesis):\n",
    "#     inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, padding=True)\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**inputs).logits\n",
    "#         probabilities = torch.softmax(logits, dim=-1)\n",
    "#     return probabilities\n",
    "\n",
    "# def evaluate_support(data, nli_model, nli_tokenizer):\n",
    "#     support_scores = []\n",
    "\n",
    "#     for idx, row in data.iterrows():\n",
    "#         premise = row['gen_rationale_mask']\n",
    "#         hypothesis = row['hypothesis']\n",
    "#         probabilities = predict_nli(nli_model, nli_tokenizer, premise, hypothesis)\n",
    "        \n",
    "#         # {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
    "#         contradiction_prob = probabilities[0][0].item()\n",
    "#         if contradiction_prob > 0.5:\n",
    "#             print(f\"Premise: {premise}\")\n",
    "#             print(f\"Hypothesis: {hypothesis}\")\n",
    "#             print(f\"Probabilities: {probabilities[0]}\")\n",
    "# #         print(probabilities[0])\n",
    "#         support = contradiction_prob < 0.5 \n",
    "\n",
    "#         support_scores.append({\n",
    "#             'contradiction_prob': contradiction_prob,\n",
    "#             'support': support\n",
    "#         })\n",
    "\n",
    "#     return support_scores\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, hypothesis_col='hypothesis', threshold=threshold):\n",
    "    support_scores = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['gen_rationale_mask'] if use_mask else row['generated_mask']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        \n",
    "        no_entail_prob = calc_low_support_score(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "    \n",
    "        if no_entail_prob > threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {no_entail_prob}\")\n",
    "#         print(probabilities[0])\n",
    "        support = no_entail_prob < threshold \n",
    "\n",
    "        support_scores.append({\n",
    "            'no_entail_prob': no_entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "\n",
    "    return support_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e7af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate simulatability\n",
    "support_results = evaluate_support(data, nli_model, nli_tokenizer)\n",
    "support_df = pd.DataFrame(support_results)\n",
    "for column in support_df.columns:\n",
    "    data[column] = support_df[column]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d80bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of contradiction_prob\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.hist(data['no_entail_prob'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of no_entail_prob')\n",
    "plt.xlabel('no_entail_prob')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'support' is False\n",
    "filtered_data = data[data['no_entail_prob'] >= 0.5]\n",
    "print(len(filtered_data))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18349706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'support' is False\n",
    "filtered_data2 = data[data['no_entail_prob'] < 0.1]\n",
    "print(len(filtered_data2))\n",
    "filtered_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_with_support.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[['question','correct_answer','predicted_answer', 'hypothesis', 'gen_rationale_mask', 'no_entail_prob']]\\\n",
    ".to_csv('filtered_data.csv', index=False)\n",
    "filtered_data2[['question','correct_answer','predicted_answer', 'hypothesis', 'gen_rationale_mask', 'no_entail_prob']]\\\n",
    ".to_csv('filtered_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b701a16",
   "metadata": {},
   "source": [
    "# Make all of it in one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf71e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# load the tokenizer and model only if they are not already defined\n",
    "if 'nli_tokenizer' not in globals():\n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "if 'nli_model' not in globals():\n",
    "    nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "def calculate_meteor(reference, candidate):\n",
    "    reference_tokens = reference.split()\n",
    "    candidate_tokens = candidate.split()\n",
    "    score = meteor_score([reference_tokens], candidate_tokens)\n",
    "    return score\n",
    "\n",
    "def display_scores(reference, candidate):\n",
    "    bleu_score = calculate_bleu(reference, candidate)\n",
    "    rouge_scores = calculate_rouge(reference, candidate)\n",
    "    meteor_score = calculate_meteor(reference, candidate)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(\"ROUGE Scores:\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"METEOR Score: {meteor_score:.4f}\")\n",
    "    \n",
    "def get_longest_rationale(rationale_list):\n",
    "    rationales = eval(rationale_list)\n",
    "    return max(rationales, key=len) if isinstance(rationales, list) else ''\n",
    "\n",
    "def calc_low_support_score(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            contra_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, contra_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[1]\n",
    "\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    predicted_answer = str(predicted_answer)\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        no_entail_prob = calc_low_support_score(premise, hypothesis, use_mask=use_mask, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        support = no_entail_prob < threshold \n",
    "        if no_entail_prob > threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {no_entail_prob}\")\n",
    "        support_scores.append({\n",
    "            'no_entail_prob': no_entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "    return support_scores\n",
    "\n",
    "def compute_file_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# Main\n",
    "def main(file_path):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "        model_name = \"LLaVA\"\n",
    "    else:\n",
    "        model_name = file_path.split('results/')[1].split('.xlsx')[0]\n",
    "    \n",
    "    spreadsheet = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Read the specified columns from the sheet\n",
    "    columns_to_read = [\n",
    "        'question',\n",
    "        'correct_answer',\n",
    "        'predicted_answer',\n",
    "        'is_correct',\n",
    "        'groundtruth_rationale',\n",
    "        'generated_rationale'\n",
    "    ]\n",
    "\n",
    "    if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "        data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "    else:\n",
    "        data = pd.read_excel(file_path, usecols=columns_to_read)\n",
    "    data['question_no_choice'] = data.apply(lambda row: row['question'].split(' Choices:')[0], axis=1)\n",
    "    data['longest_groundtruth_rationale'] = data['groundtruth_rationale'].apply(get_longest_rationale)\n",
    "    \n",
    "    data['BLEU_score'] = data.apply(lambda row: calculate_bleu(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "    data['ROUGE_scores'] = data.apply(lambda row: calculate_rouge(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "    data['METEOR_score'] = data.apply(lambda row: calculate_meteor(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "    \n",
    "    input_data = data[['question', 'predicted_answer']].copy()\n",
    "    input_data['question'] = input_data['question'].apply(lambda x: x.split(' Choices:')[0])\n",
    "    input_data.rename(columns={'question': 'question_text', 'predicted_answer': 'answer_text'}, inplace=True)\n",
    "    input_jsonl = f'input_data_{model_name}.jsonl'\n",
    "    output_jsonl = f'{input_jsonl}.predictions'\n",
    "    with open(input_jsonl, 'w') as f:\n",
    "        for index, row in input_data.iterrows():\n",
    "            # Convert None to null\n",
    "            row_dict = {k: (v if pd.notna(v) else 'None') for k, v in row.to_dict().items()}\n",
    "            json.dump(row_dict, f)\n",
    "            f.write('\\n')\n",
    "    # Compute hash of input_jsonl\n",
    "    current_input_hash = compute_file_hash(input_jsonl)\n",
    "    hash_file = f'{input_jsonl}.hash'\n",
    "    # Check if output_jsonl exists and input_jsonl hash hasn't changed\n",
    "    run_bash_command = True\n",
    "    if os.path.exists(output_jsonl):\n",
    "        if os.path.exists(hash_file):\n",
    "            with open(hash_file, 'r') as f:\n",
    "                saved_input_hash = f.read().strip()\n",
    "                if current_input_hash == saved_input_hash:\n",
    "                    run_bash_command = False\n",
    "    # Save the current input_jsonl hash\n",
    "    with open(hash_file, 'w') as f:\n",
    "        f.write(current_input_hash)\n",
    "    \n",
    "    # The conversion step\n",
    "    # Define the full path to the script\n",
    "    script_path = '/home/<link_hidden>/REV/run_question_converter.sh'\n",
    "    if run_bash_command:\n",
    "        # Set PYTHONPATH and run the script\n",
    "        os.system(f'export PYTHONPATH=/home/<link_hidden>/REV/:$PYTHONPATH && bash {script_path} cqa {input_jsonl} cuda:0')\n",
    "    else:\n",
    "        print(f'{output_jsonl} already exists and {input_jsonl} has not changed. Skipping the bash command.')\n",
    "    \n",
    "    with open(output_jsonl, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    predictions_df.rename(columns={'question_statement_text': 'hypothesis'}, inplace=True)\n",
    "    \n",
    "    # Merge datasets based on the 'question' column\n",
    "    data = pd.merge(data, predictions_df[['question_text', 'hypothesis']], left_on='question_no_choice', right_on='question_text', how='left')\n",
    "    \n",
    "    data['gen_rationale_mask'] = data.apply(lambda row: generate_mask(row['generated_rationale'], row['predicted_answer']), axis=1)\n",
    "\n",
    "    # Evaluate support\n",
    "    support_results = evaluate_support(data, nli_model, nli_tokenizer)\n",
    "    support_df = pd.DataFrame(support_results)\n",
    "    for column in support_df.columns:\n",
    "        data[column] = support_df[column]\n",
    "        \n",
    "    # Plot the distribution of no_entail_prob\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.hist(data['no_entail_prob'], bins=50, edgecolor='black')\n",
    "    plt.title('Distribution of no_entail_prob')\n",
    "    plt.xlabel('no_entail_prob')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    support_score = data['support'].mean()\n",
    "    \n",
    "    print(f\"Support score: {support_score}\")\n",
    "    \n",
    "    data.to_csv(f'data_with_support_{model_name}.csv', index=False)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    file_paths = [\n",
    "                  '../results/Human Annotation of LLaVA+ Rationales.xlsx',\n",
    "#                   \"../results/gpt-4o_inference_one_shot_50_improved_prompt.xlsx\",\n",
    "#                   \"../results/gpt-4o_inference_two_steps_50.xlsx\",\n",
    "#                   \"../results/instructblip-flan-t5-xxl_inference_one_shot_50_improved_prompt.xlsx\",\n",
    "#                   \"../results/llava-1.5-7b-hf_inference_no_vision.xlsx\",\n",
    "#                   \"../results/gpt-4o_text_only_inference_one_shot.xlsx\",\n",
    "#                   \"../results/gpt-4o_inference_one_shot.xlsx\",\n",
    "                 ]\n",
    "    for file_path in file_paths:\n",
    "        main(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f57e8e",
   "metadata": {},
   "source": [
    "## 1.2 Pilot tests of support on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750693c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"The people in the image are located in their home. This can be inferred from the presence of a couch, which is a common piece of furniture found in homes. Additionally, the people are sitting on the couch, which further supports the idea that they are in their home. The image does not show any indications of a workplace, hospital, library, or any other location.\"\n",
    "premise = \"The image shows a casual and comfortable environment with two dogs lounging on a couch, along with personal items such as books, electronics, and various other belongings scattered around. This setting is typically indicative of a residence rather than a work, hospital, or library environment. The presence of dogs and the relaxed atmosphere further suggests a private living space. Thus, the answer is home.\"\n",
    "hypothesis = \"These people are located home.\"\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(premise)\n",
    "sentence_scores = []\n",
    "for sentence in sentences:\n",
    "    no_entail_prob = calc_low_support_score(sentence, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "    support = no_entail_prob < 0.2\n",
    "    sentence_scores.append({\n",
    "        'sentence': sentence,\n",
    "        'no_entail_prob': no_entail_prob,\n",
    "        'support': support\n",
    "    })\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7017a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(sentence_scores)\n",
    "pd.set_option('display.max_colwidth', None) # Show full column content\n",
    "# Display the DataFrame\n",
    "df\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display the DataFrame as a Markdown table\n",
    "display(Markdown(copiable_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence_level_support(data, nli_model, nli_tokenizer, use_mask=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    sentence_support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        sentences = nltk.tokenize.sent_tokenize(premise)\n",
    "        sentence_scores = []\n",
    "        for sentence in sentences:\n",
    "            no_entail_prob = calc_low_support_score(sentence, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "            support = no_entail_prob < threshold\n",
    "            sentence_scores.append({\n",
    "                'sentence': sentence,\n",
    "                'no_entail_prob': no_entail_prob,\n",
    "                'support': support\n",
    "            })\n",
    "        sentence_support_scores.append(sentence_scores)\n",
    "    return sentence_support_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4954c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        model_name = file_path.split('data_with_support_')[1].split('.csv')[0]\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Perform sentence-level support evaluation\n",
    "        sentence_level_results = evaluate_sentence_level_support(data, nli_model, nli_tokenizer)\n",
    "\n",
    "        # Add the sentence-level results as a new column\n",
    "        data['sentence_level_support'] = sentence_level_results\n",
    "\n",
    "        if not os.path.exists('data_with_sentence_level_support'):\n",
    "            os.makedirs('data_with_sentence_level_support')\n",
    "        \n",
    "        # Save the updated data to a new CSV file\n",
    "        output_file_path = f'data_with_sentence_level_support_{model_name}.csv'\n",
    "        data.to_csv(output_file_path, index=False)\n",
    "        print(f\"Processed {file_path} and saved results to {output_file_path}\")\n",
    "\n",
    "file_paths = [\n",
    "    'data_with_support_LLaVA.csv',\n",
    "    # Add other file paths\n",
    "]\n",
    "process_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfd0c5",
   "metadata": {},
   "source": [
    "## 1.3? try question+choices+answers in REV tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'export PYTHONPATH=/home/<link_hidden>/REV/:$PYTHONPATH && bash /home/<link_hidden>/REV/run_question_converter.sh cqa /home/<link_hidden>/REV/sample_input.jsonl cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93025a3",
   "metadata": {},
   "source": [
    "# Section 2: Visual Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_50_index_df = pd.read_csv(\"../set-50-idx.csv\")\n",
    "visual_fidelity = [1,1,0,1,0,1,0,'null',1,1,1,1,-1,1,1,1,1,1,1,-1,1,0,-1,1,-1,0,0,1,1,0,1,1,0,1,1,-1,1,1,-1,1,0,0,-1,-1,1,1,-1,1,1,-1]\n",
    "set_50_index_df[\"visual_fidelity\"] = visual_fidelity\n",
    "\n",
    "# Select subsets with visual_fidelity == 1 and visual_fidelity == -1\n",
    "vf_1_subset = set_50_index_df[set_50_index_df['visual_fidelity'] == 1].head(10)\n",
    "vf_0_subset = set_50_index_df[set_50_index_df['visual_fidelity'] == -1].head(10)\n",
    "\n",
    "# Combine the subsets to create the \"data\" subset\n",
    "set_50_index_df_vf_filtered = pd.concat([vf_1_subset, vf_0_subset]).reset_index(drop=True)\n",
    "set_50_index_df_vf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf_filtered = data.iloc[set_50_index_df_vf_filtered['idx']].copy()\n",
    "data_vf_filtered['visual_fidelity'] = set_50_index_df_vf_filtered['visual_fidelity'].values\n",
    "data_vf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_vacuous_rationale(question, answer):\n",
    "#     return f\"{question} The answer is {answer}.\"\n",
    "\n",
    "\n",
    "# def calculate_rev_score(data, nli_model, nli_tokenizer):\n",
    "#     rev_scores = []\n",
    "\n",
    "#     for idx, row in data.iterrows():\n",
    "#         question = row['question'].split(' Choices:')[0]\n",
    "#         correct_answer = row['predicted_answer']\n",
    "#         hypothesis = row['hypothesis']\n",
    "        \n",
    "#         generated_rationale = row['generated_rationale']\n",
    "#         vacuous_rationale = construct_vacuous_rationale(question, correct_answer)\n",
    "        \n",
    "#         # Predict using NLI model\n",
    "#         probs_generated = predict_nli(nli_model, nli_tokenizer, generated_rationale, hypothesis)\n",
    "#         probs_vacuous = predict_nli(nli_model, nli_tokenizer, vacuous_rationale, hypothesis)\n",
    "        \n",
    "#         # Calculate log-probabilities for the entailment class (class 2)\n",
    "#         log_prob_generated = torch.log(probs_generated[0][2])\n",
    "#         log_prob_vacuous = torch.log(probs_vacuous[0][2])\n",
    "        \n",
    "#         # Compute REV score\n",
    "#         rev_score = log_prob_generated - log_prob_vacuous\n",
    "#         rev_scores.append(rev_score.item())\n",
    "    \n",
    "#     data['REV_score'] = rev_scores\n",
    "#     return data\n",
    "\n",
    "# # Calculate REV scores\n",
    "# data_with_rev = calculate_rev_score(data, nli_model, nli_tokenizer)\n",
    "# data_with_rev.to_csv('data_with_REV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023773f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_with_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(texts):\n",
    "#     return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# def predict(model, inputs):\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "#     return probabilities\n",
    "\n",
    "# def compute_sim(to_use, labels_to_use):\n",
    "#     xe_correct = (to_use['predicted_labels_xe'] == to_use[labels_to_use]).astype(int)\n",
    "#     x_correct = (to_use['predicted_labels_x'] == to_use[labels_to_use]).astype(int)\n",
    "#     e_correct = (to_use['predicted_labels_e'] == to_use[labels_to_use]).astype(int)\n",
    "\n",
    "#     baseline_correct = x_correct\n",
    "#     leaking = e_correct\n",
    "#     leaked = np.where(leaking == 1)[0]\n",
    "#     nonleaked = np.where(leaking == 0)[0]\n",
    "\n",
    "#     xe_correct_leaked = xe_correct[leaked]\n",
    "#     baseline_correct_leaked = baseline_correct[leaked]\n",
    "#     xe_correct_nonleaked = xe_correct[nonleaked]\n",
    "#     baseline_correct_nonleaked = baseline_correct_nonleaked[nonleaked]\n",
    "\n",
    "#     unweighted_mean = np.mean([\n",
    "#         np.mean(xe_correct_leaked) - np.mean(baseline_correct_leaked),\n",
    "#         np.mean(xe_correct_nonleaked) - np.mean(baseline_correct_nonleaked)\n",
    "#     ])\n",
    "\n",
    "#     nonleaking_diff = np.mean(xe_correct_nonleaked) - np.mean(baseline_correct_nonleaked)\n",
    "#     leaking_diff = np.mean(xe_correct_leaked) - np.mean(baseline_correct_leaked)\n",
    "\n",
    "#     return unweighted_mean, leaking_diff, nonleaking_diff\n",
    "\n",
    "# def run_las_analysis(data, model, tokenizer):\n",
    "#     input_texts = data['question'].tolist()\n",
    "#     explanations = data['generated_rationale'].tolist()\n",
    "\n",
    "#     # Tokenize input texts\n",
    "#     input_encodings = tokenize_function(input_texts)\n",
    "#     explanation_encodings = tokenize_function(explanations)\n",
    "\n",
    "#     # Predict using the model\n",
    "#     input_probs = predict(model, input_encodings)\n",
    "#     explanation_probs = predict(model, explanation_encodings)\n",
    "\n",
    "#     data['predicted_labels_x'] = input_probs.argmax(dim=1).numpy()\n",
    "#     data['predicted_labels_e'] = explanation_probs.argmax(dim=1).numpy()\n",
    "\n",
    "#     # Use both input and explanations for predictions (xe)\n",
    "#     combined_texts = [f\"{text} {exp}\" for text, exp in zip(input_texts, explanations)]\n",
    "#     combined_encodings = tokenize_function(combined_texts)\n",
    "#     combined_probs = predict(model, combined_encodings)\n",
    "\n",
    "#     data['predicted_labels_xe'] = combined_probs.argmax(dim=1).numpy()\n",
    "\n",
    "#     # Compute LAS\n",
    "#     unweighted_mean, leaking_diff, nonleaking_diff = compute_sim(data, 'correct_answer')\n",
    "\n",
    "#     data['LAS_unweighted_mean'] = unweighted_mean\n",
    "#     data['LAS_leaking_diff'] = leaking_diff\n",
    "#     data['LAS_nonleaking_diff'] = nonleaking_diff\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e60447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the analysis\n",
    "# updated_data = run_las_analysis(data, model, tokenizer)\n",
    "# updated_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
