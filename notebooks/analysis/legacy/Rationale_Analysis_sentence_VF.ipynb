{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]\n",
    "    \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data\n",
    "file_path = '../results/Human Annotation of LLaVA+ Rationales.xlsx'\n",
    "\n",
    "if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    model_name = \"LLaVA\"\n",
    "    \n",
    "# Read the specified columns from the sheet\n",
    "columns_to_read = [\n",
    "    'question',\n",
    "    'correct_answer',\n",
    "    'predicted_answer',\n",
    "    'is_correct',\n",
    "    'groundtruth_rationale',\n",
    "    'generated_rationale',\n",
    "    'gen_rationale_distinct_pieces',\n",
    "]\n",
    "\n",
    "if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "else:\n",
    "    data = pd.read_excel(file_path, usecols=columns_to_read)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "assert len(data) == 50, f\"Expected data length of 50, but got {len(data)}\"\n",
    "\n",
    "# Load the CSV file with the new column data\n",
    "new_data = pd.read_csv('/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_mask_LLaVA.csv')\n",
    "\n",
    "# Ensure the new column exists in the new data\n",
    "if 'extracted_rationale_pieces' in new_data.columns:\n",
    "    # Replace the column in the existing DataFrame\n",
    "    data['gen_rationale_distinct_pieces'] = new_data['extracted_rationale_pieces']\n",
    "else:\n",
    "    print(\"The column 'extracted_rationale_pieces' does not exist in the new data.\")\n",
    "    \n",
    "data['image_path'] = data.index.to_series().apply(lambda x: f\"../results/img/{x}.jpg\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the gen_rationale_distinct_pieces column back into a Python list\n",
    "data['gen_rationale_distinct_pieces'] = data['gen_rationale_distinct_pieces'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the DataFrame to ensure the conversion\n",
    "data['gen_rationale_distinct_pieces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gen_rationale_distinct_pieces'][94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "        \n",
    "def calculate_cost(usage, model=\"GPT-4o\", verbose=0):\n",
    "    if model == \"GPT-4o\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    \n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_sentence_fidelity(image_path, sentence, cost_verbose=0):\n",
    "    # Read the image and convert it to base64 format\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    system_prompt = f\"\"\"You are an expert annotator. Given an image and a sentence, you have to evaluate whether whatever is described in the sentence is accurately happening in the image.\n",
    "\n",
    "Respond with one of the following:\n",
    "\n",
    "1: The sentence is talking about something that is **directly** present in the image. For example, the image is that of an airplane and the sentence mentions the airplane. \n",
    "\n",
    "0: The sentence mentions events or elements that may make common sense but are not directly depicted in the image. Also consists of cases where things \"can happen\" or \"may happen\", but are not directly shown in the image. For example, the image is that of an airplane, and the sentence mentions a pilot present. This makes common sense as an airplane usually has a pilot, but in this example, if the pilot is not directly visible in the image, you mark 0.\n",
    "\n",
    "-1: The sentence describes something that is **incorrect or contrary** to the visual content of the image. There is no reasonable basis to infer the presence of these elements, and they do not align with what is shown in the image. For example, the image is that of an airplane, and the sentence mentions a car, which is clearly incorrect.\n",
    "\n",
    "Example Evaluations:\n",
    "- Image Description: You are shown an image that represents the living room of a house. There is a green sofa, a table and a lamp shown in the image.\n",
    "  - Sentence: \"A sofa is present.\" → 1 (A sofa can be directly seen in the image.)\n",
    "  - Sentence: \"Living room can have a TV.\" → 0 (General knowledge, a living room may likely have a TV, but it is not directly shown in this image.)\n",
    "  - Sentence: \"A red sofa is visible.\" → -1 (Contradicts the image content, a green sofa is present instead of a red one.)\n",
    "\n",
    "Carefully identify the key elements in both the sentence and the image to make an accurate evaluation.\"\"\"\n",
    "    \n",
    "    user_input = f\"Sentence: {sentence}\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        # Split the content into score and reason\n",
    "        parts = content.split(': ', 1)\n",
    "\n",
    "        # Assign score and reason, using a default value for reason if it doesn't exist\n",
    "        score = int(parts[0])\n",
    "        reason = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "        return score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three examples: sentences with different visual fidelity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image0_path = \"../results/img/0.jpg\"\n",
    "sentence_0_1 = \"He is standing on the side of the road.\"\n",
    "gpt_sentence_fidelity(image0_path, sentence_0_1, cost_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3_path = \"../results/img/3.jpg\"\n",
    "sentence_3_1 = \"Airport workers have access to designated parking areas within the airport premises, which allows them to park their vehicles close to their workstations.\"\n",
    "gpt_sentence_fidelity(image3_path, sentence_3_1, cost_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image16_path = \"../results/img/16.jpg\"\n",
    "sentence_16_0 = \"The computer near the woman in blue is an Acer computer.\"\n",
    "gpt_sentence_fidelity(image16_path, sentence_16_0, cost_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fidelity(data, image_path_column, rationale_column):\n",
    "    # Make a copy of the original DataFrame to avoid SettingWithCopyWarning\n",
    "    data_copy = data.copy()\n",
    "    score_results = []\n",
    "    reason_results = []\n",
    "    for index, row in data_copy.iterrows():\n",
    "        image_path = row[image_path_column]\n",
    "        rationale_pieces = row[rationale_column]\n",
    "        fidelity_scores = []\n",
    "        fidelity_reasons = []\n",
    "        for sentence in rationale_pieces:\n",
    "            score, reason = gpt_sentence_fidelity(image_path, sentence)\n",
    "            fidelity_scores.append(score)\n",
    "            fidelity_reasons.append(reason)\n",
    "        score_results.append(fidelity_scores)\n",
    "        reason_results.append(fidelity_reasons)\n",
    "    \n",
    "    # Add the results as a new column to the DataFrame using .loc\n",
    "    data_copy.loc[:, 'sentence_fidelity_scores'] = score_results\n",
    "    data_copy.loc[:, 'sentence_fidelity_reasons'] = reason_results\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_sentence_VF = evaluate_fidelity(data, 'image_path', 'gen_rationale_distinct_pieces')\n",
    "data_with_sentence_VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_dataframe_to_csv(dataframe, model_name):\n",
    "    # Define the directory and filename\n",
    "    directory = \"data_with_sentence_VF\"\n",
    "    filename = f\"data_with_sentence_VF_{model_name}.csv\"\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "    print(f\"Dataframe successfully saved to {file_path}.\")\n",
    "\n",
    "save_dataframe_to_csv(data_with_sentence_VF, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Now read the file data_with_sentence_VF/data_with_sentence_VF_LLaVA.csv.\n",
    "data = pd.read_csv('data_with_sentence_VF/data_with_sentence_VF_LLaVA.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a specific column\n",
    "# column_to_export = data[['sentence_fidelity_scores', 'sentence_fidelity_reasons']]\n",
    "\n",
    "# # Export the selected column to an Excel file\n",
    "# column_to_export.to_excel('temp.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data merge with the human annotations\n",
    "human_annotated_s_VF = pd.read_excel('../results/Human Annotation of LLaVA+ Rationales.xlsx', header=1, usecols=['Sentence-wise Visual Fidelity']).dropna()\n",
    "# Reindex the human_annotated_s_VF DataFrame to match the data DataFrame\n",
    "human_annotated_s_VF.index = data.index\n",
    "data['Sentence-wise Visual Fidelity'] = human_annotated_s_VF['Sentence-wise Visual Fidelity']\n",
    "data.rename(columns={'sentence_fidelity_scores': 'Sentence-wise Visual Fidelity (Automated)', \n",
    "                     'Sentence-wise Visual Fidelity': 'Sentence-wise Visual Fidelity (Human)'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Function to parse string representation of list to actual list\n",
    "def parse_list_string(list_str):\n",
    "    return ast.literal_eval(list_str)\n",
    "\n",
    "# Function to calculate VF score\n",
    "def calculate_vf_score(row, col_name):\n",
    "    vf_scores = parse_list_string(row[col_name])\n",
    "    vf_score_percentage = sum(vf_scores) / len(vf_scores) if len(vf_scores) > 0 else 0\n",
    "    return vf_score_percentage\n",
    "\n",
    "# Adding new columns for VF scores\n",
    "data[\"VF Score (Automated)\"] = data.apply(calculate_vf_score, col_name=\"Sentence-wise Visual Fidelity (Automated)\", axis=1)\n",
    "data[\"VF Score (Human)\"] = data.apply(calculate_vf_score, col_name=\"Sentence-wise Visual Fidelity (Human)\", axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Calculate the correlation between the automated and human VF scores\n",
    "correlation = data[\"VF Score (Automated)\"].corr(data[\"VF Score (Human)\"])\n",
    "print(f\"Correlation between Automated and Human VF Scores: {correlation:.4f}\")\n",
    "\n",
    "# Function to add jitter\n",
    "def add_jitter(arr, jitter_amount=0):\n",
    "    return arr + np.random.uniform(-jitter_amount, jitter_amount, arr.shape)\n",
    "\n",
    "# Add jitter to the data\n",
    "x_jittered = add_jitter(data[\"VF Score (Automated)\"])\n",
    "y_jittered = add_jitter(data[\"VF Score (Human)\"])\n",
    "\n",
    "# Plot the scatter plot with jittered data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_jittered, y_jittered, color='blue', alpha=0.3)\n",
    "\n",
    "# The best-fit line\n",
    "sns.regplot(x=data[\"VF Score (Automated)\"], y=data[\"VF Score (Human)\"], scatter=False, color='red')\n",
    "\n",
    "# Format the x-ticks to limit the number of decimals\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "\n",
    "plt.title(\"Automated vs. Human Visual Fidelity Scores\")\n",
    "plt.xlabel(\"VF Score (Automated)\")\n",
    "plt.ylabel(\"VF Score (Human)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Create DataFrame\n",
    "tmp_df1 = pd.DataFrame()\n",
    "flattened_sVF_scores = pd.DataFrame()\n",
    "\n",
    "# Convert the string representation of lists into actual lists\n",
    "tmp_df1['Sentence-wise Visual Fidelity (Human)'] = [ast.literal_eval(x) for x in data['Sentence-wise Visual Fidelity (Human)']]\n",
    "tmp_df1['Sentence-wise Visual Fidelity (Automated)'] = [ast.literal_eval(x) for x in data['Sentence-wise Visual Fidelity (Automated)']]\n",
    "\n",
    "# Flatten the lists\n",
    "flattened_sVF_scores['Sentence-wise Visual Fidelity (Human)'] = [item for sublist in tmp_df['Sentence-wise Visual Fidelity (Human)'] for item in sublist]\n",
    "flattened_sVF_scores['Sentence-wise Visual Fidelity (Automated)'] = [item for sublist in tmp_df['Sentence-wise Visual Fidelity (Automated)'] for item in sublist]\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation = flattened_sVF_scores.corr().iloc[0, 1]\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_sVF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculating confusion matrix\n",
    "conf_matrix = confusion_matrix(flattened_sVF_scores['Sentence-wise Visual Fidelity (Human)'], flattened_sVF_scores['Sentence-wise Visual Fidelity (Automated)'], labels=[-1, 0, 1])\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=['-1', '0', '1'], yticklabels=['-1', '0', '1'])\n",
    "plt.xlabel('Automated Scores')\n",
    "plt.ylabel('Human Scores')\n",
    "plt.title('Confusion Matrix: Sentence-wise Visual Fidelity (Human vs Automated)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, calculate the % of cases with at least one VF subscore -1.\n",
    "def calculate_vf_mismatch(row, col_name):\n",
    "    vf_scores = parse_list_string(row[col_name])\n",
    "    mismatch = -1 in vf_scores\n",
    "    return mismatch\n",
    "\n",
    "# Return two scores, one for the automated and one for the human VF scores\n",
    "automated_vf_mismatch = data.apply(calculate_vf_mismatch, col_name=\"Sentence-wise Visual Fidelity (Automated)\", axis=1)\n",
    "human_vf_mismatch = data.apply(calculate_vf_mismatch, col_name=\"Sentence-wise Visual Fidelity (Human)\", axis=1)\n",
    "print(f\"Automated VF Mismatch: {automated_vf_mismatch.mean() * 100}%\")\n",
    "print(f\"Human VF Mismatch: {human_vf_mismatch.mean() * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the VF scores\n",
    "automated_vf_avg = data[\"VF Score (Automated)\"].mean()\n",
    "human_vf_avg = data[\"VF Score (Human)\"].mean()\n",
    "print(f\"Automated VF Average: {automated_vf_avg:.3f}\")\n",
    "print(f\"Human VF Average: {human_vf_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_dataframe_to_csv(dataframe, model_name):\n",
    "    # Define the directory and filename\n",
    "    directory = \"data_with_sentence_VF\"\n",
    "    filename = f\"data_with_sentence_VF_{model_name}.csv\"\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "    print(f\"Dataframe successfully saved to {file_path}.\")\n",
    "\n",
    "save_dataframe_to_csv(data, \"LLaVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
