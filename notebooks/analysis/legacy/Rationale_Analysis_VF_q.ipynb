{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]\n",
    "    \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_type = \"validation\" # or training\n",
    "\n",
    "# Read data\n",
    "file_path = '../results/Human Annotation of LLaVA+ Rationales.xlsx'\n",
    "\n",
    "if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    model_name = \"LLaVA\"\n",
    "    \n",
    "if dataset_type == \"training\":\n",
    "    # Read the specified columns from the sheet\n",
    "    columns_to_read = [\n",
    "        'question',\n",
    "        'correct_answer',\n",
    "        'predicted_answer',\n",
    "        'is_correct',\n",
    "        'groundtruth_rationale',\n",
    "        'generated_rationale',\n",
    "        'gen_rationale_distinct_pieces',\n",
    "    ]\n",
    "    data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "elif dataset_type == \"validation\":\n",
    "    columns_to_read = [\n",
    "        'question',\n",
    "        'correct_answer',\n",
    "        'predicted_qa_answer',\t\n",
    "        'qa_is_correct',\t\n",
    "        'groundtruth_rationale',\t\n",
    "        'direct_answer',\t\n",
    "        'one_step_question',\t\n",
    "        'one_step_answer',\t\n",
    "        'one_step_is_correct',\n",
    "        'one_step_rationale'\n",
    "    ]\n",
    "    data = pd.read_excel(file_path, usecols=columns_to_read, sheet_name='val_500_set')\n",
    "    \n",
    "data['image_path'] = data.index.to_series().apply(lambda x: f\"../results/img/{dataset_type}/{x}.jpg\")\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only these examples: bike in front of the bus (43), cat growling at the car (25), number 10 example (32),\n",
    "# dog walking in the rain (35), and firefighters in front of the building (46)\n",
    "data_small = data[data.index.isin([43, 25, 32, 35, 46])].copy()\n",
    "data_small.drop(columns=['gen_rationale_distinct_pieces', 'groundtruth_rationale'], inplace=True)\n",
    "data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index 9, 16, 25, 32, 0, 29, 27, 1\n",
    "data_combined = data.loc[[9, 16, 25, 32, 0, 29, 27, 1]].copy()\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "        \n",
    "def calculate_cost(usage, model=\"GPT-4o\", verbose=0):\n",
    "    if model == \"GPT-4o\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    \n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_gen_vf_questions(row, rationale_column_name, cost_verbose=0):\n",
    "    # # Read the image and convert it to base64 format\n",
    "    # with open(row['image_path'], \"rb\") as image_file:\n",
    "    #     encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    system_prompt = f\"\"\"Given a question, answer, and rationale, generate a list of all possible yes/no questions that can be answered by examining the corresponding image associated with the original question. Focus on creating questions that can be visually verified or refuted based on the details provided in the rationale. Ensure the questions are specific and directly pertain to aspects that are visually relevant and mentioned in the rationale. Avoid generating questions about elements that the rationale explicitly states are not relevant or present. Also avoid generating questions that check for the same visual detail. Questions should be verifiable with a \"yes\" answer rather than a \"no.\"\n",
    "\n",
    "Here is one example:\n",
    "Input: \n",
    "Question: Why is the person wearing a helmet?\n",
    "Answer: For safety\n",
    "Rationale: The person is wearing a helmet because they are riding a bicycle on a busy city street. Helmets are commonly used to protect against head injuries in case of accidents, especially in areas with heavy traffic.\n",
    "\n",
    "Good Questions:\n",
    "1. Is the person wearing a helmet while riding a bicycle?\n",
    "Reason: This question is directly answerable by observing whether the person on the bicycle is wearing a helmet in the image. \n",
    "2. Is the street in the image busy with traffic?\n",
    "Reason: This question can be visually verified by looking at the amount of traffic on the street in the image.\n",
    "\n",
    "Bad Questions:\n",
    "1. Is the person wearing the helmet because they are concerned about head injuries?\n",
    "Reason: This question is not good because it assumes the person’s intentions or concerns, which cannot be visually verified from the image.\n",
    "2. Does wearing a helmet suggest that the person is highly safety-conscious?\n",
    "Reason: This question relies on inference and external knowledge about the person’s mindset, rather than on observable details from the image.\n",
    "3. Is the person not wearing sunglasses?\n",
    "Reason: This question is not good because it asks for verification by absence and can only be answered with a \"no,\" which is not the preferred type of question.\n",
    "\n",
    "Respond with a list of (good) questions (without the reasons).\"\"\"\n",
    "    \n",
    "    user_input = f\"\"\"Question: {row['question']}\n",
    "Answer: {row['predicted_answer']}\n",
    "Rationale: {row[rationale_column_name]}\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                    # NO IMAGE PROVIDED\n",
    "                    # {\n",
    "                    #     \"type\": \"image_url\",\n",
    "                    #     \"image_url\": {\n",
    "                    #         \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                    #     }\n",
    "                    # }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # content is a list of questions, separated by a newline character\n",
    "        # 1. ... \\n 2. ... \\n 3. ...\n",
    "\n",
    "        # Split the content into individual questions and return a python list\n",
    "        parts = content.split('\\n')\n",
    "        questions = [part.split('. ')[1] for part in parts]\n",
    "\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One example\n",
    "gpt_gen_vf_questions(data.iloc[0], cost_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_small\n",
    "df = data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'vf_questions' column if it doesn't exist\n",
    "if 'vf_questions' not in df.columns:\n",
    "    df['vf_questions'] = None  # Or use an empty list []\n",
    "    \n",
    "for idx, row in df.iterrows():\n",
    "    questions = gpt_gen_vf_questions(row, cost_verbose=1)\n",
    "    # add the questions to the dataframe\n",
    "    df.at[idx, 'vf_questions'] = questions\n",
    "    print(f\"Questions for example {idx}:\")\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"{i+1}. {question}\")\n",
    "    print()\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'vf_questions' column if it doesn't exist\n",
    "if 'vf_questions_GPT_r' not in df.columns:\n",
    "    df['vf_questions_GPT_r'] = None  # Or use an empty list []\n",
    "    \n",
    "for idx, row in df.iterrows():\n",
    "    questions = gpt_gen_vf_questions(row, \"gpt-4o_rationale\", cost_verbose=1)\n",
    "    # add the questions to the dataframe\n",
    "    df.at[idx, 'vf_questions_GPT_r'] = questions\n",
    "    print(f\"Questions for example {idx}:\")\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"{i+1}. {question}\")\n",
    "    print()\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LLaVA to check vf_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining, LlavaForConditionalGeneration\n",
    "\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def inference(image, question, mode = \"qa\", hyperparams = None, max_new_tokens=40):\n",
    "  # inputs = processor(images=image, text=make_prompt({'question': question}), return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "  inputs = processor(text=question, images=image, return_tensors=\"pt\").to(\"cuda\")#, torch.float16)\n",
    "  if mode == \"qa\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=-1,\n",
    "                               max_new_tokens=max_new_tokens)\n",
    "                              #  max_length=max_new_tokens)\n",
    "  elif mode == \"rationale\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=1.1, # choose from [1, 1.5, 2]\n",
    "                               max_new_tokens=max_new_tokens,\n",
    "                               )\n",
    "                            #  max_length=max_new_tokens,\n",
    "                            #  )\n",
    "  elif mode == \"vf_question\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=-1,\n",
    "                               max_new_tokens=1,\n",
    "                               )\n",
    "\n",
    "  # Decode and print the answer\n",
    "  answer = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "  answer = answer.split(question.split(\"<image>\")[1])[1].strip()\n",
    "  return answer\n",
    "\n",
    "\n",
    "image0 = Image.open(df.iloc[0]['image_path'])\n",
    "question0 = f\"<image>\\nUSER:Question: {df.iloc[0]['vf_questions'][0]}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "inference(image0, question0, mode=\"vf_question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_vf_questions(data):\n",
    "    # if 'vf_answers_LLaVA' not in data.columns:\n",
    "    #     data['vf_answers_LLaVA'] = None\n",
    "    if 'vf_answers_LLaVA_GPT_r' not in data.columns:\n",
    "        data['vf_answers_LLaVA_GPT_r'] = None\n",
    "    for idx, row in data.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        answer_list = []\n",
    "        # Image\n",
    "        display(image)\n",
    "        # for i, question in enumerate(row['vf_questions']):\n",
    "        #     question = f\"<image>\\nUSER:Question: {question}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "        #     answer = inference(image, question, mode=\"vf_question\")\n",
    "        #     answer_list.append(answer)\n",
    "        #     # Print image, question, and answer\n",
    "        #     print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        # data.at[idx, 'vf_answers_LLaVA'] = answer_list\n",
    "        for i, question in enumerate(row['vf_questions_GPT_r']):\n",
    "            question = f\"<image>\\nUSER:Question: {question}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "            answer = inference(image, question, mode=\"vf_question\")\n",
    "            answer_list.append(answer)\n",
    "            # Print image, question, and answer\n",
    "            print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        data.at[idx, 'vf_answers_LLaVA_GPT_r'] = answer_list\n",
    "    return data\n",
    "\n",
    "df = answer_vf_questions(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to a xlsx file\n",
    "df.to_excel(\"data_balanced_vf_questions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPT to check vf_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_answer_vf_questions(question, image_path, cost_verbose=0):\n",
    "    # Read the image and convert it to base64 format\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    user_input = f\"\"\"Question: {question}. Based on the information provided in the image, answer with 'yes' or 'no'. Provide one-word answer only.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-2024-08-06\",\n",
    "        \"messages\": [\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\n",
    "            #             \"type\": \"text\",\n",
    "            #             \"text\": system_prompt\n",
    "            #         }\n",
    "            #     ]\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, verbose=cost_verbose)\n",
    "        content = response.json()['choices'][0]['message']['content'].strip().strip('.')\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def answer_vf_questions(data):\n",
    "    # if 'vf_answers_GPT' not in data.columns:\n",
    "    #     data['vf_answers_GPT'] = None\n",
    "    if 'vf_answers_GPT_GPT_r' not in data.columns:\n",
    "        data['vf_answers_GPT_GPT_r'] = None\n",
    "    for idx, row in data.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        answer_list = []\n",
    "        # Image\n",
    "        display(image)\n",
    "        # for i, question in enumerate(row['vf_questions']):\n",
    "        #     answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "        #     answer_list.append(answer)\n",
    "        #     # Print image, question, and answer\n",
    "        #     print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        # data.at[idx, 'vf_answers_GPT'] = answer_list\n",
    "        for i, question in enumerate(row['vf_questions_GPT_r']):\n",
    "            answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "            answer_list.append(answer)\n",
    "            # Print image, question, and answer\n",
    "            print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        data.at[idx, 'vf_answers_GPT_GPT_r'] = answer_list\n",
    "    return data\n",
    "\n",
    "df = answer_vf_questions(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to a xlsx file\n",
    "# df.to_excel(\"data_balanced_vf_questions.xlsx\", index=False)\n",
    "df.to_excel(\"data_balanced_vf_questions_GPT_r.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the correlation between vf_answers_LLaVA and vf_answers_GPT\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Flatten the lists\n",
    "flat_llava_answers = [item for sublist in df['vf_answers_LLaVA_GPT_r'] for item in sublist]\n",
    "flat_gpt_answers = [item for sublist in df['vf_answers_GPT_GPT_r'] for item in sublist]\n",
    "\n",
    "# Map 'Yes' to 1 and 'No' to 0 for correlation calculation\n",
    "mapping = {'Yes': 1, 'No': 0}\n",
    "flat_llava_mapped = [mapping[answer] for answer in flat_llava_answers]\n",
    "flat_gpt_mapped = [mapping[answer] for answer in flat_gpt_answers]\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr, p_value = spearmanr(flat_llava_mapped, flat_gpt_mapped)\n",
    "\n",
    "spearman_corr, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "cohen_kappa = cohen_kappa_score(flat_llava_mapped, flat_gpt_mapped)\n",
    "\n",
    "cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
