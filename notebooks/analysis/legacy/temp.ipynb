{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file \"../results/Human Annotation of LLaVA+ Rationales.xlsx\" and open the sheet \"LLAVA-1.5 Rationales-val 500 se\"\n",
    "\n",
    "# Its columns look like this: reindex\tquestion\tcorrect_answer\tpredicted_answer\tis_correct\tgenerated_rationale\timage_path\tvf_questions\tvf_answers_GPT\thypothesis\talternative_hypotheses\tgen_rationale_mask\talt_ent_prob\tstrict_sim\n",
    "\n",
    "# We want a new column called ent_prob that that uses the following fuctions:\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure the model for 8-bit loading\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "if 'nli_tokenizer' not in globals():\n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "if 'nli_model' not in globals():\n",
    "    nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")\n",
    "    \n",
    "def calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            no_entail_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, no_entail_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[0]\n",
    "\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    predicted_answer = str(predicted_answer)\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if use_pieces:\n",
    "            premise = row['concat_rationale_pieces_mask'] if use_mask else row['concat_rationale_pieces']\n",
    "        else: \n",
    "            premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        \n",
    "        hypothesis = row[hypothesis_col]\n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        support = entail_prob > threshold \n",
    "        if entail_prob < threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {entail_prob}\")\n",
    "        support_scores.append({\n",
    "            'entail_prob': entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "    return support_scores\n",
    "\n",
    "# Load the Excel file and specific sheet\n",
    "file_path = \"../results/Human Annotation of LLaVA+ Rationales.xlsx\"\n",
    "sheet_name = \"LLAVA-1.5 Rationales-val 500 se\"\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "# Apply the entailment probability calculation to each row and create a new column\n",
    "data['ent_prob'] = data.progress_apply(\n",
    "    lambda row: calc_support_prob(row['gen_rationale_mask'], row['hypothesis'], nli_model, nli_tokenizer), axis=1\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data as temp.xlsx\n",
    "data.to_excel('ent_prob_human_anno.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('ent_prob_human_anno.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert scientific notation to fixed-point notation\n",
    "def convert_to_fixed_point(alt_ent_prob):\n",
    "    # Convert the string to a Python list\n",
    "    numbers = eval(alt_ent_prob)\n",
    "    # Format each number to 4 decimal points\n",
    "    formatted_numbers = [f\"{num:.4f}\" for num in numbers]\n",
    "    # Convert the list back to string format\n",
    "    return str(formatted_numbers)\n",
    "\n",
    "# Apply the conversion function to the column\n",
    "data['alt_ent_prob'] = data['alt_ent_prob'].apply(convert_to_fixed_point)\n",
    "\n",
    "# Display the updated data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine strict_sim\n",
    "def calculate_strict_sim(row):\n",
    "    ent_prob = row['ent_prob']\n",
    "    # Convert string representation of list to Python list and cast elements to float\n",
    "    alt_ent_prob = [float(item) for item in eval(row['alt_ent_prob'])]\n",
    "    return ent_prob >= 0.5 and all(item < 0.5 for item in alt_ent_prob)\n",
    "\n",
    "# Update the strict_sim column\n",
    "data['strict_sim'] = data.apply(calculate_strict_sim, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('ent_prob_human_anno_v2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
