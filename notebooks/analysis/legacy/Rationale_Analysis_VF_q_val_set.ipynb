{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]\n",
    "    \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_type = \"validation\" # or training\n",
    "\n",
    "# Read data\n",
    "# file_path = '../results/Human Annotation of LLaVA+ Rationales.xlsx'\n",
    "file_path = '../results/llava-1.5-7b-hf_val_500_two_steps.xlsx'\n",
    "\n",
    "model_name = \"LLaVA\"\n",
    "    \n",
    "if dataset_type == \"training\":\n",
    "    # Read the specified columns from the sheet\n",
    "    columns_to_read = [\n",
    "        'question',\n",
    "        'correct_answer',\n",
    "        'predicted_answer',\n",
    "        'is_correct',\n",
    "        'groundtruth_rationale',\n",
    "        'generated_rationale',\n",
    "        'gen_rationale_distinct_pieces',\n",
    "    ]\n",
    "    data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "elif dataset_type == \"validation\":\n",
    "    # columns_to_read = [\n",
    "    #     'question',\n",
    "    #     'correct_answer',\n",
    "    #     'predicted_qa_answer',\t\n",
    "    #     'qa_is_correct',\t\n",
    "    #     'groundtruth_rationale',\t\n",
    "    #     'direct_answer',\t\n",
    "    #     'one_step_question',\t\n",
    "    #     'one_step_answer',\t\n",
    "    #     'one_step_is_correct',\n",
    "    #     'one_step_rationale',\n",
    "    #     'image_path'\n",
    "    # ]\n",
    "    columns_to_read = [\n",
    "        'question',\n",
    "        'correct_answer',\n",
    "        'predicted_answer',\t\n",
    "        'is_correct',\t\n",
    "        'generated_rationale',\n",
    "        'image_path'\n",
    "    ]\n",
    "    # data = pd.read_excel(file_path, usecols=columns_to_read, sheet_name='val_500_set')\n",
    "    data = pd.read_excel(file_path, usecols=columns_to_read)\n",
    "    \n",
    "# data['image_path'] = data.index.to_series().apply(lambda x: f\"../results/img/{dataset_type}/{x}.jpg\")\n",
    "data['image_path_split'] = data['image_path'].apply(lambda x: x.split('//'))\n",
    "data['image_path'] = data['image_path_split'].apply(lambda x: f\"../results/img/validation/{x[1]}\")\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.drop('image_path_split', axis=1, inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['generated_rationale'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file to store the total cost\n",
    "COST_FILE = \"total_cost.txt\"\n",
    "\n",
    "def read_total_cost():\n",
    "    if os.path.exists(COST_FILE):\n",
    "        with open(COST_FILE, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "            return float(content) if not content == \"\" else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def write_total_cost(cost):\n",
    "    prev_cost = read_total_cost()\n",
    "    new_total_cost = prev_cost + cost\n",
    "    with open(COST_FILE, \"w\") as file:\n",
    "        file.write(f\"{new_total_cost}\")\n",
    "        \n",
    "def calculate_cost(usage, model, verbose=0):\n",
    "    if model == \"gpt-4o-2024-05-13\":\n",
    "        input_cost_per_token = 0.005 / 1000\n",
    "        output_cost_per_token = 0.015 / 1000\n",
    "    if model == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_token = 0.0025 / 1000\n",
    "        output_cost_per_token = 0.010 / 1000\n",
    "    \n",
    "    input_tokens = usage['prompt_tokens']\n",
    "    output_tokens = usage['completion_tokens']\n",
    "    cost = (input_tokens * input_cost_per_token) + (output_tokens * output_cost_per_token)\n",
    "    if verbose: print(f\"The cost incurred is ${cost:.3f}\")\n",
    "    write_total_cost(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_gen_vf_questions(row, rationale_column_name, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-2024-08-06\"\n",
    "    # # Read the image and convert it to base64 format\n",
    "    # with open(row['image_path'], \"rb\") as image_file:\n",
    "    #     encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    system_prompt = f\"\"\"You will be shown a question about an image, along with an answer, and a rationale that explains the answer based on details from the image. Your task is to generate a list of yes/no questions that verify the details about the image that are **explicitly** mentioned in the rationale. Your questions should be phrased such that the answer to that question being yes means that the detail in the rationale is correct. Focus on creating questions that can be visually verified or refuted based on the details provided in the rationale. Ensure the questions are specific and directly pertain to aspects that are visually relevant and mentioned in the rationale. Avoid generating questions about elements that are not mentioned in the rationale, or the rationale explicitly states are not relevant or present. Also avoid generating multiple questions that check for the same visual detail.\n",
    "\n",
    "Here is one example:\n",
    "Input: \n",
    "Question: Why is the person wearing a helmet?\n",
    "Answer: For safety\n",
    "Rationale: The person is wearing a helmet because they are riding a bicycle on a busy city street. Helmets are commonly used to protect against head injuries in case of accidents, especially in areas with heavy traffic.\n",
    "\n",
    "Good Questions:\n",
    "1. Is the person wearing a helmet while riding a bicycle?\n",
    "Reason: This question is directly answerable by observing whether the person on the bicycle is wearing a helmet in the image. \n",
    "2. Is the street in the image busy with traffic?\n",
    "Reason: This question can be visually verified by looking at the amount of traffic on the street in the image.\n",
    "\n",
    "Bad Questions:\n",
    "1. Is the person wearing the helmet because they are concerned about head injuries?\n",
    "Reason: This question is not good because it assumes the person’s intentions or concerns, which cannot be visually verified from the image.\n",
    "2. Does wearing a helmet suggest that the person is highly safety-conscious?\n",
    "Reason: This question relies on inference and external knowledge about the person’s mindset, rather than on observable details from the image.\n",
    "3. Is there any indication that the person is wearing a helmet for safety reasons?\n",
    "Reason: This question verifies the answer to the original question, rather than verifying a detail about the image that's mentioned in the rationale.\n",
    "4. Is the person wearing a safety vest?\n",
    "Reason: This question is not good because it tries to verify details about the image that are not explicitly mentioned in the rationale.\n",
    "5. Is the person not wearing sunglasses?\n",
    "Reason: This question is not good because it asks for verification by absence and can only be answered with a \"no,\" which is not the preferred type of question.\n",
    "\n",
    "Respond with a list of (good) questions (without the reasons), starting from '1. '\"\"\"\n",
    "    \n",
    "#     user_input = f\"\"\"Question: {row['question']}\n",
    "# Answer: {row['one_step_answer']}\n",
    "# Rationale: {row[rationale_column_name]}\"\"\"\n",
    "\n",
    "    user_input = f\"\"\"Question: {row['question']}\n",
    "Answer: {row['predicted_answer']}\n",
    "Rationale: {row[rationale_column_name]}\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        \n",
    "        content = response.json()['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # content is a list of questions, separated by a newline character\n",
    "        # 1. ... \\n 2. ... \\n 3. ...\n",
    "\n",
    "        try:\n",
    "            # Split the content into individual questions and return a python list\n",
    "            if '\\n' in content:\n",
    "                parts = content.split('\\n')\n",
    "            else:\n",
    "                parts = content.split('. ')  # Split by \". \" as a fallback\n",
    "            questions = []\n",
    "\n",
    "            for part in parts:\n",
    "                try:\n",
    "                    # Attempt to split and take the second part\n",
    "                    question = part.split('. ')[1]\n",
    "                    questions.append(question)\n",
    "                except IndexError:\n",
    "                    # If there's an issue with splitting, add the entire part or handle as needed\n",
    "                    questions.append(part)\n",
    "        except Exception as e:\n",
    "            questions = [content]\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One example\n",
    "# gpt_gen_vf_questions(data.iloc[0], 'one_step_rationale', cost_verbose=1)\n",
    "gpt_gen_vf_questions(data.iloc[0], 'generated_rationale', cost_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'vf_questions' column if it doesn't exist\n",
    "if 'vf_questions' not in df.columns:\n",
    "    df['vf_questions'] = None  # Or use an empty list []\n",
    "    \n",
    "for idx, row in df.iterrows():\n",
    "#     questions = gpt_gen_vf_questions(row, 'one_step_rationale')\n",
    "    questions = gpt_gen_vf_questions(row, 'generated_rationale')\n",
    "    # add the questions to the dataframe\n",
    "    df.at[idx, 'vf_questions'] = questions\n",
    "    print(f\"Questions for example {idx}:\")\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"{i+1}. {question}\")\n",
    "    print()\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to a xlsx file\n",
    "df.to_excel(\"data_balanced_vf_questions_val_2step.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LLaVA to check vf_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining, LlavaForConditionalGeneration\n",
    "\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "model.to(\"cuda:9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def inference(image, question, mode = \"qa\", hyperparams = None, max_new_tokens=40):\n",
    "  # inputs = processor(images=image, text=make_prompt({'question': question}), return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "  inputs = processor(text=question, images=image, return_tensors=\"pt\").to(\"cuda:9\")#, torch.float16)\n",
    "  if mode == \"qa\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=-1,\n",
    "                               max_new_tokens=max_new_tokens)\n",
    "                              #  max_length=max_new_tokens)\n",
    "  elif mode == \"rationale\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=1.1, # choose from [1, 1.5, 2]\n",
    "                               max_new_tokens=max_new_tokens,\n",
    "                               )\n",
    "                            #  max_length=max_new_tokens,\n",
    "                            #  )\n",
    "  elif mode == \"vf_question\":\n",
    "      outputs = model.generate(**inputs,\n",
    "                               num_beams=5,\n",
    "                               length_penalty=-1,\n",
    "                               max_new_tokens=1,\n",
    "                               )\n",
    "\n",
    "  # Decode and print the answer\n",
    "  answer = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "  answer = answer.split(question.split(\"<image>\")[1])[1].strip()\n",
    "  return answer\n",
    "\n",
    "\n",
    "image0 = Image.open(df.iloc[0]['image_path'])\n",
    "question0 = f\"<image>\\nUSER:Question: {df.iloc[0]['vf_questions'][0]}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "inference(image0, question0, mode=\"vf_question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_vf_questions(data):\n",
    "    # if 'vf_answers_LLaVA' not in data.columns:\n",
    "    #     data['vf_answers_LLaVA'] = None\n",
    "    if 'vf_answers_LLaVA_GPT_r' not in data.columns:\n",
    "        data['vf_answers_LLaVA_GPT_r'] = None\n",
    "    for idx, row in data.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        answer_list = []\n",
    "        # Image\n",
    "        display(image)\n",
    "        # for i, question in enumerate(row['vf_questions']):\n",
    "        #     question = f\"<image>\\nUSER:Question: {question}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "        #     answer = inference(image, question, mode=\"vf_question\")\n",
    "        #     answer_list.append(answer)\n",
    "        #     # Print image, question, and answer\n",
    "        #     print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        # data.at[idx, 'vf_answers_LLaVA'] = answer_list\n",
    "        for i, question in enumerate(row['vf_questions_GPT_r']):\n",
    "            question = f\"<image>\\nUSER:Question: {question}. Answer with 'yes' or 'no'.\\nASSISTANT:\"\n",
    "            answer = inference(image, question, mode=\"vf_question\")\n",
    "            answer_list.append(answer)\n",
    "            # Print image, question, and answer\n",
    "            print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        data.at[idx, 'vf_answers_LLaVA_GPT_r'] = answer_list\n",
    "    return data\n",
    "\n",
    "df = answer_vf_questions(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to a xlsx file\n",
    "df.to_excel(\"data_balanced_vf_questions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPT to check vf_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt_answer_vf_questions(question, image_path, cost_verbose=0):\n",
    "    model_name = \"gpt-4o-2024-08-06\"\n",
    "    # Read the image and convert it to base64 format\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    user_input = f\"\"\"Question: {question}. Based on the information provided in the image, answer with 'yes' or 'no'. Provide one-word answer only.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": [\n",
    "            #         {\n",
    "            #             \"type\": \"text\",\n",
    "            #             \"text\": system_prompt\n",
    "            #         }\n",
    "            #     ]\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_input,\n",
    "                    }, \n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(response.json())\n",
    "        return None\n",
    "    else:\n",
    "        usage = response.json()['usage']\n",
    "        calculate_cost(usage, model_name, verbose=cost_verbose)\n",
    "        content = response.json()['choices'][0]['message']['content'].strip().strip('.')\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def answer_vf_questions(data):\n",
    "    if 'vf_answers_GPT' not in data.columns:\n",
    "        data['vf_answers_GPT'] = None\n",
    "    # if 'vf_answers_GPT_GPT_r' not in data.columns:\n",
    "    #     data['vf_answers_GPT_GPT_r'] = None\n",
    "    for idx, row in data.iterrows():\n",
    "        image = Image.open(row['image_path'])\n",
    "        answer_list = []\n",
    "        # Image\n",
    "        display(image)\n",
    "        for i, question in enumerate(row['vf_questions']):\n",
    "            answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "            answer_list.append(answer)\n",
    "            # Print image, question, and answer\n",
    "            print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        data.at[idx, 'vf_answers_GPT'] = answer_list\n",
    "        # for i, question in enumerate(row['vf_questions_GPT_r']):\n",
    "        #     answer = gpt_answer_vf_questions(question, row['image_path'], cost_verbose=1)\n",
    "        #     answer_list.append(answer)\n",
    "        #     # Print image, question, and answer\n",
    "        #     print(f\"Image {idx}, Question {i+1}: {question}\\nAnswer: {answer}\")\n",
    "        # data.at[idx, 'vf_answers_GPT_GPT_r'] = answer_list\n",
    "    return data\n",
    "\n",
    "df = answer_vf_questions(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_excel('data_balanced_vf_questions_val_set_GPT_r.xlsx')\n",
    "# df.drop('vf_answers_LLaVA_GPT_r', axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to a xlsx file\n",
    "output_path = \"data_vf_questions_val_2steps.xlsx\"\n",
    "# df.to_excel(\"data_balanced_vf_questions.xlsx\", index=False)\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "import os\n",
    "\n",
    "# First, save the dataframe to an excel file (without images) to manipulate it with openpyxl\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Load the saved excel file\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Add images to the new column in the excel file\n",
    "for index, row in df.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    # Check if the image file exists before adding\n",
    "    if os.path.exists(img_path):\n",
    "        img = ExcelImage(img_path)\n",
    "        img_cell = f\"R{index + 2}\"  # Placing the image starting from row 2, column R\n",
    "        ws.add_image(img, img_cell)\n",
    "\n",
    "# Save the updated excel file with images\n",
    "output_path_with_images = output_path\n",
    "wb.save(output_path_with_images)\n",
    "\n",
    "output_path_with_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skippppppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the correlation between vf_answers_LLaVA and vf_answers_GPT\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Flatten the lists\n",
    "flat_llava_answers = [item for sublist in df['vf_answers_LLaVA_GPT_r'] for item in sublist]\n",
    "flat_gpt_answers = [item for sublist in df['vf_answers_GPT_GPT_r'] for item in sublist]\n",
    "\n",
    "# Map 'Yes' to 1 and 'No' to 0 for correlation calculation\n",
    "mapping = {'Yes': 1, 'No': 0}\n",
    "flat_llava_mapped = [mapping[answer] for answer in flat_llava_answers]\n",
    "flat_gpt_mapped = [mapping[answer] for answer in flat_gpt_answers]\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr, p_value = spearmanr(flat_llava_mapped, flat_gpt_mapped)\n",
    "\n",
    "spearman_corr, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's kappa\n",
    "cohen_kappa = cohen_kappa_score(flat_llava_mapped, flat_gpt_mapped)\n",
    "\n",
    "cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
