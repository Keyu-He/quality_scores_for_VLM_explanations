{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install nltk rouge-score sacrebleu openpyxl openai torch transformers datasets matplotlib bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# OPENAI api key\n",
    "api_key_path = '../../OPENAI_key.txt'\n",
    "with open(api_key_path, 'r') as file:\n",
    "    api_key = file.read().strip().split('\\n')[0]\n",
    "    \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: BLEU, ROUGE, METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Support / Simulatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and model only if they are not already defined\n",
    "if 'nli_tokenizer' not in globals():\n",
    "    nli_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "if 'nli_model' not in globals():\n",
    "    nli_model = AutoModelForSeq2SeqLM.from_pretrained(\"soumyasanyal/nli-entailment-verifier-xxl\", load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    score = sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "def calculate_meteor(reference, candidate):\n",
    "    reference_tokens = reference.split()\n",
    "    candidate_tokens = candidate.split()\n",
    "    score = meteor_score([reference_tokens], candidate_tokens)\n",
    "    return score\n",
    "\n",
    "def display_scores(reference, candidate):\n",
    "    bleu_score = calculate_bleu(reference, candidate)\n",
    "    rouge_scores = calculate_rouge(reference, candidate)\n",
    "    meteor_score = calculate_meteor(reference, candidate)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(\"ROUGE Scores:\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"METEOR Score: {meteor_score:.4f}\")\n",
    "    \n",
    "def get_longest_rationale(rationale_list):\n",
    "    rationales = eval(rationale_list)\n",
    "    return max(rationales, key=len) if isinstance(rationales, list) else ''\n",
    "\n",
    "def calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer):\n",
    "    def get_score(nli_model, nli_tokenizer, input_ids):\n",
    "        pos_ids = nli_tokenizer('Yes').input_ids\n",
    "        neg_ids = nli_tokenizer('No').input_ids\n",
    "        pos_id = pos_ids[0]\n",
    "        neg_id = neg_ids[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = nli_model(input_ids, decoder_input_ids=torch.zeros((input_ids.size(0), 1), dtype=torch.long)).logits\n",
    "            pos_logits = logits[:, 0, pos_id]\n",
    "            neg_logits = logits[:, 0, neg_id]\n",
    "            posneg_logits = torch.cat([pos_logits.unsqueeze(-1), neg_logits.unsqueeze(-1)], dim=1)\n",
    "\n",
    "            # Cast to float before applying softmax\n",
    "            posneg_logits = posneg_logits.float()\n",
    "            scores = torch.nn.functional.softmax(posneg_logits, dim=1)\n",
    "            entail_score = scores[:, 0].item()\n",
    "            no_entail_score = scores[:, 1].item()\n",
    "        \n",
    "        return entail_score, no_entail_score\n",
    "    \n",
    "    prompt = f\"Premise: {premise}\\nHypothesis: {hypothesis}\\nGiven the premise, is the hypothesis correct?\\nAnswer:\"\n",
    "    input_ids = nli_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    return get_score(nli_model, nli_tokenizer, input_ids)[0]\n",
    "\n",
    "def generate_mask(generated_rationale, predicted_answer):\n",
    "    # Create a regex pattern to match the predicted answer case-insensitively and as a whole word\n",
    "    predicted_answer = str(predicted_answer)\n",
    "    pattern = re.compile(r'\\b' + re.escape(predicted_answer) + r'\\b', re.IGNORECASE)\n",
    "    return pattern.sub(\"<mask>\", generated_rationale)\n",
    "\n",
    "def evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if use_pieces:\n",
    "            premise = row['concat_rationale_pieces_mask'] if use_mask else row['concat_rationale_pieces']\n",
    "        else: \n",
    "            premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        \n",
    "        hypothesis = row[hypothesis_col]\n",
    "        entail_prob = calc_support_prob(premise, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "        support = entail_prob > threshold \n",
    "        if entail_prob < threshold:\n",
    "            print(f\"Premise: {premise}\")\n",
    "            print(f\"Hypothesis: {hypothesis}\")\n",
    "            print(f\"Probability: {entail_prob}\")\n",
    "        support_scores.append({\n",
    "            'entail_prob': entail_prob,\n",
    "            'support': support\n",
    "        })\n",
    "    return support_scores\n",
    "\n",
    "def compute_file_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"A fossil fuel is a kind of natural resource. Coal is a kind of fossil fuel.\"\n",
    "hypothesis = \"Coal is a kind of natural resource.\"\n",
    "calc_support_prob(premise, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = '../results/Human Annotation of LLaVA+ Rationales.xlsx'\n",
    "\n",
    "if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    model_name = \"LLaVA\"\n",
    "else:\n",
    "    model_name = file_path.split('results/')[1].split('.xlsx')[0]\n",
    "\n",
    "spreadsheet = pd.ExcelFile(file_path)\n",
    "\n",
    "# Read the specified columns from the sheet\n",
    "columns_to_read = [\n",
    "    'question',\n",
    "    'correct_answer',\n",
    "    'predicted_answer',\n",
    "    'is_correct',\n",
    "    'groundtruth_rationale',\n",
    "    'generated_rationale'\n",
    "]\n",
    "\n",
    "if file_path == '../results/Human Annotation of LLaVA+ Rationales.xlsx':\n",
    "    data = pd.read_excel(file_path, header=1, usecols=columns_to_read)\n",
    "else:\n",
    "    data = pd.read_excel(file_path, usecols=columns_to_read)\n",
    "data['question_no_choice'] = data.apply(lambda row: row['question'].split(' Choices:')[0], axis=1)\n",
    "data['longest_groundtruth_rationale'] = data['groundtruth_rationale'].apply(get_longest_rationale)\n",
    "\n",
    "data['BLEU_score'] = data.apply(lambda row: calculate_bleu(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "data['ROUGE_scores'] = data.apply(lambda row: calculate_rouge(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "data['METEOR_score'] = data.apply(lambda row: calculate_meteor(row['longest_groundtruth_rationale'], row['generated_rationale']), axis=1)\n",
    "\n",
    "input_data = data[['question', 'predicted_answer']].copy()\n",
    "input_data['question'] = input_data['question'].apply(lambda x: x.split(' Choices:')[0])\n",
    "input_data.rename(columns={'question': 'question_text', 'predicted_answer': 'answer_text'}, inplace=True)\n",
    "input_jsonl = f'input_data_{model_name}.jsonl'\n",
    "output_jsonl = f'{input_jsonl}.predictions'\n",
    "with open(input_jsonl, 'w') as f:\n",
    "    for index, row in input_data.iterrows():\n",
    "        # Convert None to null\n",
    "        row_dict = {k: (v if pd.notna(v) else 'None') for k, v in row.to_dict().items()}\n",
    "        json.dump(row_dict, f)\n",
    "        f.write('\\n')\n",
    "# Compute hash of input_jsonl\n",
    "current_input_hash = compute_file_hash(input_jsonl)\n",
    "hash_file = f'{input_jsonl}.hash'\n",
    "# Check if output_jsonl exists and input_jsonl hash hasn't changed\n",
    "run_bash_command = True\n",
    "if os.path.exists(output_jsonl):\n",
    "    if os.path.exists(hash_file):\n",
    "        with open(hash_file, 'r') as f:\n",
    "            saved_input_hash = f.read().strip()\n",
    "            if current_input_hash == saved_input_hash:\n",
    "                run_bash_command = False\n",
    "# Save the current input_jsonl hash\n",
    "with open(hash_file, 'w') as f:\n",
    "    f.write(current_input_hash)\n",
    "    \n",
    "# The conversion step\n",
    "# Define the full path to the script\n",
    "script_path = '/home/<link_hidden>/REV/run_question_converter.sh'\n",
    "if run_bash_command:\n",
    "    # Set PYTHONPATH and run the script\n",
    "    os.system(f'export PYTHONPATH=/home/<link_hidden>/REV/:$PYTHONPATH && bash {script_path} cqa {input_jsonl} cuda:0')\n",
    "else:\n",
    "    print(f'{output_jsonl} already exists and {input_jsonl} has not changed. Skipping the bash command.')\n",
    "\n",
    "with open(output_jsonl, 'r') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.rename(columns={'question_statement_text': 'hypothesis'}, inplace=True)\n",
    "\n",
    "# Merge datasets based on the 'question' column\n",
    "data = pd.merge(data, predictions_df[['question_text', 'hypothesis']], left_on='question_no_choice', right_on='question_text', how='left')\n",
    "\n",
    "data['gen_rationale_mask'] = data.apply(lambda row: generate_mask(row['generated_rationale'], row['predicted_answer']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate set R which contains rationale pieces for every instance\n",
    "\n",
    "def extract_distinct_rationale_pieces(hypothesis, rationale):\n",
    "    prompt=f\"\"\"Please break the following rationale into distinct pieces, and keep only the ones that are not semantically equivalent to the hypothesis. Output the final answer in a Python list format.\n",
    "\n",
    "Example:\n",
    "Hypothesis: The man by the bags is waiting for a delivery.\n",
    "Rationale: The man by the bags is waiting for a delivery, as indicated by the presence of the suitcases and the fact that he is standing on the side of the road. The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\n",
    "Output: [\"Suitcases are present in the image.\", \"The man is standing on the side of the road.\", \"The other options, such as a skateboarder, train, or cab, do not seem to be relevant to the situation depicted in the image.\"]\n",
    "\n",
    "Task:\n",
    "Hypothesis: {hypothesis}\n",
    "Rationale: {rationale}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    message = response.choices[0].message.content\n",
    "    start_index = message.find('[')\n",
    "    end_index = message.rfind(']')\n",
    "    R_list = message[start_index:end_index+1]\n",
    "    R_list = ast.literal_eval(R_list)\n",
    "    return R_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row and store the result in a new column\n",
    "data['extracted_rationale_pieces'] = data.apply(\n",
    "    lambda row: extract_distinct_rationale_pieces(row['hypothesis'], row['generated_rationale']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated dataset as a CSV file\n",
    "if not os.path.exists('data_with_rationale_pieces'):\n",
    "    os.makedirs('data_with_rationale_pieces')\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "output_file_path = f'data_with_rationale_pieces/data_with_rationale_pieces_{model_name}.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(f\"Saved results to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"data_with_support_gpt-4o_inference_one_shot.csv\",\n",
    "             \"data_with_support_gpt-4o_text_only_inference_one_shot.csv\",\n",
    "             \"data_with_support_llava-1.5-7b-hf_inference_no_vision.csv\"]\n",
    "\n",
    "def calc_inform_and_sim(file_path):\n",
    "    model_name = file_path.split('support_')[1].split('.csv')[0]\n",
    "    data = pd.read_csv(file_path)\n",
    "    if 'entail_prob' not in data.columns and 'no_entail_prob' in data.columns:\n",
    "        data['entail_prob'] = 1 - data['no_entail_prob']\n",
    "        \n",
    "    data['extracted_rationale_pieces'] = data.apply(\n",
    "        lambda row: extract_distinct_rationale_pieces(row['hypothesis'], row['generated_rationale']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Save the updated dataset as a CSV file\n",
    "    if not os.path.exists('data_with_rationale_pieces'):\n",
    "        os.makedirs('data_with_rationale_pieces')\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    output_file_path = f'data_with_rationale_pieces/data_with_rationale_pieces_{model_name}.csv'\n",
    "    data.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved results to {output_file_path}\")\n",
    "    \n",
    "    # Double check with this line!! is x python list (then check len(x)==0) or a string (then check len(x)==2)?\n",
    "    data['informative'] = data['extracted_rationale_pieces'].apply(lambda x: False if len(x) == 0 else True)\n",
    "    \n",
    "    data.rename(columns={'support':\"simulatable\"}, inplace=True)\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    # Save the updated dataset as a CSV file\n",
    "    if not os.path.exists('data_with_inform_sim'):\n",
    "        os.makedirs('data_with_inform_sim')\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    output_file_path = f'data_with_inform_sim/data_with_inform_sim_{model_name}.csv'\n",
    "    data.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved results to {output_file_path}\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    calc_inform_and_sim(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [\"/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_mask_LLaVA.csv\",\n",
    "                \"/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_llava-1.5-7b-hf_inference_no_vision.csv\",\n",
    "                \"/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_gpt-4o_inference_one_shot.csv\",\n",
    "                \"/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_gpt-4o_text_only_inference_one_shot.csv\",\n",
    "]\n",
    "\n",
    "for dataset_path in dataset_paths:\n",
    "    data = pd.read_csv(dataset_path)\n",
    "#     data['informative'] = data['extracted_rationale_pieces'].apply(lambda x: False if len(x) == 2 else True)\n",
    "#     data.to_csv(dataset_path, index=False)\n",
    "    avg_acc = np.mean(data['is_correct'])\n",
    "    if dataset_path == \"/home/<link_hidden>/<hidden>/notebooks/analysis/data_with_inform_sim/data_with_inform_sim_mask_LLaVA.csv\":\n",
    "        avg_support = np.mean(data['simulatable_mask'])\n",
    "    else:\n",
    "        avg_support = np.mean(data['simulatable'])\n",
    "    avg_informativeness = np.mean(data['informative'])\n",
    "    print(f\"Avg. accuracy: {avg_acc}\")\n",
    "    print(f\"Avg. support: {avg_support}\")\n",
    "    print(f\"Avg. informativeness: {avg_informativeness}\")  \n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"LLaVA\"\n",
    "data = pd.read_csv(f'data_with_inform_sim/data_with_inform_sim_mask_{model_name}.csv')\n",
    "data = pd.read_csv(f'data_with_inform_sim/data_with_inform_sim_gpt-4o_inference_one_shot.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['informative'] = data['extracted_rationale_pieces'].apply(lambda x: False if len(x) == 2 else True)\n",
    "# data['simulatable'] = data.apply(lambda row: calc_low_support_score(row['generated_rationale'], row['hypothesis']) <= 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "false_informative_rows = data[data['informative'].apply(lambda x: x == False)]\n",
    "false_informative_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data\n",
    "false_simulatable_rows = data[data['simulatable'].apply(lambda x: x == False)]\n",
    "false_simulatable_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset as a CSV file\n",
    "if not os.path.exists('data_with_inform_sim'):\n",
    "    os.makedirs('data_with_inform_sim')\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "output_file_path = f'data_with_inform_sim/data_with_inform_sim_{model_name}.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(f\"Saved results to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot test: use extracted rationale pieces as the premise when calculating simulatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str_to_paragraph(string_list):\n",
    "    # Convert the string to a Python list\n",
    "    list_of_strings = ast.literal_eval(string_list)\n",
    "    # Concatenate the strings\n",
    "    paragraph = ' '.join(list_of_strings)\n",
    "    return paragraph\n",
    "\n",
    "data = pd.read_csv(\"data_with_inform_sim/data_with_inform_sim_mask_LLaVA.csv\")\n",
    "data['concat_rationale_pieces'] = data['extracted_rationale_pieces'].apply(concat_str_to_paragraph)\n",
    "data['concat_rationale_pieces_mask'] = data.apply(lambda row: generate_mask(row['concat_rationale_pieces'], row['predicted_answer']), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"support_use_pieces\"] = evaluate_support(data, nli_model, nli_tokenizer, use_mask=False, use_pieces=True)\n",
    "data[\"prob_support_use_pieces\"] = data[\"support_use_pieces\"].apply(lambda x: x['entail_prob'])\n",
    "data[\"support_use_pieces\"] = data[\"support_use_pieces\"].apply(lambda x: x['support'])\n",
    "\n",
    "data[\"support_use_pieces_mask\"] = evaluate_support(data, nli_model, nli_tokenizer, use_mask=True, use_pieces=True)\n",
    "data[\"prob_support_use_pieces_mask\"] = data[\"support_use_pieces_mask\"].apply(lambda x: x['entail_prob'])\n",
    "data[\"support_use_pieces_mask\"] = data[\"support_use_pieces_mask\"].apply(lambda x: x['support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_score_use_pieces = data['support_use_pieces'].astype(int).mean()\n",
    "support_score_use_pieces_mask = data['support_use_pieces_mask'].astype(int).mean()\n",
    "print(f\"support_score_use_pieces: {support_score_use_pieces}\")\n",
    "print(f\"support_score_use_pieces_mask: {support_score_use_pieces_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = f'data_with_inform_sim/data_with_inform_sim_LLaVA_v2.csv'\n",
    "data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All sections below are not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pilot tests of support on sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"The people in the image are located in their home. This can be inferred from the presence of a couch, which is a common piece of furniture found in homes. Additionally, the people are sitting on the couch, which further supports the idea that they are in their home. The image does not show any indications of a workplace, hospital, library, or any other location.\"\n",
    "premise = \"The image shows a casual and comfortable environment with two dogs lounging on a couch, along with personal items such as books, electronics, and various other belongings scattered around. This setting is typically indicative of a residence rather than a work, hospital, or library environment. The presence of dogs and the relaxed atmosphere further suggests a private living space. Thus, the answer is home.\"\n",
    "hypothesis = \"These people are located home.\"\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(premise)\n",
    "sentence_scores = []\n",
    "for sentence in sentences:\n",
    "    no_entail_prob = calc_low_support_score(sentence, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "    support = no_entail_prob < 0.2\n",
    "    sentence_scores.append({\n",
    "        'sentence': sentence,\n",
    "        'no_entail_prob': no_entail_prob,\n",
    "        'support': support\n",
    "    })\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(sentence_scores)\n",
    "pd.set_option('display.max_colwidth', None) # Show full column content\n",
    "# Display the DataFrame\n",
    "df\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display the DataFrame as a Markdown table\n",
    "display(Markdown(copiable_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence_level_support(data, nli_model, nli_tokenizer, use_mask=False, hypothesis_col='hypothesis', threshold=0.5):\n",
    "    sentence_support_scores = []\n",
    "    for idx, row in data.iterrows():\n",
    "        premise = row['gen_rationale_mask'] if use_mask else row['generated_rationale']\n",
    "        hypothesis = row[hypothesis_col]\n",
    "        sentences = nltk.tokenize.sent_tokenize(premise)\n",
    "        sentence_scores = []\n",
    "        for sentence in sentences:\n",
    "            no_entail_prob = calc_low_support_score(sentence, hypothesis, nli_model=nli_model, nli_tokenizer=nli_tokenizer)\n",
    "            support = no_entail_prob < threshold\n",
    "            sentence_scores.append({\n",
    "                'sentence': sentence,\n",
    "                'no_entail_prob': no_entail_prob,\n",
    "                'support': support\n",
    "            })\n",
    "        sentence_support_scores.append(sentence_scores)\n",
    "    return sentence_support_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        model_name = file_path.split('data_with_support_')[1].split('.csv')[0]\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Perform sentence-level support evaluation\n",
    "        sentence_level_results = evaluate_sentence_level_support(data, nli_model, nli_tokenizer)\n",
    "\n",
    "        # Add the sentence-level results as a new column\n",
    "        data['sentence_level_support'] = sentence_level_results\n",
    "\n",
    "        if not os.path.exists('data_with_sentence_level_support'):\n",
    "            os.makedirs('data_with_sentence_level_support')\n",
    "        \n",
    "        # Save the updated data to a new CSV file\n",
    "        output_file_path = f'data_with_sentence_level_support_{model_name}.csv'\n",
    "        data.to_csv(output_file_path, index=False)\n",
    "        print(f\"Processed {file_path} and saved results to {output_file_path}\")\n",
    "\n",
    "file_paths = [\n",
    "    'data_with_support_LLaVA.csv',\n",
    "    # Add other file paths\n",
    "]\n",
    "process_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try question+choices+answers in REV tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'export PYTHONPATH=/home/<link_hidden>/REV/:$PYTHONPATH && bash /home/<link_hidden>/REV/run_question_converter.sh cqa /home/<link_hidden>/REV/sample_input.jsonl cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_50_index_df = pd.read_csv(\"../set-50-idx.csv\")\n",
    "visual_fidelity = [1,1,0,1,0,1,0,'null',1,1,1,1,-1,1,1,1,1,1,1,-1,1,0,-1,1,-1,0,0,1,1,0,1,1,0,1,1,-1,1,1,-1,1,0,0,-1,-1,1,1,-1,1,1,-1]\n",
    "set_50_index_df[\"visual_fidelity\"] = visual_fidelity\n",
    "\n",
    "# Select subsets with visual_fidelity == 1 and visual_fidelity == -1\n",
    "vf_1_subset = set_50_index_df[set_50_index_df['visual_fidelity'] == 1].head(10)\n",
    "vf_0_subset = set_50_index_df[set_50_index_df['visual_fidelity'] == -1].head(10)\n",
    "\n",
    "# Combine the subsets to create the \"data\" subset\n",
    "set_50_index_df_vf_filtered = pd.concat([vf_1_subset, vf_0_subset]).reset_index(drop=True)\n",
    "set_50_index_df_vf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf_filtered = data.iloc[set_50_index_df_vf_filtered['idx']].copy()\n",
    "data_vf_filtered['visual_fidelity'] = set_50_index_df_vf_filtered['visual_fidelity'].values\n",
    "data_vf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_vacuous_rationale(question, answer):\n",
    "#     return f\"{question} The answer is {answer}.\"\n",
    "\n",
    "\n",
    "# def calculate_rev_score(data, nli_model, nli_tokenizer):\n",
    "#     rev_scores = []\n",
    "\n",
    "#     for idx, row in data.iterrows():\n",
    "#         question = row['question'].split(' Choices:')[0]\n",
    "#         correct_answer = row['predicted_answer']\n",
    "#         hypothesis = row['hypothesis']\n",
    "        \n",
    "#         generated_rationale = row['generated_rationale']\n",
    "#         vacuous_rationale = construct_vacuous_rationale(question, correct_answer)\n",
    "        \n",
    "#         # Predict using NLI model\n",
    "#         probs_generated = predict_nli(nli_model, nli_tokenizer, generated_rationale, hypothesis)\n",
    "#         probs_vacuous = predict_nli(nli_model, nli_tokenizer, vacuous_rationale, hypothesis)\n",
    "        \n",
    "#         # Calculate log-probabilities for the entailment class (class 2)\n",
    "#         log_prob_generated = torch.log(probs_generated[0][2])\n",
    "#         log_prob_vacuous = torch.log(probs_vacuous[0][2])\n",
    "        \n",
    "#         # Compute REV score\n",
    "#         rev_score = log_prob_generated - log_prob_vacuous\n",
    "#         rev_scores.append(rev_score.item())\n",
    "    \n",
    "#     data['REV_score'] = rev_scores\n",
    "#     return data\n",
    "\n",
    "# # Calculate REV scores\n",
    "# data_with_rev = calculate_rev_score(data, nli_model, nli_tokenizer)\n",
    "# data_with_rev.to_csv('data_with_REV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_with_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(texts):\n",
    "#     return tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# def predict(model, inputs):\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "#     return probabilities\n",
    "\n",
    "# def compute_sim(to_use, labels_to_use):\n",
    "#     xe_correct = (to_use['predicted_labels_xe'] == to_use[labels_to_use]).astype(int)\n",
    "#     x_correct = (to_use['predicted_labels_x'] == to_use[labels_to_use]).astype(int)\n",
    "#     e_correct = (to_use['predicted_labels_e'] == to_use[labels_to_use]).astype(int)\n",
    "\n",
    "#     baseline_correct = x_correct\n",
    "#     leaking = e_correct\n",
    "#     leaked = np.where(leaking == 1)[0]\n",
    "#     nonleaked = np.where(leaking == 0)[0]\n",
    "\n",
    "#     xe_correct_leaked = xe_correct[leaked]\n",
    "#     baseline_correct_leaked = baseline_correct[leaked]\n",
    "#     xe_correct_nonleaked = xe_correct[nonleaked]\n",
    "#     baseline_correct_nonleaked = baseline_correct_nonleaked[nonleaked]\n",
    "\n",
    "#     unweighted_mean = np.mean([\n",
    "#         np.mean(xe_correct_leaked) - np.mean(baseline_correct_leaked),\n",
    "#         np.mean(xe_correct_nonleaked) - np.mean(baseline_correct_nonleaked)\n",
    "#     ])\n",
    "\n",
    "#     nonleaking_diff = np.mean(xe_correct_nonleaked) - np.mean(baseline_correct_nonleaked)\n",
    "#     leaking_diff = np.mean(xe_correct_leaked) - np.mean(baseline_correct_leaked)\n",
    "\n",
    "#     return unweighted_mean, leaking_diff, nonleaking_diff\n",
    "\n",
    "# def run_las_analysis(data, model, tokenizer):\n",
    "#     input_texts = data['question'].tolist()\n",
    "#     explanations = data['generated_rationale'].tolist()\n",
    "\n",
    "#     # Tokenize input texts\n",
    "#     input_encodings = tokenize_function(input_texts)\n",
    "#     explanation_encodings = tokenize_function(explanations)\n",
    "\n",
    "#     # Predict using the model\n",
    "#     input_probs = predict(model, input_encodings)\n",
    "#     explanation_probs = predict(model, explanation_encodings)\n",
    "\n",
    "#     data['predicted_labels_x'] = input_probs.argmax(dim=1).numpy()\n",
    "#     data['predicted_labels_e'] = explanation_probs.argmax(dim=1).numpy()\n",
    "\n",
    "#     # Use both input and explanations for predictions (xe)\n",
    "#     combined_texts = [f\"{text} {exp}\" for text, exp in zip(input_texts, explanations)]\n",
    "#     combined_encodings = tokenize_function(combined_texts)\n",
    "#     combined_probs = predict(model, combined_encodings)\n",
    "\n",
    "#     data['predicted_labels_xe'] = combined_probs.argmax(dim=1).numpy()\n",
    "\n",
    "#     # Compute LAS\n",
    "#     unweighted_mean, leaking_diff, nonleaking_diff = compute_sim(data, 'correct_answer')\n",
    "\n",
    "#     data['LAS_unweighted_mean'] = unweighted_mean\n",
    "#     data['LAS_leaking_diff'] = leaking_diff\n",
    "#     data['LAS_nonleaking_diff'] = nonleaking_diff\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the analysis\n",
    "# updated_data = run_las_analysis(data, model, tokenizer)\n",
    "# updated_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
