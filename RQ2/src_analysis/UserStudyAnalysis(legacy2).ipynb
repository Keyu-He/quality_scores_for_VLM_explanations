{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if participants did multiple attempts in different settings (which should not happen).\n",
    "\n",
    "import json\n",
    "import glob, os\n",
    "from itertools import combinations\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "# Dictionary to hold the set of user_ids for each setting.\n",
    "settings_user_ids = {}\n",
    "\n",
    "for setting in settings:\n",
    "    print(f\"Processing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    user_ids = set()\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            # Add all user_ids from the current JSON file.\n",
    "            user_ids.update(batch_data.keys())\n",
    "    \n",
    "    settings_user_ids[setting] = user_ids\n",
    "    print(f\"{len(user_ids)} participants in {setting}\")\n",
    "\n",
    "# Compare overlaps between each pair of settings.\n",
    "print(\"\\nOverlap between settings:\")\n",
    "for s1, s2 in combinations(settings, 2):\n",
    "    overlap = settings_user_ids[s1].intersection(settings_user_ids[s2])\n",
    "    print(f\"Overlap between {s1} and {s2}: {len(overlap)} participants\")\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"Participants: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if participants did multiple attempts in the same settings (which there should be some, but that is fine)\n",
    "\n",
    "import json\n",
    "import glob, os\n",
    "from collections import Counter\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "for setting in settings:\n",
    "    print(f\"Processing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    attempts_counter = Counter()\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            # Update counter with all user_ids in the file.\n",
    "            attempts_counter.update(batch_data.keys())\n",
    "    \n",
    "    total_attempts = sum(attempts_counter.values())\n",
    "    unique_participants = len(attempts_counter)\n",
    "    print(f\"Total attempts in {setting}: {total_attempts}\")\n",
    "    print(f\"Unique participants in {setting}: {unique_participants}\")\n",
    "    \n",
    "    # Identify users with more than one attempt.\n",
    "    multiple_attempts = {user: count for user, count in attempts_counter.items() if count > 1}\n",
    "    if multiple_attempts:\n",
    "        print(f\"In setting {setting}, the following participants did multiple attempts: {multiple_attempts}\")\n",
    "    else:\n",
    "        print(f\"In setting {setting}, no participant did multiple attempts.\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "setting = settings[-1]\n",
    "print(f\"Setting: {setting}\")\n",
    "files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "data = {}\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        batch_data = json.load(f)\n",
    "        for user_id, session in batch_data.items():\n",
    "            if user_id in data:\n",
    "                data[user_id].append(session)  # Append to list instead of overwriting\n",
    "            else:\n",
    "                data[user_id] = [session]  # Initialize as list\n",
    "\n",
    "print(f\"{sum(len(sessions) for sessions in data.values())} total sessions loaded from {len(files)} files\")\n",
    "all_instances = sum([session['interactions'] for sessions in data.values() for session in sessions], [])\n",
    "print(len(all_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the interactions\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_margin(p, n):\n",
    "    return math.sqrt(p*(1-p)/n) if n > 0 else 0\n",
    "\n",
    "def compute_proportion_ci(p, n):\n",
    "    margin = compute_margin(p, n)\n",
    "    return (p - margin, p + margin)\n",
    "\n",
    "def compute_mean_std(values):\n",
    "    n = len(values)\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values, ddof=1)\n",
    "    se = std_val/np.sqrt(n) if n > 0 else 0\n",
    "    return (mean_val - se, mean_val + se)\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1-x['question']['prediction_is_correct'] for x in instances])       # 0 means AI is correct, 1 means AI is incorrect\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "\n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    total_accuracy = (true_positives + true_negatives) / len(ground_truths)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if false_positives + true_negatives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Utility: 0 if user is unsure, 1 if user correctly predicts AI correctness, -1 if user incorrectly predicts AI correctness\n",
    "    individual_utilities = np.array([0 if preds[i] == 2 else 1-2*np.abs(preds[i] - ground_truths[i]) for i in range(len(preds))])\n",
    "    utility = np.mean(individual_utilities)\n",
    "    \n",
    "    # Compute 95% CIs.\n",
    "    total_accuracy_ci = compute_proportion_ci(total_accuracy, len(ground_truths))\n",
    "    unsure_rate_ci    = compute_proportion_ci(unsure_rate, len(preds))\n",
    "    \n",
    "    non_unsure = preds != 2\n",
    "    non_unsure_n = np.sum(non_unsure)\n",
    "    accuracy_ci = compute_proportion_ci(accuracy, non_unsure_n) if non_unsure_n > 0 else (0, 0)\n",
    "    \n",
    "    precision_n = true_positives + false_positives\n",
    "    precision_ci = compute_proportion_ci(precision, precision_n) if precision_n > 0 else (0, 0)\n",
    "    \n",
    "    recall_n = true_positives + false_negatives\n",
    "    recall_ci = compute_proportion_ci(recall, recall_n) if recall_n > 0 else (0, 0)\n",
    "    \n",
    "    fpr_n = false_positives + true_negatives\n",
    "    fpr_ci = compute_proportion_ci(false_positive_rate, fpr_n) if fpr_n > 0 else (0, 0)\n",
    "    \n",
    "    utility_ci = compute_mean_std(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'total_accuracy_ci': total_accuracy_ci,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_ci': accuracy_ci,\n",
    "        'precision': precision,\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': recall,\n",
    "        'recall_ci': recall_ci,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'fpr_ci': fpr_ci,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'unsure_rate_ci': unsure_rate_ci,\n",
    "        'utility': utility,\n",
    "        'utility_ci': utility_ci,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "answeronly_results = evaluate_answers('answeronly', all_instances)\n",
    "withexplanation_results = evaluate_answers('withexplanation', all_instances)\n",
    "withexplanationquality_results = evaluate_answers('withexplanationquality', all_instances)\n",
    "\n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\t\\tFPR\\t\\tUtility\")\n",
    "# print(\"-\"*140)\n",
    "# for stage, results in zip(\n",
    "#     ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "#     [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "# ):\n",
    "#     print(f\"{stage:<25}\\t{results['unsure_rate']:.1%}\\t\\t{results['total_accuracy']:.1%}\\t\\t{results['accuracy']:.1%}\\t\\t{results['precision']:.3f}\\t\\t{results['recall']:.3f}\\t\\t{results['f1']:.3f}\\t\\t{results['false_positive_rate']:.3f}\\t\\t{results['utility']:.3f}\")\n",
    "    \n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\tFPR\\t\\tUtility\")\n",
    "print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "# print(\"-\"*150) \n",
    "print(\"-\"*100)\n",
    "results_for_easy_copy = []\n",
    "for stage_name, results in zip(\n",
    "    ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "    [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "):\n",
    "    # Calculate margins as half the width of each CI.\n",
    "    unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "    total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "    acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "    prec_margin      = (results['precision_ci'][1] - results['precision_ci'][0]) / 2 if results['precision_ci'] != (0, 0) else 0\n",
    "    recall_margin    = (results['recall_ci'][1] - results['recall_ci'][0]) / 2 if results['recall_ci'] != (0, 0) else 0\n",
    "    fpr_margin       = (results['fpr_ci'][1] - results['fpr_ci'][0]) / 2 if results['fpr_ci'] != (0, 0) else 0\n",
    "    utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "    # For proportion metrics, we print as percentages; f1 is printed as a decimal (CI not computed); utility remains as a number.\n",
    "    print(f\"{stage_name:<25}\\t\"\n",
    "          f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}\\t\"\n",
    "          f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}\\t\"\n",
    "          f\"{results['accuracy']:.1%} ± {acc_margin:.1%}\\t\"\n",
    "        #   f\"{results['precision']:.1%} ± {prec_margin:.1%}\\t\"\n",
    "        #   f\"{results['recall']:.1%} ± {recall_margin:.1%}\\t\"\n",
    "        #   f\"{results['f1']:.3f}\\t\"\n",
    "        #   f\"{results['false_positive_rate']:.1%} ± {fpr_margin:.1%}\\t\"\n",
    "          f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n",
    "    copyable_result = f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}, \"\n",
    "    copyable_result += f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}, \"\n",
    "    copyable_result += f\"{results['accuracy']:.1%} ± {acc_margin:.1%}, \"\n",
    "    copyable_result += f\"{results['utility']:.3f} ± {utility_margin:.3f}\"\n",
    "    \n",
    "    results_for_easy_copy.append(copyable_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results in a format that can be easily copied.\n",
    "print(\"Results for easy copy:\")\n",
    "for result in results_for_easy_copy:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Definitions of helper functions.\n",
    "def compute_margin(p, n):\n",
    "    return 1.96 * math.sqrt(p * (1 - p) / n) if n > 0 else 0\n",
    "\n",
    "def compute_proportion_ci(p, n):\n",
    "    margin = compute_margin(p, n)\n",
    "    return (p - margin, p + margin)\n",
    "\n",
    "def compute_mean_ci(values):\n",
    "    n = len(values)\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values, ddof=1)\n",
    "    se = std_val / np.sqrt(n) if n > 0 else 0\n",
    "    return (mean_val - 1.96 * se, mean_val + 1.96 * se)\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1 - x['question']['prediction_is_correct'] for x in instances])\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "    \n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    total_accuracy = (true_positives + true_negatives) / len(ground_truths)\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Utility: 0 if unsure; 1 if correct; -1 if incorrect.\n",
    "    individual_utilities = np.array([\n",
    "        0 if preds[i] == 2 else 1 - 2 * abs(preds[i] - ground_truths[i])\n",
    "        for i in range(len(preds))\n",
    "    ])\n",
    "    utility = np.mean(individual_utilities)\n",
    "    \n",
    "    total_accuracy_ci = compute_proportion_ci(total_accuracy, len(ground_truths))\n",
    "    unsure_rate_ci    = compute_proportion_ci(unsure_rate, len(preds))\n",
    "    \n",
    "    non_unsure = preds != 2\n",
    "    non_unsure_n = np.sum(non_unsure)\n",
    "    accuracy_ci = compute_proportion_ci(accuracy, non_unsure_n) if non_unsure_n > 0 else (0, 0)\n",
    "    \n",
    "    precision_n = true_positives + false_positives\n",
    "    precision_ci = compute_proportion_ci(precision, precision_n) if precision_n > 0 else (0, 0)\n",
    "    \n",
    "    recall_n = true_positives + false_negatives\n",
    "    recall_ci = compute_proportion_ci(recall, recall_n) if recall_n > 0 else (0, 0)\n",
    "    \n",
    "    fpr_n = false_positives + true_negatives\n",
    "    fpr_ci = compute_proportion_ci(false_positive_rate, fpr_n) if fpr_n > 0 else (0, 0)\n",
    "    \n",
    "    utility_ci = compute_mean_ci(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'total_accuracy_ci': total_accuracy_ci,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_ci': accuracy_ci,\n",
    "        'precision': precision,\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': recall,\n",
    "        'recall_ci': recall_ci,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'fpr_ci': fpr_ci,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'unsure_rate_ci': unsure_rate_ci,\n",
    "        'utility': utility,\n",
    "        'utility_ci': utility_ci,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "def compute_unsure_rate(interactions, stage):\n",
    "    preds = np.array([x['user_selections'][stage] for x in interactions])\n",
    "    return np.mean(preds == 2)\n",
    "\n",
    "# List of settings.\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# Process each setting individually.\n",
    "for setting in settings:\n",
    "    print(f\"\\nProcessing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    # Aggregate sessions by user.\n",
    "    data = {}  # key: user_id, value: list of sessions (each session is a dict)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            for user_id, session in batch_data.items():\n",
    "                data.setdefault(user_id, []).append(session)\n",
    "    total_users = len(data)\n",
    "    total_attempts = sum(len(sessions) for sessions in data.values())\n",
    "    print(f\"Found {total_users} users in setting '{setting}' with {total_attempts} total attempts.\")\n",
    "    \n",
    "    # Filter: Only include users with >=20% unsure rate in step 1.\n",
    "    filtered_instances = []\n",
    "    count_filtered_users = 0\n",
    "    count_filtered_attempts = 0\n",
    "    for user, sessions in data.items():\n",
    "        interactions = []\n",
    "        for session in sessions:\n",
    "            interactions.extend(session['interactions'])\n",
    "        rate_ans = compute_unsure_rate(interactions, 'answeronly')\n",
    "        if rate_ans >= 0.20:\n",
    "            filtered_instances.extend(interactions)\n",
    "            count_filtered_users += 1\n",
    "            count_filtered_attempts += len(sessions)\n",
    "    \n",
    "    print(f\"{count_filtered_users} users remain after filtering for >=20% unsure rate in step 1, with {count_filtered_attempts} attempts.\")\n",
    "    \n",
    "    if len(filtered_instances) == 0:\n",
    "        print(\"No interactions remain after filtering. Skipping evaluation for this setting.\")\n",
    "        continue\n",
    "\n",
    "    # Evaluate the filtered interactions for each stage.\n",
    "    answeronly_results = evaluate_answers('answeronly', filtered_instances)\n",
    "    withexplanation_results = evaluate_answers('withexplanation', filtered_instances)\n",
    "    withexplanationquality_results = evaluate_answers('withexplanationquality', filtered_instances)\n",
    "    \n",
    "    # Print the formatted table.\n",
    "    print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "    print(\"-\" * 100)\n",
    "    for stage_name, results in zip(\n",
    "        ['Answer Only', 'With Explanation', 'With Explanation + Quality'],\n",
    "        [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "    ):\n",
    "        unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "        total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "        acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "        utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "        print(f\"{stage_name:<25}\\t\"\n",
    "              f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}\\t\"\n",
    "              f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}\\t\"\n",
    "              f\"{results['accuracy']:.1%} ± {acc_margin:.1%}\\t\"\n",
    "              f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n",
    "    # Print CSV-friendly copyable results\n",
    "    result_strs = []\n",
    "    for results in [answeronly_results, withexplanation_results, withexplanationquality_results]:\n",
    "        unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "        total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "        acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "        utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "        s = (f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}, \"\n",
    "            f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}, \"\n",
    "            f\"{results['accuracy']:.1%} ± {acc_margin:.1%}, \"\n",
    "            f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "        result_strs.append(s)\n",
    "\n",
    "    print(\", \".join(result_strs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of settings.\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# Aggregate sessions across all settings by user.\n",
    "aggregated_data = {}  # key: user_id, value: list of sessions\n",
    "for setting in settings:\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            for user_id, session in batch_data.items():\n",
    "                aggregated_data.setdefault(user_id, []).append(session)\n",
    "\n",
    "total_sessions = sum(len(sessions) for sessions in aggregated_data.values())\n",
    "print(f\"Aggregated {total_sessions} sessions from {len(aggregated_data)} users across {len(settings)} settings.\")\n",
    "\n",
    "# Helper: compute unsure rate for a given stage.\n",
    "def compute_unsure_rate(interactions, stage):\n",
    "    preds = np.array([x['user_selections'][stage] for x in interactions])\n",
    "    return np.mean(preds == 2)\n",
    "\n",
    "# Helper: compute accuracy for a given stage.\n",
    "# Every response is counted; if the user is \"unsure\" (i.e. selection == 2) it is considered incorrect.\n",
    "def compute_user_accuracy_for_stage(interactions, stage):\n",
    "    preds = np.array([x['user_selections'][stage] for x in interactions])\n",
    "    # Ground truth: 0 means AI is correct, 1 means AI is incorrect.\n",
    "    ground_truths = np.array([1 - x['question']['prediction_is_correct'] for x in interactions])\n",
    "    # A response is correct if it is not 'unsure' and matches the ground truth.\n",
    "    is_correct = ((preds == 0) & (ground_truths == 0)) | ((preds == 1) & (ground_truths == 1))\n",
    "    accuracy = np.mean(is_correct)\n",
    "    return accuracy\n",
    "\n",
    "# Dictionaries to store per-user metrics.\n",
    "unsure_rate_answeronly = {}           # Stage 1: 'answeronly'\n",
    "unsure_rate_withexplanation = {}       # Stage 2: 'withexplanation'\n",
    "unsure_rate_withexplanationquality = {} # Stage 3: 'withexplanationquality'\n",
    "accuracy_answeronly = {}\n",
    "accuracy_withexplanation = {}\n",
    "accuracy_withexplanationquality = {}\n",
    "\n",
    "# Compute per-user metrics by aggregating interactions across sessions.\n",
    "for user, sessions in aggregated_data.items():\n",
    "    interactions = []\n",
    "    for session in sessions:\n",
    "        interactions.extend(session['interactions'])\n",
    "    # Compute unsure rates.\n",
    "    rate_ans = compute_unsure_rate(interactions, 'answeronly')\n",
    "    rate_expl = compute_unsure_rate(interactions, 'withexplanation')\n",
    "    rate_explqual = compute_unsure_rate(interactions, 'withexplanationquality')\n",
    "    # Compute accuracies.\n",
    "    acc_ans = compute_user_accuracy_for_stage(interactions, 'answeronly')\n",
    "    acc_expl = compute_user_accuracy_for_stage(interactions, 'withexplanation')\n",
    "    acc_explqual = compute_user_accuracy_for_stage(interactions, 'withexplanationquality')\n",
    "    # Only record users where all metrics are computed.\n",
    "    if acc_ans is not None and acc_expl is not None and acc_explqual is not None:\n",
    "        unsure_rate_answeronly[user] = rate_ans\n",
    "        unsure_rate_withexplanation[user] = rate_expl\n",
    "        unsure_rate_withexplanationquality[user] = rate_explqual\n",
    "        accuracy_answeronly[user] = acc_ans\n",
    "        accuracy_withexplanation[user] = acc_expl\n",
    "        accuracy_withexplanationquality[user] = acc_explqual\n",
    "\n",
    "print(f\"Computed metrics for {len(unsure_rate_answeronly)} users.\")\n",
    "\n",
    "### Figure 1: Original Unsure (Step 1) vs. Accuracy (Step 2/3) ###\n",
    "\n",
    "# Prepare data for Step 2.\n",
    "users_common_step2 = [user for user in unsure_rate_answeronly if user in accuracy_withexplanation]\n",
    "x_unsure = np.array([unsure_rate_answeronly[user] for user in users_common_step2])\n",
    "y_acc_step2 = np.array([accuracy_withexplanation[user] for user in users_common_step2])\n",
    "\n",
    "# Prepare data for Step 3.\n",
    "users_common_step3 = [user for user in unsure_rate_answeronly if user in accuracy_withexplanationquality]\n",
    "x_unsure_3 = np.array([unsure_rate_answeronly[user] for user in users_common_step3])\n",
    "y_acc_step3 = np.array([accuracy_withexplanationquality[user] for user in users_common_step3])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot: Step 1 Unsure vs. Step 2 Accuracy.\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_unsure, y_acc_step2, alpha=0.6)\n",
    "# Best-fit line.\n",
    "slope, intercept = np.polyfit(x_unsure, y_acc_step2, 1)\n",
    "x_line = np.linspace(x_unsure.min(), x_unsure.max(), 100)\n",
    "plt.plot(x_line, slope * x_line + intercept, color='red', label=f'y={slope:.2f}x+{intercept:.2f}')\n",
    "plt.xlabel(\"Unsure Rate in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Accuracy in With Explanation (Step 2)\")\n",
    "plt.title(\"Step 2 Accuracy vs. Step 1 Unsure Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Subplot: Step 1 Unsure vs. Step 3 Accuracy.\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_unsure_3, y_acc_step3, alpha=0.6, color='orange')\n",
    "# Best-fit line.\n",
    "slope3, intercept3 = np.polyfit(x_unsure_3, y_acc_step3, 1)\n",
    "x_line3 = np.linspace(x_unsure_3.min(), x_unsure_3.max(), 100)\n",
    "plt.plot(x_line3, slope3 * x_line3 + intercept3, color='red', label=f'y={slope3:.2f}x+{intercept3:.2f}')\n",
    "plt.xlabel(\"Unsure Rate in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Accuracy in With Explanation + Quality (Step 3)\")\n",
    "plt.title(\"Step 3 Accuracy vs. Step 1 Unsure Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Figure 2: Unsure Rate vs. Unsure Rate ###\n",
    "\n",
    "# For Unsure vs Unsure, we compare:\n",
    "# - Step 1 Unsure vs. Step 2 Unsure\n",
    "# - Step 1 Unsure vs. Step 3 Unsure\n",
    "\n",
    "# Prepare data for Step 2.\n",
    "users_common_unsure2 = [user for user in unsure_rate_answeronly if user in unsure_rate_withexplanation]\n",
    "x_unsure_rate = np.array([unsure_rate_answeronly[user] for user in users_common_unsure2])\n",
    "y_unsure_rate_step2 = np.array([unsure_rate_withexplanation[user] for user in users_common_unsure2])\n",
    "\n",
    "# Prepare data for Step 3.\n",
    "users_common_unsure3 = [user for user in unsure_rate_answeronly if user in unsure_rate_withexplanationquality]\n",
    "x_unsure_rate_3 = np.array([unsure_rate_answeronly[user] for user in users_common_unsure3])\n",
    "y_unsure_rate_step3 = np.array([unsure_rate_withexplanationquality[user] for user in users_common_unsure3])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot: Step 1 Unsure vs. Step 2 Unsure.\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_unsure_rate, y_unsure_rate_step2, alpha=0.6)\n",
    "slope_u2, intercept_u2 = np.polyfit(x_unsure_rate, y_unsure_rate_step2, 1)\n",
    "x_line_u2 = np.linspace(x_unsure_rate.min(), x_unsure_rate.max(), 100)\n",
    "plt.plot(x_line_u2, slope_u2 * x_line_u2 + intercept_u2, color='red', label=f'y={slope_u2:.2f}x+{intercept_u2:.2f}')\n",
    "plt.xlabel(\"Unsure Rate in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Unsure Rate in With Explanation (Step 2)\")\n",
    "plt.title(\"Step 2 Unsure vs. Step 1 Unsure\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Subplot: Step 1 Unsure vs. Step 3 Unsure.\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_unsure_rate_3, y_unsure_rate_step3, alpha=0.6, color='orange')\n",
    "slope_u3, intercept_u3 = np.polyfit(x_unsure_rate_3, y_unsure_rate_step3, 1)\n",
    "x_line_u3 = np.linspace(x_unsure_rate_3.min(), x_unsure_rate_3.max(), 100)\n",
    "plt.plot(x_line_u3, slope_u3 * x_line_u3 + intercept_u3, color='red', label=f'y={slope_u3:.2f}x+{intercept_u3:.2f}')\n",
    "plt.xlabel(\"Unsure Rate in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Unsure Rate in With Explanation + Quality (Step 3)\")\n",
    "plt.title(\"Step 3 Unsure vs. Step 1 Unsure\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Figure 3: Accuracy vs. Accuracy ###\n",
    "\n",
    "# For Accuracy vs Accuracy, we compare:\n",
    "# - Step 1 Accuracy vs. Step 2 Accuracy\n",
    "# - Step 1 Accuracy vs. Step 3 Accuracy\n",
    "\n",
    "# Prepare data for Step 2.\n",
    "users_common_acc2 = [user for user in accuracy_answeronly if user in accuracy_withexplanation]\n",
    "x_acc = np.array([accuracy_answeronly[user] for user in users_common_acc2])\n",
    "y_acc_step2_only = np.array([accuracy_withexplanation[user] for user in users_common_acc2])\n",
    "\n",
    "# Prepare data for Step 3.\n",
    "users_common_acc3 = [user for user in accuracy_answeronly if user in accuracy_withexplanationquality]\n",
    "x_acc_3 = np.array([accuracy_answeronly[user] for user in users_common_acc3])\n",
    "y_acc_step3_only = np.array([accuracy_withexplanationquality[user] for user in users_common_acc3])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot: Step 1 Accuracy vs. Step 2 Accuracy.\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_acc, y_acc_step2_only, alpha=0.6)\n",
    "slope_a2, intercept_a2 = np.polyfit(x_acc, y_acc_step2_only, 1)\n",
    "x_line_a2 = np.linspace(x_acc.min(), x_acc.max(), 100)\n",
    "plt.plot(x_line_a2, slope_a2 * x_line_a2 + intercept_a2, color='red', label=f'y={slope_a2:.2f}x+{intercept_a2:.2f}')\n",
    "plt.xlabel(\"Accuracy in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Accuracy in With Explanation (Step 2)\")\n",
    "plt.title(\"Step 2 Accuracy vs. Step 1 Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Subplot: Step 1 Accuracy vs. Step 3 Accuracy.\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_acc_3, y_acc_step3_only, alpha=0.6, color='orange')\n",
    "slope_a3, intercept_a3 = np.polyfit(x_acc_3, y_acc_step3_only, 1)\n",
    "x_line_a3 = np.linspace(x_acc_3.min(), x_acc_3.max(), 100)\n",
    "plt.plot(x_line_a3, slope_a3 * x_line_a3 + intercept_a3, color='red', label=f'y={slope_a3:.2f}x+{intercept_a3:.2f}')\n",
    "plt.xlabel(\"Accuracy in Answer Only (Step 1)\")\n",
    "plt.ylabel(\"Accuracy in With Explanation + Quality (Step 3)\")\n",
    "plt.title(\"Step 3 Accuracy vs. Step 1 Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of settings.\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# Define the stages.\n",
    "stages = ['answeronly', 'withexplanation', 'withexplanationquality']\n",
    "\n",
    "# Create a list to hold all records.\n",
    "records = []\n",
    "\n",
    "# Loop over each setting and load the JSON files.\n",
    "for setting in settings:\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            # Each JSON file contains sessions keyed by user_id.\n",
    "            for user_id, session in batch_data.items():\n",
    "                interactions = session.get('interactions', [])\n",
    "                # Loop over each question in the batch.\n",
    "                for q_idx, interaction in enumerate(interactions):\n",
    "                    # For each stage in the question, record the user selection.\n",
    "                    for stage in stages:\n",
    "                        selection = interaction.get('user_selections', {}).get(stage, None)\n",
    "                        if selection is not None:\n",
    "                            records.append({\n",
    "                                \"setting\": setting,\n",
    "                                \"user_id\": user_id,\n",
    "                                \"question_idx\": q_idx + 1,  # Use 1-index for question numbers.\n",
    "                                \"stage\": stage,\n",
    "                                \"selection\": selection,\n",
    "                                \"is_unsure\": 1 if selection == 2 else 0\n",
    "                            })\n",
    "\n",
    "# Convert the list of records into a pandas DataFrame.\n",
    "df = pd.DataFrame(records)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sessions across all settings by user.\n",
    "aggregated_data = {}  # key: user_id, value: list of sessions\n",
    "for setting in settings:\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            for user_id, session in batch_data.items():\n",
    "                # Record the setting in the session so we know which task the session belongs to.\n",
    "                session['setting'] = setting\n",
    "                aggregated_data.setdefault(user_id, []).append(session)\n",
    "print(\"\\nUsers with <20% unsure rate in Answer Only (Step 1):\")\n",
    "for user, sessions in aggregated_data.items():\n",
    "    # Collect all interactions and settings for this user.\n",
    "    interactions = []\n",
    "    user_settings = set()\n",
    "    for session in sessions:\n",
    "        interactions.extend(session['interactions'])\n",
    "        user_settings.add(session['setting'])\n",
    "    # Compute unsure rate and accuracy for Step 1.\n",
    "    rate_ans = compute_unsure_rate(interactions, 'answeronly')\n",
    "    acc_step3 = compute_user_accuracy_for_stage(interactions, 'withexplanationquality')\n",
    "\n",
    "    \n",
    "    if rate_ans < 0.2:\n",
    "        print(f\"User ID: {user}\")\n",
    "        print(f\"  Unsure Rate (Step 1): {rate_ans:.2f}\")\n",
    "        print(f\"  Accuracy (Step 3): {acc_step3:.2f}\")\n",
    "        print(f\"  Settings: {list(user_settings)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by setting, stage, and question index to compute average unsure rate.\n",
    "grouped = df.groupby(['setting', 'stage', 'question_idx'])['is_unsure'].mean().reset_index()\n",
    "# Convert the unsure rate to percentage.\n",
    "grouped['unsure_rate_percent'] = grouped['is_unsure'] * 100\n",
    "\n",
    "print(\"\\nGrouped average unsure rates (first few rows):\")\n",
    "print(grouped.head())\n",
    "\n",
    "# Create a 6x3 grid plot: rows correspond to settings, columns correspond to stages.\n",
    "fig, axes = plt.subplots(nrows=len(settings), ncols=3, figsize=(18, 25), sharex=True, sharey=True)\n",
    "\n",
    "for i, setting in enumerate(settings):\n",
    "    for j, stage in enumerate(stages):\n",
    "        ax = axes[i, j]\n",
    "        # Filter data for the current setting and stage.\n",
    "        df_subset = grouped[(grouped['setting'] == setting) & (grouped['stage'] == stage)]\n",
    "        ax.plot(df_subset['question_idx'], df_subset['unsure_rate_percent'], marker='o')\n",
    "        ax.set_title(f\"{setting}\\n{stage}\")\n",
    "        ax.set_xlabel(\"Question Number\")\n",
    "        ax.set_ylabel(\"Unsure Rate (%)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- All 6 settings in one graph (per stage) ---\n",
    "# Create a new figure with 3 subplots (one for each stage) and plot all settings on each.\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "for j, stage in enumerate(stages):\n",
    "    ax = axes[j]\n",
    "    for setting in settings:\n",
    "        df_subset = grouped[(grouped['setting'] == setting) & (grouped['stage'] == stage)]\n",
    "        ax.plot(df_subset['question_idx'], df_subset['unsure_rate_percent'], marker='o', label=setting)\n",
    "    ax.set_title(f\"Stage: {stage}\")\n",
    "    ax.set_xlabel(\"Question Number\")\n",
    "    ax.set_ylabel(\"Unsure Rate (%)\")\n",
    "    ax.legend(fontsize='small', loc='best')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Average across all settings ---\n",
    "# First, compute the average unsure rate across all settings for each stage and question index.\n",
    "avg_grouped = grouped.groupby(['stage', 'question_idx'])['unsure_rate_percent'].mean().reset_index()\n",
    "\n",
    "# Plot the average across settings for each stage in one graph.\n",
    "plt.figure(figsize=(8, 6))\n",
    "for stage in stages:\n",
    "    df_stage_avg = avg_grouped[avg_grouped['stage'] == stage]\n",
    "    plt.plot(df_stage_avg['question_idx'], df_stage_avg['unsure_rate_percent'], marker='o', label=stage)\n",
    "plt.xlabel(\"Question Number\")\n",
    "plt.ylabel(\"Average Unsure Rate (%)\")\n",
    "plt.title(\"Average Unsure Rate across All Settings\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot separate graphs for each setting.\n",
    "unique_settings = grouped['setting'].unique()\n",
    "\n",
    "for setting in unique_settings:\n",
    "    # Filter data for the current setting.\n",
    "    df_setting = grouped[grouped['setting'] == setting]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot one line per stage.\n",
    "    for stage in stages:\n",
    "        df_stage = df_setting[df_setting['stage'] == stage]\n",
    "        plt.plot(df_stage['question_idx'], df_stage['unsure_rate_percent'], \n",
    "                 marker='o', label=stage)\n",
    "    \n",
    "    plt.xlabel(\"Question Number\")\n",
    "    plt.ylabel(\"Average Unsure Rate (%)\")\n",
    "    plt.title(f\"Unsure Rate over Questions for Setting: {setting}\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(values):\n",
    "    \"\"\"Return the standard error of the given values.\"\"\"\n",
    "    n = len(values)\n",
    "    return np.std(values, ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_unsure_rate(interactions, stage):\n",
    "    \"\"\"Compute the unsure rate (proportion of responses == 2) and its standard error.\"\"\"\n",
    "    preds = np.array([x['user_selections'][stage] for x in interactions])\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    mean_unsure = np.mean(unsure_indicator)\n",
    "    se_unsure = compute_se(unsure_indicator)\n",
    "    return mean_unsure, se_unsure\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    \"\"\"\n",
    "    For a given stage and list of interactions, compute:\n",
    "      - total accuracy (all instances, counting unsure as incorrect)\n",
    "      - accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - unsure rate\n",
    "      - utility (0 if unsure, 1 if correct, -1 if incorrect)\n",
    "    Returns a dictionary with means, standard errors and additional per-instance values.\n",
    "    \"\"\"\n",
    "    # Ground truth: 0 means AI is correct; 1 means AI is incorrect.\n",
    "    ground_truths = np.array([1 - x['question']['prediction_is_correct'] for x in instances])\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    \n",
    "    # Total accuracy: count unsure (value 2) as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == ground_truths)).astype(float)\n",
    "    total_acc_mean = np.mean(correct_all)\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy for non-unsure responses.\n",
    "    non_unsure_mask = (preds != 2)\n",
    "    if np.sum(non_unsure_mask) > 0:\n",
    "        correct_non_unsure = (preds[non_unsure_mask] == ground_truths[non_unsure_mask]).astype(float)\n",
    "        acc_mean = np.mean(correct_non_unsure)\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "    \n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_rate_mean = np.mean(unsure_indicator)\n",
    "    unsure_rate_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility: 0 if unsure; 1 if correct; -1 if incorrect.\n",
    "    individual_utilities = np.array([\n",
    "        0 if p == 2 else 1 - 2 * abs(p - gt)\n",
    "        for p, gt in zip(preds, ground_truths)\n",
    "    ])\n",
    "    util_mean = np.mean(individual_utilities)\n",
    "    util_se = compute_se(individual_utilities)\n",
    "    \n",
    "    return {\n",
    "        'total_accuracy_mean': total_acc_mean,\n",
    "        'total_accuracy_se': total_acc_se,\n",
    "        'accuracy_mean': acc_mean,\n",
    "        'accuracy_se': acc_se,\n",
    "        'unsure_rate_mean': unsure_rate_mean,\n",
    "        'unsure_rate_se': unsure_rate_se,\n",
    "        'utility_mean': util_mean,\n",
    "        'utility_se': util_se,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Settings & Data Directory\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# ---------------------------\n",
    "# Define the Unsure-Rate Thresholds to Try\n",
    "# ---------------------------\n",
    "unsure_thresholds = [0.0, 0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "\n",
    "# This list will collect results for each threshold and setting.\n",
    "all_results = []\n",
    "\n",
    "# Loop over each filtering criterion.\n",
    "for threshold in unsure_thresholds:\n",
    "    print(f\"\\n\\nProcessing for unsure_rate_threshold = {threshold}\")\n",
    "    for setting in settings:\n",
    "        print(f\"  Processing setting: {setting}\")\n",
    "        pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        # Aggregate sessions for each user.\n",
    "        data = {}  # key: user_id, value: list of sessions\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                batch_data = json.load(f)\n",
    "                for user_id, session in batch_data.items():\n",
    "                    data.setdefault(user_id, []).append(session)\n",
    "                    \n",
    "        # Collect all interactions per user who pass the threshold.\n",
    "        filtered_instances = []\n",
    "        for user, sessions in data.items():\n",
    "            interactions = []\n",
    "            for session in sessions:\n",
    "                interactions.extend(session['interactions'])\n",
    "            mean_unsure, _ = compute_unsure_rate(interactions, 'answeronly')\n",
    "            # Use the current threshold filtering.\n",
    "            if mean_unsure >= threshold:\n",
    "                filtered_instances.extend(interactions)\n",
    "                \n",
    "        # If no interactions remain, record NaN scores.\n",
    "        if len(filtered_instances) == 0:\n",
    "            print(f\"    No interactions remain after filtering. Setting scores to NaN.\")\n",
    "            m1 = m2 = m3 = m4 = m5 = np.nan\n",
    "        else:\n",
    "            # Evaluate each stage.\n",
    "            answeronly_results = evaluate_answers('answeronly', filtered_instances)\n",
    "            withexplanation_results = evaluate_answers('withexplanation', filtered_instances)\n",
    "            withexplanationquality_results = evaluate_answers('withexplanationquality', filtered_instances)\n",
    "            \n",
    "            # ---------------------------\n",
    "            # Compute M1–M4 from Step 2 (With Explanation) to Step 3 (With Explanation + Quality)\n",
    "            # ---------------------------\n",
    "            # For M1–M4, we first count transitions between utility values.\n",
    "            def count_transitions(val2, val3):\n",
    "                return sum(1 for u2, u3 in zip(withexplanation_results['individual_utilities'],\n",
    "                                                withexplanationquality_results['individual_utilities'])\n",
    "                           if u2 == val2 and u3 == val3)\n",
    "    \n",
    "            # M1: P(S3 ≠ U | S2 = U) -- from Answer Only unsure (utility 0)\n",
    "            denom_M1 = count_transitions(0, -1) + count_transitions(0, 0) + count_transitions(0, 1)\n",
    "            num_M1 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "            m1 = num_M1 / denom_M1 if denom_M1 > 0 else np.nan\n",
    "    \n",
    "            # M2: P(S3 = C | S2 = U, S3 ≠ U)\n",
    "            denom_M2 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "            m2 = count_transitions(0, 1) / denom_M2 if denom_M2 > 0 else np.nan\n",
    "    \n",
    "            # M3: P(S3 ≠ I | S2 = I) -- from Answer Only incorrect (utility -1)\n",
    "            denom_M3 = count_transitions(-1, -1) + count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "            num_M3 = count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "            m3 = num_M3 / denom_M3 if denom_M3 > 0 else np.nan\n",
    "    \n",
    "            # M4: P(S3 ≠ C | S2 = C) -- from Answer Only correct (utility 1)\n",
    "            denom_M4 = count_transitions(1, -1) + count_transitions(1, 0) + count_transitions(1, 1)\n",
    "            num_M4 = count_transitions(1, -1) + count_transitions(1, 0)\n",
    "            m4 = num_M4 / denom_M4 if denom_M4 > 0 else np.nan\n",
    "            \n",
    "            # M5: Define Recovery Gap as the difference (M3 – M4)\n",
    "            m5 = m3 - m4 if (not np.isnan(m3) and not np.isnan(m4)) else np.nan\n",
    "            \n",
    "        all_results.append({\n",
    "            \"Threshold\": threshold,\n",
    "            \"Setting\": setting,\n",
    "            \"SettingShort\": setting.split('_llava')[0],\n",
    "            \"M1\": m1,\n",
    "            \"M2\": m2,\n",
    "            \"M3\": m3,\n",
    "            \"M4\": m4,\n",
    "            \"M5\": m5,\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results.\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\nCollected M1–M5 scores:\")\n",
    "print(df_results)\n",
    "\n",
    "# ---------------------------\n",
    "# Plot: For Each Threshold, a Grouped Bar Plot by Setting\n",
    "# ---------------------------\n",
    "unique_thresholds = sorted(df_results[\"Threshold\"].unique())\n",
    "n_thresh = len(unique_thresholds)\n",
    "ncols = 3\n",
    "nrows = math.ceil(n_thresh / ncols)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, thresh in enumerate(unique_thresholds):\n",
    "    ax = axes[i]\n",
    "    # Filter the data for the current threshold\n",
    "    df_thresh = df_results[df_results[\"Threshold\"] == thresh].set_index(\"SettingShort\")\n",
    "    # Only keep the M1–M5 scores.\n",
    "    df_plot = df_thresh[['M1', 'M2', 'M3', 'M4', 'M5']]\n",
    "    df_plot.plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f\"Threshold = {thresh}\")\n",
    "    ax.set_ylabel(\"Score (Fraction)\")\n",
    "    ax.set_ylim([0, 1]) \n",
    "\n",
    "    labels = [tick.get_text() for tick in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels, rotation=60)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "    ax.legend(title=\"Metric\", fontsize=8, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Hide any extra subplots if the grid is larger than the number of thresholds.\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(values):\n",
    "    \"\"\"Return the standard error of the given values.\"\"\"\n",
    "    n = len(values)\n",
    "    return np.std(values, ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_unsure_rate(interactions, stage):\n",
    "    \"\"\"Compute the unsure rate (proportion of responses == 2) and its standard error.\"\"\"\n",
    "    preds = np.array([x['user_selections'][stage] for x in interactions])\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    mean_unsure = np.mean(unsure_indicator)\n",
    "    se_unsure = compute_se(unsure_indicator)\n",
    "    return mean_unsure, se_unsure\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    \"\"\"\n",
    "    For a given stage and list of interactions, compute:\n",
    "      - total accuracy (all instances, counting unsure as incorrect)\n",
    "      - accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - unsure rate\n",
    "      - utility (0 if unsure, 1 if correct, -1 if incorrect)\n",
    "    Returns a dictionary with means, standard errors and additional per-instance values.\n",
    "    \"\"\"\n",
    "    # Ground truth: 0 means AI is correct; 1 means AI is incorrect.\n",
    "    ground_truths = np.array([1 - x['question']['prediction_is_correct'] for x in instances])\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    \n",
    "    # Total accuracy: count unsure (value 2) as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == ground_truths)).astype(float)\n",
    "    total_acc_mean = np.mean(correct_all)\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy for non-unsure responses.\n",
    "    non_unsure_mask = (preds != 2)\n",
    "    if np.sum(non_unsure_mask) > 0:\n",
    "        correct_non_unsure = (preds[non_unsure_mask] == ground_truths[non_unsure_mask]).astype(float)\n",
    "        acc_mean = np.mean(correct_non_unsure)\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "    \n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_rate_mean = np.mean(unsure_indicator)\n",
    "    unsure_rate_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility (here treated as the \"accuracy distribution\"):\n",
    "    individual_utilities = np.array([\n",
    "        0 if p == 2 else 1 - 2 * abs(p - gt)\n",
    "        for p, gt in zip(preds, ground_truths)\n",
    "    ])\n",
    "    util_mean = np.mean(individual_utilities)\n",
    "    util_se = compute_se(individual_utilities)\n",
    "    \n",
    "    return {\n",
    "        'total_accuracy_mean': total_acc_mean,\n",
    "        'total_accuracy_se': total_acc_se,\n",
    "        'accuracy_mean': acc_mean,\n",
    "        'accuracy_se': acc_se,\n",
    "        'unsure_rate_mean': unsure_rate_mean,\n",
    "        'unsure_rate_se': unsure_rate_se,\n",
    "        'utility_mean': util_mean,\n",
    "        'utility_se': util_se,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Deduce Metadata from Setting Name\n",
    "# ---------------------------\n",
    "def deduce_setting_info(setting):\n",
    "    \"\"\"\n",
    "    Returns a tuple (study_setting, url_suffix) based on naming conventions.\n",
    "    Adjust this as needed.\n",
    "    \"\"\"\n",
    "    if \"numeric\" in setting:\n",
    "        if \"vf_\" in setting and \"contr\" not in setting and \"avg\" not in setting:\n",
    "            study_setting = \"VF only numeric\"\n",
    "            url_suffix = \"&quality_type=vf_only&quality_format=numeric\"\n",
    "        elif \"contr_\" in setting:\n",
    "            study_setting = \"Contr only numeric\"\n",
    "            url_suffix = \"&quality_type=contr_only&quality_format=numeric\"\n",
    "        elif \"showbothmetrics\" in setting:\n",
    "            study_setting = \"VF & Contr both numeric\"\n",
    "            url_suffix = \"&quality_type=vf_contr_both&quality_format=numeric\"\n",
    "        elif \"avg_vf_contr\" in setting:\n",
    "            study_setting = \"Avg(VF Contr)\"\n",
    "            url_suffix = \"&quality_type=single_numeric&quality_format=numeric\"\n",
    "        else:\n",
    "            study_setting = setting\n",
    "            url_suffix = \"\"\n",
    "    elif \"descriptive\" in setting:\n",
    "        if \"vf_\" in setting and \"contr\" not in setting and \"avg\" not in setting:\n",
    "            study_setting = \"VF only descriptive\"\n",
    "            url_suffix = \"&quality_type=vf_only&quality_format=descriptive\"\n",
    "        elif \"contr_\" in setting:\n",
    "            study_setting = \"Contr only descriptive\"\n",
    "            url_suffix = \"&quality_type=contr_only&quality_format=descriptive\"\n",
    "        elif \"showbothmetrics\" in setting:\n",
    "            study_setting = \"VF & Contr both descriptive\"\n",
    "            url_suffix = \"&quality_type=vf_contr_both&quality_format=descriptive\"\n",
    "        else:\n",
    "            study_setting = setting\n",
    "            url_suffix = \"\"\n",
    "    elif \"showbothmetrics\" in setting and \"descriptive\" not in setting:\n",
    "        # Process as numeric\n",
    "        study_setting = \"VF & Contr both numeric\"\n",
    "        url_suffix = \"&quality_type=vf_contr_both&quality_format=numeric\"\n",
    "    else:\n",
    "        study_setting = setting\n",
    "        url_suffix = \"\"\n",
    "    return study_setting, url_suffix\n",
    "\n",
    "# ---------------------------\n",
    "# Settings & Directory\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# ---------------------------\n",
    "# Define the Unsure-Rate Thresholds to Try\n",
    "# ---------------------------\n",
    "unsure_thresholds = [0.0, 0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "\n",
    "# Assume fixed metadata across settings/experiments\n",
    "dataset_name = \"A-OKVQA\"\n",
    "vlm_name = \"LLaVA-1.5\"\n",
    "\n",
    "# Print CSV header (including a \"Filtering\" column to identify the threshold)\n",
    "csv_header = [\n",
    "    \"Filtering\", \"User study setting\", \"URL suffix\", \"# Annotations\", \"# Unique Users\",\n",
    "    \"After Stage 1 Unsure Rate\", \"After Stage 1 Total Acc\", \"After Stage 1 Not Unsure Acc\", \"After Stage 1 Acc. Distribution\",\n",
    "    \"After Stage 2 Unsure Rate\", \"After Stage 2 Total Acc\", \"After Stage 2 Not Unsure Acc\", \"After Stage 2 Acc. Distribution\",\n",
    "    \"After Stage 3 Unsure Rate\", \"After Stage 3 Total Acc\", \"After Stage 3 Not Unsure Acc\", \"After Stage 3 Acc. Distribution\"\n",
    "]\n",
    "print(\",\".join(csv_header))\n",
    "\n",
    "# Loop over each filtering criterion and study setting\n",
    "for threshold in unsure_thresholds:\n",
    "    for setting in settings:\n",
    "        pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        # Aggregate sessions for each user.\n",
    "        data = {}  # key: user_id, value: list of sessions\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                batch_data = json.load(f)\n",
    "                for user_id, session in batch_data.items():\n",
    "                    data.setdefault(user_id, []).append(session)\n",
    "        \n",
    "        # Collect all interactions per user that pass the filtering criterion\n",
    "        filtered_instances = []\n",
    "        passed_users = set()\n",
    "        for user, sessions in data.items():\n",
    "            user_interactions = []\n",
    "            for session in sessions:\n",
    "                user_interactions.extend(session['interactions'])\n",
    "            mean_unsure, _ = compute_unsure_rate(user_interactions, 'answeronly')\n",
    "            if mean_unsure >= threshold:\n",
    "                filtered_instances.extend(user_interactions)\n",
    "                passed_users.add(user)\n",
    "        \n",
    "        num_annotations = len(filtered_instances)\n",
    "        num_users = len(passed_users)\n",
    "        \n",
    "        # Prepare strings for stage metrics.\n",
    "        # If no interactions remain, mark all metrics as \"N/A\"\n",
    "        if num_annotations == 0:\n",
    "            s1_unsure = s1_total = s1_not_unsure = s1_util = \"N/A\"\n",
    "            s2_unsure = s2_total = s2_not_unsure = s2_util = \"N/A\"\n",
    "            s3_unsure = s3_total = s3_not_unsure = s3_util = \"N/A\"\n",
    "        else:\n",
    "            res1 = evaluate_answers('answeronly', filtered_instances)\n",
    "            res2 = evaluate_answers('withexplanation', filtered_instances)\n",
    "            res3 = evaluate_answers('withexplanationquality', filtered_instances)\n",
    "            \n",
    "            s1_unsure    = f\"{res1['unsure_rate_mean']*100:.1f}% ± {res1['unsure_rate_se']*100:.1f}%\"\n",
    "            s1_total     = f\"{res1['total_accuracy_mean']*100:.1f}% ± {res1['total_accuracy_se']*100:.1f}%\"\n",
    "            s1_not_unsure = (f\"{res1['accuracy_mean']*100:.1f}% ± {res1['accuracy_se']*100:.1f}%\" \n",
    "                             if not np.isnan(res1['accuracy_mean']) else \"N/A\")\n",
    "            s1_util      = f\"{res1['utility_mean']:.3f} ± {res1['utility_se']:.3f}\"\n",
    "    \n",
    "            s2_unsure    = f\"{res2['unsure_rate_mean']*100:.1f}% ± {res2['unsure_rate_se']*100:.1f}%\"\n",
    "            s2_total     = f\"{res2['total_accuracy_mean']*100:.1f}% ± {res2['total_accuracy_se']*100:.1f}%\"\n",
    "            s2_not_unsure = (f\"{res2['accuracy_mean']*100:.1f}% ± {res2['accuracy_se']*100:.1f}%\" \n",
    "                             if not np.isnan(res2['accuracy_mean']) else \"N/A\")\n",
    "            s2_util      = f\"{res2['utility_mean']:.3f} ± {res2['utility_se']:.3f}\"\n",
    "    \n",
    "            s3_unsure    = f\"{res3['unsure_rate_mean']*100:.1f}% ± {res3['unsure_rate_se']*100:.1f}%\"\n",
    "            s3_total     = f\"{res3['total_accuracy_mean']*100:.1f}% ± {res3['total_accuracy_se']*100:.1f}%\"\n",
    "            s3_not_unsure = (f\"{res3['accuracy_mean']*100:.1f}% ± {res3['accuracy_se']*100:.1f}%\" \n",
    "                             if not np.isnan(res3['accuracy_mean']) else \"N/A\")\n",
    "            s3_util      = f\"{res3['utility_mean']:.3f} ± {res3['utility_se']:.3f}\"\n",
    "        \n",
    "        # Deduce study setting name and URL suffix from the setting string.\n",
    "        study_setting, url_suffix = deduce_setting_info(setting)\n",
    "        \n",
    "        # Create the CSV row (as a list of strings)\n",
    "        row = [\n",
    "            f\"{threshold:.1f}\",\n",
    "            study_setting,\n",
    "            url_suffix,\n",
    "            str(num_annotations),\n",
    "            str(num_users),\n",
    "            s1_unsure,\n",
    "            s1_total,\n",
    "            s1_not_unsure,\n",
    "            s1_util,\n",
    "            s2_unsure,\n",
    "            s2_total,\n",
    "            s2_not_unsure,\n",
    "            s2_util,\n",
    "            s3_unsure,\n",
    "            s3_total,\n",
    "            s3_not_unsure,\n",
    "            s3_util\n",
    "        ]\n",
    "        # Print the row as a comma separated line.\n",
    "        print(\",\".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval_expl",
   "language": "python",
   "name": "eval_expl"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
