{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Calculate classifier calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_filename = \"../data/llava-1.5_with_image.json\"\n",
    "data = json.load(open(data_filename, \"r\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_fidelities = np.array([x['visual_fidelity'] for x in data])\n",
    "contrastiveness_scores = np.array([x['contrastiveness'] for x in data])\n",
    "correctnesses = np.array([x['prediction_is_correct'] for x in data])\n",
    "\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, ax, title):\n",
    "    bin_num_positives, bin_mean_probs, bin_num_instances, bin_centers = [], [], [], []\n",
    "    for i in range(10):\n",
    "        bin_lower = i/10\n",
    "        bin_upper = (i+1)/10 if i != 9 else 1.01\n",
    "        bin_indices = [j for j in range(len(y_prob)) if bin_lower <= y_prob[j] < bin_upper]\n",
    "        if len(bin_indices) != 0:\n",
    "            bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "            bin_num_instances.append(len(bin_indices))\n",
    "            bin_centers.append((i+0.5)/10)\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "            bin_centers.append((i+0.5)/10)\n",
    "\n",
    "    # Calculate expected calibration error\n",
    "    ece = 0\n",
    "    for i in range(len(bin_num_positives)):\n",
    "        ece += bin_num_instances[i] * np.abs(bin_num_positives[i] - bin_mean_probs[i])\n",
    "    ece /= sum(bin_num_instances)\n",
    "\n",
    "    #print(bin_num_positives, bin_mean_probs, bin_num_instances)\n",
    "    df = pd.DataFrame({'bin_num_positives': bin_num_positives, 'bin_centers': bin_centers, 'bin_num_instances': bin_num_instances})\n",
    "    sns.barplot(x='bin_centers', y='bin_num_positives', data=df, ax=ax, hue='bin_num_instances', palette='crest', edgecolor='black', linewidth=1.2, width=1, hue_norm=(0, 200))\n",
    "    ax.grid(axis='y')\n",
    "    ax.plot([-0.5, 9.5], [0, 1], \"k--\")\n",
    "    #ax.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "    ax.set_xlim(-0.5, 9.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"Metric Score\", fontsize='x-large')\n",
    "    ax.set_ylabel(\"Prediction Accuracy\", fontsize='x-large')\n",
    "    ax.set_title(f\"{title}\\nCalibration Error = {ece:.4f}\", fontsize='large')\n",
    "    ax.set_xticks(np.arange(-0.5, 10.5, 1))\n",
    "    ax.set_xticklabels([i/10 for i in range(11)])\n",
    "    \n",
    "    ax.legend().remove()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(18, 4), dpi=200)\n",
    "fig.suptitle(\"Calibration of VisualFidelity and Contrastiveness metrics\", fontsize='xx-large')\n",
    "plot_calibration_curve(correctnesses, visual_fidelities, ax[0], \"VisualFidelity\")\n",
    "plot_calibration_curve(correctnesses, contrastiveness_scores, ax[1], \"Contrastiveness\")\n",
    "plot_calibration_curve(correctnesses, np.array([(x+y)/2 for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[2], \"(VisualFidelity + Contrastiveness)/2\")\n",
    "plot_calibration_curve(correctnesses, np.array([min(x, y) for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[3], \"min(VisualFidelity, Contrastiveness)\")\n",
    "plot_calibration_curve(correctnesses, np.array([x*y for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[4], \"VisualFidelity * Contrastiveness\")\n",
    "\n",
    "norm = plt.Normalize(0, 200)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"crest\", norm=norm)\n",
    "ax[4].figure.colorbar(sm, ax=ax[4])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_distributions(y_true, y_prob, title, ax):\n",
    "    y_prob_when_correct = y_prob[y_true == 1]\n",
    "    y_prob_when_incorrect = y_prob[y_true == 0]\n",
    "    ax.hist(y_prob_when_correct, bins=10, range=(0, 1), color='#006164', edgecolor='black', linewidth=1.2, histtype='bar', density=False, alpha=0.7)\n",
    "    ax.hist(y_prob_when_incorrect, bins=10, range=(0, 1), color='#db4325', edgecolor='black', linewidth=1.2, histtype='bar', density=False, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Metric Score\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(16, 4), dpi=200)\n",
    "fig.suptitle(\"Confidence Distributions of VisualFidelity and Contrastiveness metrics\", fontsize='xx-large')\n",
    "plot_confidence_distributions(correctnesses, visual_fidelities, \"VisualFidelity\", ax[0])\n",
    "plot_confidence_distributions(correctnesses, contrastiveness_scores, \"Contrastiveness\", ax[1])\n",
    "plot_confidence_distributions(correctnesses, np.array([(x+y)/2 for x, y in zip(visual_fidelities, contrastiveness_scores)]), \"(VisualFidelity + Contrastiveness)/2\", ax[2])\n",
    "plot_confidence_distributions(correctnesses, np.array([min(x, y) for x, y in zip(visual_fidelities, contrastiveness_scores)]), \"min(VisualFidelity, Contrastiveness)\", ax[3])\n",
    "plot_confidence_distributions(correctnesses, np.array([x*y for x, y in zip(visual_fidelities, contrastiveness_scores)]), \"VisualFidelity * Contrastiveness\", ax[4])\n",
    "\n",
    "ax[4].legend([\"Conf when VLM Correct\", \"Conf when VLM Incorrect\"])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 1.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
