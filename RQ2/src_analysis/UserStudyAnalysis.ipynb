{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Steps Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(series):\n",
    "    \"\"\"Compute the standard error on a pandas Series.\"\"\"\n",
    "    n = series.count()\n",
    "    return series.std(ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_metrics(df, stage):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of interactions (with a column for the stage and a column for ground_truth),\n",
    "    compute:\n",
    "      - Total accuracy (unsure responses, coded as 2, count as incorrect)\n",
    "      - Accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - Unsure rate\n",
    "      - Utility (0 if unsure; 1 if correct; -1 if incorrect)\n",
    "    Returns also the computed utility as a Series.\n",
    "    \"\"\"\n",
    "    preds = df[stage]\n",
    "    # Total accuracy: count unsure as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == df['ground_truth'])).astype(float)\n",
    "    total_acc_mean = correct_all.mean()\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy on non-unsure responses.\n",
    "    non_unsure = preds != 2\n",
    "    if non_unsure.sum() > 0:\n",
    "        correct_non_unsure = (df.loc[non_unsure, stage] == df.loc[non_unsure, 'ground_truth']).astype(float)\n",
    "        acc_mean = correct_non_unsure.mean()\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "\n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_mean = unsure_indicator.mean()\n",
    "    unsure_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility: 0 if unsure; else 1 if correct, -1 if incorrect.\n",
    "    # (i.e., 1 - 2*abs(pred - ground_truth))\n",
    "    def calc_util(row):\n",
    "        p = row[stage]\n",
    "        gt = row['ground_truth']\n",
    "        return 0 if p == 2 else 1 - 2*abs(p - gt)\n",
    "    utility = df.apply(calc_util, axis=1)\n",
    "    util_mean = utility.mean()\n",
    "    util_se = compute_se(utility)\n",
    "    \n",
    "    return total_acc_mean, total_acc_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, utility\n",
    "\n",
    "# ---------------------------\n",
    "# Settings and Directories\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'vf_descriptive_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'combined12_llava1.5_with_image_q20_i10_s0',\n",
    "    'combined123_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# To store scores and confusion matrices across settings\n",
    "all_setting_scores = []\n",
    "all_confusion_matrices = []\n",
    "consistent_choice_threshold = 0.9  # 90% threshold\n",
    "df_map = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Process Each Setting via DataFrame\n",
    "# ---------------------------\n",
    "for setting in settings:\n",
    "    print(f\"\\nProcessing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    # Create a list of records (each record is an interaction)\n",
    "    records = []\n",
    "    for file in files:\n",
    "        batch_id = os.path.basename(file).split('.')[0]\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "        # Each file contains sessions for multiple users.\n",
    "        for user_id, session in batch_data.items():\n",
    "            interactions = session['interactions']\n",
    "            for interaction in interactions:\n",
    "                rec = {\n",
    "                    'user_id': user_id,\n",
    "                    'batch': batch_id,\n",
    "                    'question_i': interaction['question_i'],\n",
    "                    'answeronly': interaction['user_selections'].get('answeronly'),\n",
    "                    'withexplanation': interaction['user_selections'].get('withexplanation'),\n",
    "                    'withexplanationquality': interaction['user_selections'].get('withexplanationquality'),\n",
    "                    # Ground truth: defined as 1 - prediction_is_correct.\n",
    "                    'ground_truth': 1 - interaction['question']['prediction_is_correct']\n",
    "                }\n",
    "                records.append(rec)\n",
    "    \n",
    "    # Build DataFrame from the records.\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Found {df['user_id'].nunique()} users and {len(df)} interactions for setting '{setting}'.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Filter Users Based on Consistency\n",
    "    # ---------------------------\n",
    "    # For each user, look at all responses across all stages and drop users\n",
    "    # where one response (0, 1, or 2) comprises ≥90% of their responses.\n",
    "    def user_is_valid(group):\n",
    "        responses = pd.concat([group['answeronly'], group['withexplanation'], group['withexplanationquality']])\n",
    "        if responses.empty:\n",
    "            return False\n",
    "        norm_counts = responses.value_counts(normalize=True)\n",
    "        if norm_counts.max() >= consistent_choice_threshold:\n",
    "            # If desired, you could print which user is filtered out.\n",
    "            # print(f\"User {group.name} filtered out; response distribution: {norm_counts.to_dict()}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_valid = df.groupby('user_id').filter(user_is_valid)\n",
    "\n",
    "    if df_valid.empty:\n",
    "        print(\"No interactions remain after filtering. Skipping evaluation for this setting.\")\n",
    "        continue\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Filter Batches with More Than 30 Annotations\n",
    "    # ---------------------------\n",
    "    # For each batch, if it has been annotated more than 30 times (i.e. more than three users,\n",
    "    # assuming each user annotates about 10 interactions), print the annotation count\n",
    "    # and then shorten the batch to include only the first 30 annotations.\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count > 30:\n",
    "            print(f\"Batch {batch} has been annotated {count} times, exceeding 30 annotations. Shortening to first 30 annotations.\")\n",
    "    # Group by 'batch' and take only the first 30 entries for each batch\n",
    "    df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
    "    print(f\"{df_valid['user_id'].nunique()} users remain after filtering; {len(df_valid)} interactions remain.\")\n",
    "    \n",
    "    \n",
    "    # Indicate which batches need to add users\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count % 10: raise ValueError('# Annotation should be divisible by 10, but found not??')\n",
    "        count = count // 10\n",
    "        if count > 3:\n",
    "            raise ValueError(f\"After filtering, # users taking the batch should not > 3??\")\n",
    "        diff = 3 - count\n",
    "        if diff != 0:\n",
    "            print(f\"Batch {batch} need {diff} more users to annotate.\")\n",
    "            \n",
    "    # Add df to the df_map for later use.\n",
    "    df_map[setting] = df_valid\n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Metrics for Each Stage Using DataFrames\n",
    "    # ---------------------------\n",
    "    stage_names = ['Answer Only', 'With Explanation', 'With Explanation + Quality']\n",
    "    stages = ['answeronly', 'withexplanation', 'withexplanationquality']\n",
    "    metrics = {}\n",
    "    utilities = {}\n",
    "    csv_prints = []\n",
    "    print(\"\\nStage                    \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "    for stage, name in zip(stages, stage_names):\n",
    "        tot_acc, tot_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, util_series = compute_metrics(df_valid, stage)\n",
    "        metrics[stage] = {\n",
    "            'total_accuracy_mean': tot_acc,\n",
    "            'total_accuracy_se': tot_se,\n",
    "            'accuracy_mean': acc_mean,\n",
    "            'accuracy_se': acc_se,\n",
    "            'unsure_rate_mean': unsure_mean,\n",
    "            'unsure_rate_se': unsure_se,\n",
    "            'utility_mean': util_mean,\n",
    "            'utility_se': util_se\n",
    "        }\n",
    "        utilities[stage] = util_series\n",
    "        print(f\"{name:<25}\\t{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%\\t\"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%\\t\"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%\\t\"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        csv_prints.append(f\"{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%, \"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%, \"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%, \"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        \n",
    "    print(\", \".join(csv_prints))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utility Gains Between Stages\n",
    "    # ---------------------------\n",
    "    gain_explanation = utilities['withexplanation'] - utilities['answeronly']\n",
    "    gain_quality = utilities['withexplanationquality'] - utilities['withexplanation']\n",
    "    expl_gain_mean = gain_explanation.mean()\n",
    "    expl_gain_se = gain_explanation.std(ddof=1) / np.sqrt(len(gain_explanation)) if len(gain_explanation) > 1 else 0\n",
    "    qual_gain_mean = gain_quality.mean()\n",
    "    qual_gain_se = gain_quality.std(ddof=1) / np.sqrt(len(gain_quality)) if len(gain_quality) > 1 else 0\n",
    "    \n",
    "    print(f\"\\nSetting: {setting}\")\n",
    "    print(\"Stage\\t\\tUnsure Rate\\tUtility over previous stage\")\n",
    "    print(f\"Explanation\\t {metrics['withexplanation']['unsure_rate_mean']*100:5.1f}%\\t\\t {expl_gain_mean:+.3f} ± {expl_gain_se:.3f}\")\n",
    "    print(f\"Quality\\t\\t {metrics['withexplanationquality']['unsure_rate_mean']*100:5.1f}%\\t\\t {qual_gain_mean:+.3f} ± {qual_gain_se:.3f}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Confusion Matrix and M1–M4 Scores\n",
    "    # ---------------------------\n",
    "    # Compute utility for withexplanation and withexplanationquality directly.\n",
    "    def compute_utility(val, gt):\n",
    "        return 0 if val == 2 else 1 - 2*abs(val-gt)\n",
    "    \n",
    "    df_valid['util_withexplanation'] = df_valid.apply(lambda row: compute_utility(row['withexplanation'], row['ground_truth']), axis=1)\n",
    "    df_valid['util_withexplanationquality'] = df_valid.apply(lambda row: compute_utility(row['withexplanationquality'], row['ground_truth']), axis=1)\n",
    "    \n",
    "    # For the confusion matrix, map utilities to indices: -1 -> 0, 0 -> 1, 1 -> 2.\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    label_order = [-1, 0, 1]\n",
    "    label_names = ['Incorrect', 'Unsure', 'Correct']\n",
    "    \n",
    "    try:\n",
    "        cm = np.zeros((3, 3), dtype=int)\n",
    "        for _, row in df_valid.iterrows():\n",
    "            u2 = row['util_withexplanation']\n",
    "            u3 = row['util_withexplanationquality']\n",
    "            cm[mapping[u2], mapping[u3]] += 1\n",
    "        cm_transposed = cm.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing confusion matrix or M1–M4 scores: {e}\")\n",
    "        print(f\"This would be expected if setting {setting} is combined12 or combined123.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nSetting: {setting} Step 2 -> Step 3 (transposed)\")\n",
    "    print(\"\\t\\t\" + \"\\t\".join(label_names))\n",
    "    for i, label in enumerate(label_names):\n",
    "        row_values = \"\\t\".join(str(cm_transposed[i, j]) for j in range(3))\n",
    "        print(f\"{label:<10}\\t {row_values}\")\n",
    "    \n",
    "    # Compute M1–M4 using counts from the confusion matrix.\n",
    "    def count_transitions(val2, val3):\n",
    "        return ((df_valid['util_withexplanation'] == val2) & (df_valid['util_withexplanationquality'] == val3)).sum()\n",
    "    \n",
    "    denom_M1 = count_transitions(0, -1) + count_transitions(0, 0) + count_transitions(0, 1)\n",
    "    num_M1 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m1 = num_M1 / denom_M1 if denom_M1 > 0 else np.nan\n",
    "\n",
    "    denom_M2 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m2 = count_transitions(0, 1) / denom_M2 if denom_M2 > 0 else np.nan\n",
    "\n",
    "    denom_M3 = count_transitions(-1, -1) + count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    num_M3 = count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    m3 = num_M3 / denom_M3 if denom_M3 > 0 else np.nan\n",
    "\n",
    "    denom_M4 = count_transitions(1, -1) + count_transitions(1, 0) + count_transitions(1, 1)\n",
    "    num_M4 = count_transitions(1, -1) + count_transitions(1, 0)\n",
    "    m4 = num_M4 / denom_M4 if denom_M4 > 0 else np.nan\n",
    "\n",
    "    print(\"\\nM1–M4 Scores:\")\n",
    "    print(f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\")\n",
    "    print(f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\")\n",
    "    print(f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\")\n",
    "    print(f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    print(f\"{m1:.2%}, {m2:.2%}, {m3:.2%}, {m4:.2%}, {m3-m4:.2%}\")\n",
    "\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Print Examples Where Step 2 is Correct and Step 3 is Incorrect\n",
    "    # ---------------------------\n",
    "    print(\"\\nExamples where Step 2 is correct and Step 3 is incorrect:\")\n",
    "    examples = df_valid[\n",
    "        (df_valid['withexplanation'] != 2) &\n",
    "        (df_valid['withexplanation'] == df_valid['ground_truth']) &\n",
    "        (df_valid['withexplanationquality'] != 2) &\n",
    "        (df_valid['withexplanationquality'] != df_valid['ground_truth'])\n",
    "    ]\n",
    "    if not examples.empty:\n",
    "        for idx, row in examples.head(5).iterrows():\n",
    "            print(row.to_json())\n",
    "    else:\n",
    "        print(\"No examples found.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot: Confusion Matrix and M1–M4 Scores for the Current Setting\n",
    "    # ---------------------------\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "    ax.set_title(f\"Setting {setting.split('_llava')[0]} Step 2->Step 3\\n(WithExplanation -> WithExplanationQuality)\")\n",
    "    ax.set_xticks(np.arange(len(label_names)))\n",
    "    ax.set_yticks(np.arange(len(label_names)))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_xlabel(\"WithExplanation\")\n",
    "    ax.set_ylabel(\"WithExplanationQuality\")\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # Plot M1–M4 scores as text.\n",
    "    ax = axes[1]\n",
    "    ax.axis('off')\n",
    "    score_text = (f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\\n\"\n",
    "                  f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\\n\"\n",
    "                  f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\\n\"\n",
    "                  f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    ax.text(0.5, 0.5, score_text, fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title(\"M1–M4 Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save scores and confusion matrix for later comparison/plotting.\n",
    "    all_setting_scores.append({\n",
    "        \"Setting\": setting,\n",
    "        \"Resolution Rate\": m1,\n",
    "        \"Positive Conversion Rate\": m2,\n",
    "        \"Error Recovery Rate\": m3,\n",
    "        \"Correct Loss Rate\": m4\n",
    "    })\n",
    "    all_confusion_matrices.append((setting, cm_transposed))\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Grouped Bar Chart for M1–M4 Scores Across Settings\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores[\"Recovery Gap\"] = df_scores[\"Error Recovery Rate\"] - df_scores[\"Correct Loss Rate\"]\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    \n",
    "    # Transpose so that rows become score types.\n",
    "    df_scores_T = df_scores[[\"Resolution Rate\", \"Positive Conversion Rate\", \"Error Recovery Rate\",\n",
    "                             \"Correct Loss Rate\", \"Recovery Gap\"]].transpose()\n",
    "    \n",
    "    ax = df_scores_T.plot(kind='bar', figsize=(12, 6))\n",
    "    ax.set_ylabel(\"Score (Fraction)\")\n",
    "    ax.set_title(\"Scores by Score Type Across Settings\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.legend(title=\"Setting\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot a Grid of Confusion Matrices for Every Setting\n",
    "# ---------------------------\n",
    "if all_confusion_matrices:\n",
    "    n_settings = len(all_confusion_matrices)\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n_settings / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5))\n",
    "    if n_settings > 1:\n",
    "        axes = np.array(axes).flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, (setting, cm_transposed) in zip(axes, all_confusion_matrices):\n",
    "        im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "        ax.set_title(setting.split('_llava')[0])\n",
    "        ax.set_xticks(np.arange(len(label_names)))\n",
    "        ax.set_yticks(np.arange(len(label_names)))\n",
    "        ax.set_xticklabels(label_names)\n",
    "        ax.set_yticklabels(label_names)\n",
    "        ax.set_xlabel(\"WithExplanation\")\n",
    "        ax.set_ylabel(\"WithExplanationQuality\")\n",
    "        for i in range(len(label_names)):\n",
    "            for j in range(len(label_names)):\n",
    "                ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "                \n",
    "    # Turn off any extra axes.\n",
    "    for ax in axes[len(all_confusion_matrices):]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Updated Bar Plot for Resolution Rate and Error Recovery Rate (Separate Plots)\n",
    "# with Larger Text, Renamed x‑Tick Labels, and Specified Y‑Range for Resolution Rate Plot\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    import matplotlib.ticker as mtick\n",
    "    # Create a DataFrame from the scores and simplify the setting names.\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    df_scores.index.name = None  # Remove the index label from the x-axis.\n",
    "    \n",
    "    # Define a mapping from the abbreviated setting names to more natural names.\n",
    "    rename_mapping = {\n",
    "        'vf_numeric': 'Numeric VF',\n",
    "        'contr_numeric': 'Numeric CONTR',\n",
    "        'showbothmetrics': 'Both Metrics (Numeric)',\n",
    "        'avg_vf_contr_numeric': 'Average VF/CONTR (Numeric)',\n",
    "        'showbothmetrics_descriptive': 'Both Metrics (Descriptive)',\n",
    "        'vf_descriptive': 'Descriptive VF',\n",
    "        'contr_descriptive': 'Descriptive CONTR'\n",
    "    }\n",
    "    \n",
    "    # Extract the two metrics separately.\n",
    "    resolution = df_scores[\"Resolution Rate\"]\n",
    "    error_recovery = df_scores[\"Error Recovery Rate\"]\n",
    "    \n",
    "    # Set up subplots for each metric.\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(12, 7))\n",
    "    \n",
    "    # Plot Resolution Rate.\n",
    "    resolution.plot(kind=\"bar\", ax=axes[0], color=\"skyblue\")\n",
    "    axes[0].set_title(\"Resolution Rate Across Settings\", fontsize=16, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Resolution Rate (%)\", fontsize=16)\n",
    "    axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"\")  # Remove the x-axis label\n",
    "    axes[0].tick_params(axis=\"both\", labelsize=12)\n",
    "    # Replace x tick labels using the mapping.\n",
    "    new_labels = [rename_mapping.get(label.get_text(), label.get_text()) \n",
    "                  for label in axes[0].get_xticklabels()]\n",
    "    axes[0].set_xticklabels(new_labels, rotation=45, ha=\"right\", fontsize=16)\n",
    "    axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    # Set custom y-axis range\n",
    "    axes[0].set_ylim(0.6, 1)\n",
    "    # Annotate each bar (multiplying value by 100 for percentage display).\n",
    "    for rect in axes[0].patches:\n",
    "        height = rect.get_height()\n",
    "        axes[0].text(rect.get_x() + rect.get_width()/2, height, f\"{height*100:.1f}%\",\n",
    "                     ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "    \n",
    "    # Plot Error Recovery Rate.\n",
    "    error_recovery.plot(kind=\"bar\", ax=axes[1], color=\"lightgreen\")\n",
    "    axes[1].set_title(\"Error Recovery Rate Across Settings\", fontsize=16, fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Error Recovery Rate (%)\", fontsize=16)\n",
    "    axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    axes[1].set_xlabel(\"\")  # Remove the x-axis label\n",
    "    axes[1].tick_params(axis=\"both\", labelsize=12)\n",
    "    new_labels = [rename_mapping.get(label.get_text(), label.get_text()) \n",
    "                  for label in axes[1].get_xticklabels()]\n",
    "    axes[1].set_xticklabels(new_labels, rotation=45, ha=\"right\", fontsize=16)\n",
    "    axes[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axes[1].set_ylim(0, 0.35)\n",
    "    for rect in axes[1].patches:\n",
    "        height = rect.get_height()\n",
    "        axes[1].text(rect.get_x() + rect.get_width()/2, height, f\"{height*100:.1f}%\",\n",
    "                     ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_map to a file for later use.\n",
    "import pickle\n",
    "for setting, df in df_map.items():\n",
    "    # only keep the columns we need\n",
    "    df_map[setting] = df[['user_id', 'batch', 'question_i', 'answeronly', 'withexplanation', 'withexplanationquality', 'ground_truth']]\n",
    "with open('df_map.pkl', 'wb') as f:\n",
    "    pickle.dump(df_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined123 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# We do not want to include the following prolific IDs in our analysis.\n",
    "excluded_ids = ['66d4ec6fe6d7a979e98b5ff3']\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(series):\n",
    "    \"\"\"Compute the standard error on a pandas Series.\"\"\"\n",
    "    n = series.count()\n",
    "    return series.std(ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_metrics(df, stage):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of interactions (with a column for the stage and a column for ground_truth),\n",
    "    compute:\n",
    "      - Total accuracy (unsure responses, coded as 2, count as incorrect)\n",
    "      - Accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - Unsure rate\n",
    "      - Utility (0 if unsure; 1 if correct; -1 if incorrect)\n",
    "    Returns also the computed utility as a Series.\n",
    "    \"\"\"\n",
    "    preds = df[stage]\n",
    "    # Total accuracy: count unsure as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == df['ground_truth'])).astype(float)\n",
    "    total_acc_mean = correct_all.mean()\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy on non-unsure responses.\n",
    "    non_unsure = preds != 2\n",
    "    if non_unsure.sum() > 0:\n",
    "        correct_non_unsure = (df.loc[non_unsure, stage] == df.loc[non_unsure, 'ground_truth']).astype(float)\n",
    "        acc_mean = correct_non_unsure.mean()\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "\n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_mean = unsure_indicator.mean()\n",
    "    unsure_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility: 0 if unsure; else 1 if correct, -1 if incorrect.\n",
    "    # (i.e., 1 - 2*abs(pred - ground_truth))\n",
    "    def calc_util(row):\n",
    "        p = row[stage]\n",
    "        gt = row['ground_truth']\n",
    "        return 0 if p == 2 else 1 - 2*abs(p - gt)\n",
    "    utility = df.apply(calc_util, axis=1)\n",
    "    util_mean = utility.mean()\n",
    "    util_se = compute_se(utility)\n",
    "    \n",
    "    return total_acc_mean, total_acc_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, utility\n",
    "\n",
    "# ---------------------------\n",
    "# Settings and Directories\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'vf_as_prod_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_as_prod_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prod_as_vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prod_as_contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'combined12_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_vizwiz_better_sampled_q10_i10_s0',    \n",
    "]\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# To store scores and confusion matrices across settings\n",
    "all_setting_scores = []\n",
    "all_confusion_matrices = []\n",
    "consistent_choice_threshold = 0.9  # 90% threshold\n",
    "df_map = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Process Each Setting via DataFrame\n",
    "# ---------------------------\n",
    "for setting in settings:\n",
    "    print(f\"\\nProcessing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    # Create a list of records (each record is an interaction)\n",
    "    records = []\n",
    "    for file in files:\n",
    "        batch_id = os.path.basename(file).split('.')[0]\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "        # Each file contains sessions for multiple users.\n",
    "        for user_id, session in batch_data.items():\n",
    "            interactions = session['interactions']\n",
    "            for interaction in interactions:\n",
    "                rec = {\n",
    "                    'user_id': user_id,\n",
    "                    'batch': batch_id,\n",
    "                    'question_i': interaction['question_i'],\n",
    "                    'answeronly': interaction['user_selections'].get('answeronly'),\n",
    "                    'withexplanation': interaction['user_selections'].get('withexplanation'),\n",
    "                    'withexplanationquality': interaction['user_selections'].get('withexplanationquality'),\n",
    "                    # Ground truth: defined as 1 - prediction_is_correct.\n",
    "                    'ground_truth': 1 - interaction['question']['prediction_is_correct']\n",
    "                }\n",
    "                # Exclude users with excluded IDs.\n",
    "                if user_id in excluded_ids:\n",
    "                    continue\n",
    "                records.append(rec)\n",
    "    \n",
    "    # Build DataFrame from the records.\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Found {df['user_id'].nunique()} users and {len(df)} interactions for setting '{setting}'.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Filter Users Based on Consistency\n",
    "    # ---------------------------\n",
    "    # For each user, look at all responses across all stages and drop users\n",
    "    # where one response (0, 1, or 2) comprises ≥90% of their responses.\n",
    "    def user_is_valid(group):\n",
    "        responses = pd.concat([group['answeronly'], group['withexplanation'], group['withexplanationquality']])\n",
    "        if responses.empty:\n",
    "            return False\n",
    "        norm_counts = responses.value_counts(normalize=True)\n",
    "        if norm_counts.max() >= consistent_choice_threshold:\n",
    "            # If desired, you could print which user is filtered out.\n",
    "            # print(f\"User {group.name} filtered out; response distribution: {norm_counts.to_dict()}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_valid = df.groupby('user_id').filter(user_is_valid)\n",
    "\n",
    "    if df_valid.empty:\n",
    "        print(\"No interactions remain after filtering. Skipping evaluation for this setting.\")\n",
    "        continue\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Filter Batches with More Than 30 Annotations\n",
    "    # ---------------------------\n",
    "    # For each batch, if it has been annotated more than 30 times (i.e. more than three users,\n",
    "    # assuming each user annotates about 10 interactions), print the annotation count\n",
    "    # and then shorten the batch to include only the first 30 annotations.\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count > 30:\n",
    "            print(f\"Batch {batch} has been annotated {count} times, exceeding 30 annotations. Shortening to first 30 annotations.\")\n",
    "    # Group by 'batch' and take only the first 30 entries for each batch\n",
    "    df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
    "    print(f\"{df_valid['user_id'].nunique()} users remain after filtering; {len(df_valid)} interactions remain.\")\n",
    "    \n",
    "    \n",
    "    # Indicate which batches need to add users\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count % 10: raise ValueError('# Annotation should be divisible by 10, but found not??')\n",
    "        count = count // 10\n",
    "        if count > 3:\n",
    "            raise ValueError(f\"After filtering, # users taking the batch should not > 3??\")\n",
    "        diff = 3 - count\n",
    "        if diff != 0:\n",
    "            print(f\"Batch {batch} need {diff} more users to annotate.\")\n",
    "            \n",
    "    # Add df to the df_map for later use.\n",
    "    df_map[setting] = df_valid\n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Metrics for Each Stage Using DataFrames\n",
    "    # ---------------------------\n",
    "    stage_names = ['Answer Only', 'With Explanation', 'With Explanation + Quality']\n",
    "    stages = ['answeronly', 'withexplanation', 'withexplanationquality']\n",
    "    metrics = {}\n",
    "    utilities = {}\n",
    "    csv_prints = []\n",
    "    print(\"\\nStage                    \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "    for stage, name in zip(stages, stage_names):\n",
    "        tot_acc, tot_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, util_series = compute_metrics(df_valid, stage)\n",
    "        metrics[stage] = {\n",
    "            'total_accuracy_mean': tot_acc,\n",
    "            'total_accuracy_se': tot_se,\n",
    "            'accuracy_mean': acc_mean,\n",
    "            'accuracy_se': acc_se,\n",
    "            'unsure_rate_mean': unsure_mean,\n",
    "            'unsure_rate_se': unsure_se,\n",
    "            'utility_mean': util_mean,\n",
    "            'utility_se': util_se\n",
    "        }\n",
    "        utilities[stage] = util_series\n",
    "        print(f\"{name:<25}\\t{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%\\t\"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%\\t\"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%\\t\"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        csv_prints.append(f\"{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%, \"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%, \"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%, \"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        \n",
    "    print(\", \".join(csv_prints))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utility Gains Between Stages\n",
    "    # ---------------------------\n",
    "    gain_explanation = utilities['withexplanation'] - utilities['answeronly']\n",
    "    gain_quality = utilities['withexplanationquality'] - utilities['withexplanation']\n",
    "    expl_gain_mean = gain_explanation.mean()\n",
    "    expl_gain_se = gain_explanation.std(ddof=1) / np.sqrt(len(gain_explanation)) if len(gain_explanation) > 1 else 0\n",
    "    qual_gain_mean = gain_quality.mean()\n",
    "    qual_gain_se = gain_quality.std(ddof=1) / np.sqrt(len(gain_quality)) if len(gain_quality) > 1 else 0\n",
    "    \n",
    "    print(f\"\\nSetting: {setting}\")\n",
    "    print(\"Stage\\t\\tUnsure Rate\\tUtility over previous stage\")\n",
    "    print(f\"Explanation\\t {metrics['withexplanation']['unsure_rate_mean']*100:5.1f}%\\t\\t {expl_gain_mean:+.3f} ± {expl_gain_se:.3f}\")\n",
    "    print(f\"Quality\\t\\t {metrics['withexplanationquality']['unsure_rate_mean']*100:5.1f}%\\t\\t {qual_gain_mean:+.3f} ± {qual_gain_se:.3f}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Confusion Matrix and M1–M4 Scores\n",
    "    # ---------------------------\n",
    "    # Compute utility for withexplanation and withexplanationquality directly.\n",
    "    def compute_utility(val, gt):\n",
    "        return 0 if val == 2 else 1 - 2*abs(val-gt)\n",
    "    \n",
    "    df_valid['util_withexplanation'] = df_valid.apply(lambda row: compute_utility(row['withexplanation'], row['ground_truth']), axis=1)\n",
    "    df_valid['util_withexplanationquality'] = df_valid.apply(lambda row: compute_utility(row['withexplanationquality'], row['ground_truth']), axis=1)\n",
    "    \n",
    "    # For the confusion matrix, map utilities to indices: -1 -> 0, 0 -> 1, 1 -> 2.\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    label_order = [-1, 0, 1]\n",
    "    label_names = ['Incorrect', 'Unsure', 'Correct']\n",
    "    \n",
    "    try:\n",
    "        cm = np.zeros((3, 3), dtype=int)\n",
    "        for _, row in df_valid.iterrows():\n",
    "            u2 = row['util_withexplanation']\n",
    "            u3 = row['util_withexplanationquality']\n",
    "            cm[mapping[u2], mapping[u3]] += 1\n",
    "        cm_transposed = cm.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing confusion matrix or M1–M4 scores: {e}\")\n",
    "        print(f\"This would be expected if setting {setting} is combined12 or combined123.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nSetting: {setting} Step 2 -> Step 3 (transposed)\")\n",
    "    print(\"\\t\\t\" + \"\\t\".join(label_names))\n",
    "    for i, label in enumerate(label_names):\n",
    "        row_values = \"\\t\".join(str(cm_transposed[i, j]) for j in range(3))\n",
    "        print(f\"{label:<10}\\t {row_values}\")\n",
    "    \n",
    "    # Compute M1–M4 using counts from the confusion matrix.\n",
    "    def count_transitions(val2, val3):\n",
    "        return ((df_valid['util_withexplanation'] == val2) & (df_valid['util_withexplanationquality'] == val3)).sum()\n",
    "    \n",
    "    denom_M1 = count_transitions(0, -1) + count_transitions(0, 0) + count_transitions(0, 1)\n",
    "    num_M1 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m1 = num_M1 / denom_M1 if denom_M1 > 0 else np.nan\n",
    "\n",
    "    denom_M2 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m2 = count_transitions(0, 1) / denom_M2 if denom_M2 > 0 else np.nan\n",
    "\n",
    "    denom_M3 = count_transitions(-1, -1) + count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    num_M3 = count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    m3 = num_M3 / denom_M3 if denom_M3 > 0 else np.nan\n",
    "\n",
    "    denom_M4 = count_transitions(1, -1) + count_transitions(1, 0) + count_transitions(1, 1)\n",
    "    num_M4 = count_transitions(1, -1) + count_transitions(1, 0)\n",
    "    m4 = num_M4 / denom_M4 if denom_M4 > 0 else np.nan\n",
    "\n",
    "    print(\"\\nM1–M4 Scores:\")\n",
    "    print(f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\")\n",
    "    print(f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\")\n",
    "    print(f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\")\n",
    "    print(f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    print(f\"{m1:.2%}, {m2:.2%}, {m3:.2%}, {m4:.2%}, {m3-m4:.2%}\")\n",
    "\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Print Examples Where Step 2 is Correct and Step 3 is Incorrect\n",
    "    # ---------------------------\n",
    "    print(\"\\nExamples where Step 2 is correct and Step 3 is incorrect:\")\n",
    "    examples = df_valid[\n",
    "        (df_valid['withexplanation'] != 2) &\n",
    "        (df_valid['withexplanation'] == df_valid['ground_truth']) &\n",
    "        (df_valid['withexplanationquality'] != 2) &\n",
    "        (df_valid['withexplanationquality'] != df_valid['ground_truth'])\n",
    "    ]\n",
    "    if not examples.empty:\n",
    "        for idx, row in examples.head(5).iterrows():\n",
    "            print(row.to_json())\n",
    "    else:\n",
    "        print(\"No examples found.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot: Confusion Matrix and M1–M4 Scores for the Current Setting\n",
    "    # ---------------------------\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "    ax.set_title(f\"Setting {setting.split('_llava')[0]} Step 2->Step 3\\n(WithExplanation -> WithExplanationQuality)\")\n",
    "    ax.set_xticks(np.arange(len(label_names)))\n",
    "    ax.set_yticks(np.arange(len(label_names)))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_xlabel(\"WithExplanation\")\n",
    "    ax.set_ylabel(\"WithExplanationQuality\")\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # Plot M1–M4 scores as text.\n",
    "    ax = axes[1]\n",
    "    ax.axis('off')\n",
    "    score_text = (f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\\n\"\n",
    "                  f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\\n\"\n",
    "                  f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\\n\"\n",
    "                  f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    ax.text(0.5, 0.5, score_text, fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title(\"M1–M4 Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save scores and confusion matrix for later comparison/plotting.\n",
    "    all_setting_scores.append({\n",
    "        \"Setting\": setting,\n",
    "        \"Resolution Rate\": m1,\n",
    "        \"Positive Conversion Rate\": m2,\n",
    "        \"Error Recovery Rate\": m3,\n",
    "        \"Correct Loss Rate\": m4\n",
    "    })\n",
    "    all_confusion_matrices.append((setting, cm_transposed))\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Grouped Bar Chart for M1–M4 Scores Across Settings\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores[\"Recovery Gap\"] = df_scores[\"Error Recovery Rate\"] - df_scores[\"Correct Loss Rate\"]\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    \n",
    "    # Transpose so that rows become score types.\n",
    "    df_scores_T = df_scores[[\"Resolution Rate\", \"Positive Conversion Rate\", \"Error Recovery Rate\",\n",
    "                             \"Correct Loss Rate\", \"Recovery Gap\"]].transpose()\n",
    "    \n",
    "    ax = df_scores_T.plot(kind='bar', figsize=(12, 6))\n",
    "    ax.set_ylabel(\"Score (Fraction)\")\n",
    "    ax.set_title(\"Scores by Score Type Across Settings\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.legend(title=\"Setting\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot a Grid of Confusion Matrices for Every Setting\n",
    "# ---------------------------\n",
    "if all_confusion_matrices:\n",
    "    n_settings = len(all_confusion_matrices)\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n_settings / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5))\n",
    "    if n_settings > 1:\n",
    "        axes = np.array(axes).flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, (setting, cm_transposed) in zip(axes, all_confusion_matrices):\n",
    "        im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "        ax.set_title(setting.split('_llava')[0])\n",
    "        ax.set_xticks(np.arange(len(label_names)))\n",
    "        ax.set_yticks(np.arange(len(label_names)))\n",
    "        ax.set_xticklabels(label_names)\n",
    "        ax.set_yticklabels(label_names)\n",
    "        ax.set_xlabel(\"WithExplanation\")\n",
    "        ax.set_ylabel(\"WithExplanationQuality\")\n",
    "        for i in range(len(label_names)):\n",
    "            for j in range(len(label_names)):\n",
    "                ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "                \n",
    "    # Turn off any extra axes.\n",
    "    for ax in axes[len(all_confusion_matrices):]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_map to a file for later use.\n",
    "import pickle\n",
    "for setting, df in df_map.items():\n",
    "    # only keep the columns we need\n",
    "    df_map[setting] = df[['user_id', 'batch', 'question_i', 'withexplanationquality', 'ground_truth']]\n",
    "with open('df_combined123_map.pkl', 'wb') as f:\n",
    "    pickle.dump(df_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here for the analysis of user interaction data vs. confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load df_map from the file.\n",
    "with open('df_map.pkl', 'rb') as f:\n",
    "    df_map = pickle.load(f)\n",
    "print(\"Loaded df_map from file.\")\n",
    "# print the keys of df_map\n",
    "print(\"Keys in df_map:\")\n",
    "for key in df_map.keys():\n",
    "    print(key)\n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load df_combined123_map from the file.\n",
    "with open('df_combined123_map.pkl', 'rb') as f:\n",
    "    df_combined123_map = pickle.load(f)\n",
    "# print(\"Loaded df_combined123_map from file.\")\n",
    "# # print the keys of df_combined123_map\n",
    "# print(\"Keys in df_combined123_map:\")\n",
    "# for key in df_combined123_map.keys():\n",
    "#     print(key)\n",
    "    \n",
    "from collections import OrderedDict\n",
    "\n",
    "# 1) define the exact key‐order\n",
    "new_key_order = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'vf_as_prod_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_as_prod_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prod_as_vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prod_as_contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    \n",
    "    'combined12_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_vizwiz_better_sampled_q10_i10_s0',   \n",
    "]\n",
    "\n",
    "# 2) rebuild as OrderedDict\n",
    "df_combined123_map = OrderedDict(\n",
    "    (k, df_combined123_map[k]) for k in new_key_order\n",
    ")\n",
    "\n",
    "# 3) check the order\n",
    "print(\"Keys in df_combined123_map after reordering:\")\n",
    "for key in df_combined123_map.keys():\n",
    "    print(key)\n",
    "# df_combined123_map['vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Some helper functions\n",
    "def calc_util(row):\n",
    "    p  = row['withexplanationquality']    # 0=trust, 1=distrust, 2=unsure\n",
    "    gt = row['ground_truth']              # 0=correct, 1=incorrect\n",
    "    if p == 2:\n",
    "        return 0\n",
    "    return 1 - 2 * abs(p - gt)\n",
    "\n",
    "def compute_se(series):\n",
    "    \"\"\"Compute the standard error on a pandas Series of numbers (or 0/1s).\"\"\"\n",
    "    n = series.count()\n",
    "    return series.std(ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "rows = []\n",
    "for key, df in df_combined123_map.items():\n",
    "    n                = len(df)\n",
    "    annotation_count = n\n",
    "    unique_users     = df['user_id'].nunique()\n",
    "    \n",
    "    p  = df['withexplanationquality']\n",
    "    gt = df['ground_truth']\n",
    "    \n",
    "    # 1) Build 0/1 indicator series for each metric\n",
    "    unsure_ind   = (p == 2).astype(int)\n",
    "    accept_ind   = (p == 0).astype(int)\n",
    "    not_unsure   = p != 2\n",
    "    accept_not_unsure_ind = accept_ind[not_unsure]\n",
    "    correct_ind  = (((p == 0)&(gt == 0)) | ((p == 1)&(gt == 1))).astype(int)\n",
    "    correct_not_unsure_ind = correct_ind[not_unsure]\n",
    "    false_acp_ind = ((p == 0)&(gt == 1)).astype(int)\n",
    "    false_rej_ind = ((p == 1)&(gt == 0)).astype(int)\n",
    "    true_acp_ind  = ((p == 0)&(gt == 0)).astype(int)\n",
    "    true_rej_ind  = ((p == 1)&(gt == 1)).astype(int)\n",
    "    \n",
    "    # 2) Compute means (rates) and counts\n",
    "    unsure_rate    = unsure_ind.mean()\n",
    "    unsure_rate_gt_1 = unsure_ind[gt == 0].mean() if (gt == 0).sum()>0 else np.nan\n",
    "    unsure_rate_gt_0 = unsure_ind[gt == 1].mean() if (gt == 1).sum()>0 else np.nan\n",
    "    accept_rate    = accept_ind.mean()\n",
    "    not_unsure_accept_rate = accept_not_unsure_ind.mean() if not_unsure.sum()>0 else np.nan\n",
    "    total_acc      = correct_ind.mean()\n",
    "    not_unsure_acc = correct_not_unsure_ind.mean() if not_unsure.sum()>0 else np.nan\n",
    "    false_acp_rate = false_acp_ind.sum() / (gt == 1).sum() if (gt == 1).sum()>0 else np.nan\n",
    "    false_rej_rate = false_rej_ind.sum() / (gt == 0).sum() if (gt == 0).sum()>0 else np.nan\n",
    "    true_acp_rate  = true_acp_ind.sum()  / (gt == 0).sum() if (gt == 0).sum()>0 else np.nan\n",
    "    true_rej_rate  = true_rej_ind.sum()  / (gt == 1).sum() if (gt == 1).sum()>0 else np.nan\n",
    "    \n",
    "    # 3) Compute utility and its SE\n",
    "    df['utility']  = df.apply(calc_util, axis=1)\n",
    "    mean_util      = df['utility'].mean()\n",
    "    \n",
    "    # 4) Compute SEs\n",
    "    unsure_se        = compute_se(unsure_ind)\n",
    "    \n",
    "    accept_se        = compute_se(accept_ind)\n",
    "    not_unsure_accept_se = compute_se(accept_not_unsure_ind)\n",
    "    total_acc_se     = compute_se(correct_ind)\n",
    "    not_unsure_acc_se= compute_se(correct_not_unsure_ind)\n",
    "    true_acp_se      = compute_se(true_acp_ind)\n",
    "    true_rej_se      = compute_se(true_rej_ind)\n",
    "    false_acp_se     = compute_se(false_acp_ind)\n",
    "    false_rej_se     = compute_se(false_rej_ind)\n",
    "    util_se          = compute_se(df['utility'])\n",
    "    \n",
    "    rows.append({\n",
    "        'model_variant':             key,\n",
    "        'Annotations':               annotation_count,\n",
    "        'Unique Users':              unique_users,\n",
    "        'Unsure Rate':               unsure_rate,\n",
    "        'Unsure SE':                 unsure_se,\n",
    "        # 'Unsure Rate GT correct':        unsure_rate_gt_1,\n",
    "        # 'Unsure Rate GT incorrect':        unsure_rate_gt_0,\n",
    "        'Accept Rate':               accept_rate,\n",
    "        'Accept SE':                 accept_se,\n",
    "        'Not Unsure Accept Rate':    not_unsure_accept_rate,\n",
    "        'NotUnsureAccept SE':        not_unsure_accept_se,\n",
    "        'TotalAcc':                  total_acc,\n",
    "        'TotalAcc SE':               total_acc_se,\n",
    "        'NotUnsureAcc':              not_unsure_acc,\n",
    "        'NotUnsureAcc SE':           not_unsure_acc_se,\n",
    "        'False Acp Rate':            false_acp_rate,\n",
    "        'FalseAcp SE':               false_acp_se,\n",
    "        'False Rej Rate':            false_rej_rate,\n",
    "        'FalseRej SE':               false_rej_se,\n",
    "        'True Acp Rate':             true_acp_rate,\n",
    "        'TrueAcp SE':                true_acp_se,\n",
    "        'True Rej Rate':             true_rej_rate,\n",
    "        'TrueRej SE':                true_rej_se,\n",
    "        'Mean Utility':              mean_util,\n",
    "        'Utility SE':                util_se\n",
    "    })\n",
    "\n",
    "# 5) Summarize\n",
    "summary_df = pd.DataFrame(rows).set_index('model_variant')\n",
    "display(summary_df)\n",
    "\n",
    "# 6) Print each row with “mean ± SE”\n",
    "for idx, row in summary_df.iterrows():\n",
    "    print(f\"{int(row['Annotations'])}, {int(row['Unique Users'])}, \", end=\"\")\n",
    "    metrics = [\n",
    "        ('Unsure Rate','Unsure SE'),\n",
    "        ('Accept Rate','Accept SE'),\n",
    "        ('Not Unsure Accept Rate','NotUnsureAccept SE'),\n",
    "        ('TotalAcc','TotalAcc SE'),\n",
    "        ('NotUnsureAcc','NotUnsureAcc SE'),\n",
    "        ('False Acp Rate','FalseAcp SE'),\n",
    "        ('False Rej Rate','FalseRej SE'),\n",
    "        ('Mean Utility','Utility SE'),\n",
    "        ('True Acp Rate','TrueAcp SE'),\n",
    "        ('True Rej Rate','TrueRej SE'),\n",
    "        ('False Acp Rate','FalseAcp SE'),\n",
    "        ('False Rej Rate','FalseRej SE'),\n",
    "        # ('Unsure Rate GT correct','Unsure Rate GT correct'),\n",
    "        # ('Unsure Rate GT incorrect','Unsure Rate GT incorrect'),\n",
    "    ]\n",
    "    str_to_print = []\n",
    "    for mean_col, se_col in metrics:\n",
    "        if mean_col == 'Mean Utility':\n",
    "            mean = row[mean_col]\n",
    "            se   = row[se_col]\n",
    "            str_to_print.append(f\"{mean:.3f} ± {se:.3f}\")\n",
    "        else:\n",
    "            mean = row[mean_col]\n",
    "            se   = row[se_col]\n",
    "            str_to_print.append(f\"{mean:.1%} ± {se:.1%}\")\n",
    "    print(\", \".join(str_to_print), end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# grab the default deep palette\n",
    "palette = sns.color_palette('deep')\n",
    "# convert to hex\n",
    "from matplotlib.colors import to_hex\n",
    "hex_codes = [to_hex(c) for c in palette]\n",
    "print(hex_codes)\n",
    "# ['#4c72b0', '#dd8452', '#55a868', '#c44e52', '#8172b3', '#ccb974']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "all_setting_variants = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'combined12_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'contr_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "]\n",
    "\n",
    "all_setting_names = [\n",
    "    'Control',\n",
    "    'Control (VizWiz)',\n",
    "    'VF (Numeric)',\n",
    "    'VF (Numeric) (VizWiz)',\n",
    "    'VF (Descriptive)',\n",
    "    'VF (Descriptive) (VizWiz)',\n",
    "    'CONTR (Numeric)',\n",
    "    'CONTR (Descriptive)',\n",
    "    'Show Both Metrics (Numeric)',\n",
    "    'Show Both Metrics (Descriptive)',\n",
    "    'Prod(VF, CONTR)',\n",
    "]\n",
    "\n",
    "num_setting_variants = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'combined12_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'contr_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "]\n",
    "\n",
    "num_setting_names = [\n",
    "    'Control',\n",
    "    'Control (VizWiz)',\n",
    "    'VF (Numeric)',\n",
    "    'VF (Numeric) (VizWiz)',\n",
    "    'CONTR (Numeric)',\n",
    "    'Show Both Metrics (Numeric)',\n",
    "    'Prod(VF, CONTR)',\n",
    "]\n",
    "\n",
    "desc_setting_variants = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'combined12_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_vizwiz_better_sampled_q10_i10_s0',\n",
    "    'contr_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0'\n",
    "]\n",
    "\n",
    "desc_setting_names = [\n",
    "    'Control',\n",
    "    'Control (VizWiz)',\n",
    "    'VF (Descriptive)',\n",
    "    'VF (Descriptive) (VizWiz)',\n",
    "    'CONTR (Descriptive)',\n",
    "    'Show Both Metrics (Descriptive)'\n",
    "]\n",
    "\n",
    "\n",
    "# Define colors for groups\n",
    "# group_colors = {\n",
    "#     'vf': 'tab:blue',\n",
    "#     'contr': 'tab:orange',\n",
    "#     'showbothmetrics': 'tab:green',\n",
    "#     'prodmetric': 'tab:red',\n",
    "#     'other': 'gray'\n",
    "# }\n",
    "# Use seaborn palette\n",
    "group_colors = {\n",
    "    'vf': sns.color_palette(\"tab10\")[0], # '#4C72B0', # blue\n",
    "    'contr': sns.color_palette(\"tab10\")[3], #'#DD8452', # orange\n",
    "    'showbothmetrics': sns.color_palette(\"tab10\")[4], #'#55A868', # green\n",
    "    'prodmetric': sns.color_palette(\"tab10\")[2], #'#C44E52', # red\n",
    "    'other': 'lightgray'\n",
    "}\n",
    "\n",
    "# Assign colors and alphas (darker for numeric, lighter for descriptive)\n",
    "all_bar_colors = []\n",
    "all_alphas = []\n",
    "num_bar_colors = []\n",
    "num_alphas = []\n",
    "desc_bar_colors = []\n",
    "desc_alphas = []\n",
    "for name in all_setting_variants:\n",
    "    if name.startswith('vf_'):\n",
    "        group = 'vf'\n",
    "        alpha = 0.5 if 'descriptive' in name else 1.0\n",
    "    elif name.startswith('contr_'):\n",
    "        group = 'contr'\n",
    "        alpha = 0.5 if 'descriptive' in name else 1.0\n",
    "    elif name.startswith('showbothmetrics'):\n",
    "        group = 'showbothmetrics'\n",
    "        alpha = 0.5 if 'descriptive' in name else 1.0\n",
    "    elif name.startswith('prodmetric'):\n",
    "        group = 'prodmetric'\n",
    "        alpha = 1.0\n",
    "    else:\n",
    "        group = 'other'\n",
    "        alpha = 1.0\n",
    "    all_bar_colors.append(group_colors[group])\n",
    "    all_alphas.append(alpha)\n",
    "    if 'combined12' in name:\n",
    "        num_bar_colors.append(group_colors[group])\n",
    "        num_alphas.append(alpha)\n",
    "        desc_bar_colors.append(group_colors[group])\n",
    "        desc_alphas.append(alpha)\n",
    "    if 'prodmetric' in name:\n",
    "        num_bar_colors.append(group_colors[group])\n",
    "        num_alphas.append(alpha)\n",
    "    if 'numeric' in name or name == 'showbothmetrics_llava1.5_aokvqa_better_sampled_q10_i10_s0':\n",
    "        num_bar_colors.append(group_colors[group])\n",
    "        num_alphas.append(alpha)\n",
    "    if 'descriptive' in name:\n",
    "        desc_bar_colors.append(group_colors[group])\n",
    "        desc_alphas.append(alpha)\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "def plot_chart(\n",
    "    group_setting,\n",
    "    metric_name,\n",
    "    metric_error_bar_name=None,\n",
    "    y_lim_max=None,\n",
    "    n_per_group: int = 300,           # <-- how many annotations per arm\n",
    "):\n",
    "    # pick variants/names/colors exactly as before...\n",
    "    if group_setting == 'numeric':\n",
    "        setting_variants = num_setting_variants\n",
    "        setting_names    = num_setting_names\n",
    "        bar_colors       = num_bar_colors\n",
    "        alphas           = num_alphas\n",
    "    elif group_setting == 'descriptive':\n",
    "        setting_variants = desc_setting_variants\n",
    "        setting_names    = desc_setting_names\n",
    "        bar_colors       = desc_bar_colors\n",
    "        alphas           = desc_alphas\n",
    "    elif group_setting == 'all':\n",
    "        setting_variants = all_setting_variants\n",
    "        setting_names    = all_setting_names\n",
    "        bar_colors       = all_bar_colors\n",
    "        alphas           = all_alphas\n",
    "    else:\n",
    "        raise ValueError(\"Invalid group_setting. Choose 'numeric','descriptive',or 'all'.\")\n",
    "\n",
    "    # grab rates & errors\n",
    "    rates  = summary_df.loc[setting_variants, metric_name].values\n",
    "    errors = (summary_df.loc[setting_variants, metric_error_bar_name].values\n",
    "              if metric_error_bar_name else [None]*len(rates))\n",
    "\n",
    "    # --- plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x = np.arange(len(rates))\n",
    "    for xi, rate, err, color, alpha in zip(x, rates, errors, bar_colors, alphas):\n",
    "        ax.bar(xi, rate,\n",
    "               yerr=err,\n",
    "               color=color,\n",
    "               alpha=alpha,\n",
    "               error_kw={'capsize':5,'elinewidth':1} if err is not None else {})\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(setting_names, rotation=30, ha='right')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    if y_lim_max is not None:\n",
    "        ax.set_ylim(0, y_lim_max)\n",
    "    ax.set_title(f'{metric_name} Across Settings ({group_setting})')\n",
    "    for container in ax.containers:\n",
    "        if hasattr(container, \"patches\"):\n",
    "            ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- one‐sided z‐tests vs Control (variant 0) ---\n",
    "    ctrl_var   = setting_variants[0]\n",
    "    ctrl_rate  = summary_df.loc[ctrl_var, metric_name]\n",
    "    ctrl_count = int(round(ctrl_rate * n_per_group))\n",
    "\n",
    "    print(f\"\\nOne‐sided two‐prop z‐tests vs {setting_names[0]}:\\n\")\n",
    "    for var, label, rate in zip(setting_variants[1:], setting_names[1:], rates[1:]):\n",
    "        count = int(round(rate * n_per_group))\n",
    "        counts = np.array([count, ctrl_count])\n",
    "        nobs   = np.array([n_per_group, n_per_group])\n",
    "\n",
    "        # decide direction: “larger” if higher is better (e.g. NotUnsureAcc), else “smaller”\n",
    "        if 'NotUnsureAcc'.lower() in metric_name.lower() or 'Accept' in metric_name:\n",
    "            alt = 'larger'\n",
    "        else:\n",
    "            alt = 'smaller'\n",
    "\n",
    "        z_stat, p_val = proportions_ztest(counts, nobs, alternative=alt)\n",
    "        diff = rate - ctrl_rate\n",
    "\n",
    "        print(f\"{label:30} Δ={diff: .3f}   z={z_stat: .2f}   p_one‐sided={p_val:.3f}  ({alt})\")\n",
    "\n",
    "\n",
    "plot_chart('all', 'Unsure Rate', 'Unsure SE', y_lim_max = 0.13)\n",
    "plot_chart('numeric', 'Not Unsure Accept Rate', 'NotUnsureAccept SE', y_lim_max=1.0)\n",
    "plot_chart('numeric', 'NotUnsureAcc', 'NotUnsureAcc SE', y_lim_max=1.0)\n",
    "plot_chart('numeric', 'False Acp Rate', 'FalseAcp SE', y_lim_max=0.7)\n",
    "plot_chart('numeric', 'False Rej Rate', 'FalseRej SE', y_lim_max=0.2)\n",
    "\n",
    "plot_chart('all', 'False Acp Rate', 'FalseAcp SE', y_lim_max=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) configure which variants + human names you want\n",
    "variants      = num_setting_variants\n",
    "names         = num_setting_names\n",
    "\n",
    "# 2) the metric columns and their SE columns, in the order you want them on the x‑axis\n",
    "metrics       = ['NotUnsureAcc', 'False Acp Rate', 'False Rej Rate']\n",
    "se_cols       = ['NotUnsureAcc SE', 'FalseAcp SE', 'FalseRej SE']\n",
    "pretty_labels = [r'Accuracy $\\uparrow$', r'Over Reliance $\\downarrow$', r'Under Reliance $\\downarrow$']\n",
    "\n",
    "# 3) pull out a tiny DataFrame of shape (3 metrics × N conditions)\n",
    "data   = summary_df.loc[variants, metrics].T        # rows=metrics, cols=variants\n",
    "errors = summary_df.loc[variants, se_cols].T        # same shape\n",
    "\n",
    "# 4) build the clustered bar chart “by hand” so we can control everything\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "x = np.arange(len(metrics))                         # 0,1,2\n",
    "total_width = 0.8\n",
    "n = len(variants)\n",
    "bar_width = total_width / n\n",
    "\n",
    "for i, (var, label) in enumerate(zip(variants, names)):\n",
    "    vals = data[var].values\n",
    "    errs = errors[var].values\n",
    "    # center each group at x, then shift for each variant\n",
    "    xpos = x - total_width/2 + bar_width/2 + i*bar_width\n",
    "    # Replace \"Metric\" in the label with \"Score\"\n",
    "    label = label.replace(\"Metric\", \"Score\")\n",
    "\n",
    "    # draw the bar\n",
    "    ax.bar(xpos, vals, bar_width,\n",
    "           color=group_colors.get(var.split('_')[0], 'gray'),\n",
    "        #    alpha=1.0 if ('numeric' in var or var.startswith('prodmetric')) else 0.5,\n",
    "           label=label)\n",
    "\n",
    "    # draw the error‐bars\n",
    "    ax.errorbar(xpos, vals, yerr=errs,\n",
    "                fmt='none',         # no additional marker\n",
    "                capsize=5,\n",
    "                elinewidth=1,\n",
    "                ecolor='black')\n",
    "\n",
    "    # add floating labels\n",
    "    for xi, yi in zip(xpos, vals):\n",
    "        ax.text(xi, yi + 0.03, f\"{yi:.3f}\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# finish styling\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pretty_labels, rotation=0)\n",
    "ax.set_ylabel(\"Rate\")\n",
    "# ax.set_title(\"Accuracy / Over Reliance / Under Reliance by Condition\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "# --- 0) Your data setup (as before) ---\n",
    "metrics       = ['NotUnsureAcc', 'False Acp Rate', 'False Rej Rate']\n",
    "se_cols       = ['NotUnsureAcc SE', 'FalseAcp SE', 'FalseRej SE']\n",
    "metric_names  = [r'Accuracy$\\uparrow$', r'Over Reliance$\\downarrow$', r'Under Reliance$\\downarrow$']\n",
    "\n",
    "# split numeric variants into the two groups\n",
    "aokvqa_vars = [v for v in num_setting_variants if 'vizwiz' not in v]\n",
    "vizwiz_vars = [v for v in num_setting_variants if 'vizwiz' in v]\n",
    "n1, n2 = len(aokvqa_vars), len(vizwiz_vars)\n",
    "\n",
    "# pull human‑readable per‑variant names & colors\n",
    "aokvqa_names = [num_setting_names[num_setting_variants.index(v)] for v in aokvqa_vars]\n",
    "vizwiz_names = [num_setting_names[num_setting_variants.index(v)] for v in vizwiz_vars]\n",
    "aokvqa_cols  = [group_colors.get(v.split('_')[0],'gray') for v in aokvqa_vars]\n",
    "vizwiz_cols  = [group_colors.get(v.split('_')[0],'gray') for v in vizwiz_vars]\n",
    "\n",
    "# --- 1) Prepare the group legend patches ---\n",
    "# Define a mapping from your group keys to nice labels:\n",
    "group_labels = {\n",
    "    'other': 'Control',\n",
    "    'vf': 'VF (Numeric)',\n",
    "    'contr': 'CONTR (Numeric)',\n",
    "    'showbothmetrics': 'Show Both Scores (Numeric)',\n",
    "    'prodmetric': 'Prod(VF, CONTR)',\n",
    "}\n",
    "desired_order = [\n",
    "    'other',         # Control\n",
    "    'vf',            # VF (Numeric)\n",
    "    'contr',         # CONTR (Numeric)\n",
    "    'showbothmetrics',# Show Both Scores (Numeric)\n",
    "    'prodmetric',    # Prod(VF, CONTR)\n",
    "]\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=group_colors[key], label=group_labels[key])\n",
    "    for key in desired_order\n",
    "]\n",
    "\n",
    "# --- 2) Build the 3 subplots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "# total width is exactly one bar per condition\n",
    "bar_width = 1.0\n",
    "gap       = 0.5  # gap between the two sets\n",
    "\n",
    "for ax, metric, se_col, met_label in zip(axes, metrics, se_cols, metric_names):\n",
    "    # get data\n",
    "    y1 = summary_df.loc[aokvqa_vars, metric].values\n",
    "    e1 = summary_df.loc[aokvqa_vars, se_col].values\n",
    "    y2 = summary_df.loc[vizwiz_vars, metric].values\n",
    "    e2 = summary_df.loc[vizwiz_vars, se_col].values\n",
    "\n",
    "    # compute x positions\n",
    "    x1 = np.arange(n1)\n",
    "    x2 = np.arange(n2) + n1 + gap\n",
    "\n",
    "    # plot AOKVQA bars\n",
    "    ax.bar(x1, y1, bar_width, yerr=e1,\n",
    "           color=aokvqa_cols,\n",
    "           error_kw={'capsize':5,'elinewidth':1})\n",
    "    # plot VizWiz bars\n",
    "    ax.bar(x2, y2, bar_width, yerr=e2,\n",
    "           color=vizwiz_cols,\n",
    "           error_kw={'capsize':5,'elinewidth':1})\n",
    "\n",
    "    # attach value labels\n",
    "    for xi, yi in zip(np.concatenate([x1, x2]), np.concatenate([y1, y2])):\n",
    "        ax.text(xi, yi + 0.03, f\"{yi:.3f}\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # --- set up the two‐tier x‐axis labels ---\n",
    "    # 1) Do not show any x-ticks for the major axis\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    # 2) minor ticks = one per group, placed at midpoint\n",
    "    grp_locs = [\n",
    "        x1.mean(),              # center of AOKVQA block\n",
    "        x2.mean()               # center of VizWiz block\n",
    "    ]\n",
    "    ax.xaxis.set_minor_locator(FixedLocator(grp_locs))\n",
    "    ax.xaxis.set_minor_formatter(FixedFormatter(['AOKVQA', 'VizWiz']))\n",
    "    ax.tick_params(axis='x', which='minor', length=0, pad=6)  # bump those labels down\n",
    "\n",
    "    # 3) xlabel = the metric name (e.g. \"Accuracy\")\n",
    "    # ax.set_xlabel(met_label, labelpad=10, fontsize=12)\n",
    "\n",
    "    # y‐axis and title\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title(met_label)\n",
    "\n",
    "# --- 3) Put a single legend across the top and tidy up ---\n",
    "fig.legend(handles=legend_patches,\n",
    "           loc='upper center',\n",
    "           ncol=len(legend_patches),\n",
    "           frameon=False,\n",
    "           bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "def base_cond(label):\n",
    "    return label.replace(\" (VizWiz)\", \"\")\n",
    "\n",
    "metrics      = ['NotUnsureAcc', 'False Acp Rate', 'False Rej Rate']\n",
    "se_cols      = ['NotUnsureAcc SE', 'FalseAcp SE', 'FalseRej SE']\n",
    "metric_names = [r'User Accuracy$\\uparrow$', r'Over-Reliance$\\downarrow$', r'Under-Reliance$\\downarrow$']\n",
    "\n",
    "records = []\n",
    "for var, human in zip(num_setting_variants, num_setting_names):\n",
    "    grp = 'VizWiz' if 'vizwiz' in var else 'AOKVQA'\n",
    "    cond = base_cond(human)                \n",
    "    for m, se, lab in zip(metrics, se_cols, metric_names):\n",
    "        records.append({\n",
    "            'Condition': cond,             \n",
    "            'Group':     grp,\n",
    "            'Metric':    lab,\n",
    "            'Rate':      summary_df.loc[var, m],\n",
    "            'SE':        summary_df.loc[var, se]\n",
    "        })\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "variant_palette = {}\n",
    "for var, human in zip(num_setting_variants, num_setting_names):\n",
    "    cond = base_cond(human)               \n",
    "    if cond not in variant_palette:        # keep first encounter\n",
    "        key = var.split('_')[0]\n",
    "        variant_palette[cond] = group_colors.get(key, 'silver')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g = sns.FacetGrid(df, col=\"Metric\", sharey=True, height=4, aspect=1.2)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.barplot,\n",
    "    x=\"Group\", y=\"Rate\", hue=\"Condition\",\n",
    "    palette=variant_palette,\n",
    "    errorbar=None                    # or ci=None if you’re on seaborn<0.12\n",
    ")\n",
    "\n",
    "# manually overlay the SE errorbars & labels in each facet\n",
    "for ax in g.axes.flat:\n",
    "    # extract the metric for this facet\n",
    "    metric = ax.get_title().split(' = ')[-1]\n",
    "    sub = df[df.Metric == metric]\n",
    "\n",
    "    for patch, (_, row) in zip(ax.patches, sub.iterrows()):\n",
    "        x = patch.get_x() + patch.get_width()/2\n",
    "        y = patch.get_height()\n",
    "        ax.errorbar(x, y, yerr=row.SE, capsize=5, ecolor='black')\n",
    "        ax.text(x, y + 0.04, f\"{y:.3f}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # clean up each subplot\n",
    "    ax.set_xlabel(\"\")            # we’ll rely on the minor ticks for “AOKVQA/VizWiz”\n",
    "    ax.set_ylabel(\"Rate\", fontsize=16)\n",
    "    ax.set_ylim(0, 0.8)\n",
    "    ax.set_xticks([0,0.75])         # remove the major ticks\n",
    "    ax.set_xticklabels(['A-OKVQA', 'VizWiz'], fontsize=16)\n",
    "    ax.set_title(metric, fontsize=18)\n",
    "\n",
    "# remove the individual axes’ legends\n",
    "g.despine(left=True)\n",
    "for ax in g.axes.flat:\n",
    "    leg = ax.get_legend()\n",
    "    if leg is not None:\n",
    "        leg.remove()\n",
    "\n",
    "\n",
    "# ...and add one legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "rename_and_filter_labels = {\n",
    "    'Control': 'Show Explanation Only',\n",
    "    'VF (Numeric)': \"Visual Fidelity\",\n",
    "    'CONTR (Numeric)': \"Contrastiveness\",\n",
    "    'Show Both Metrics (Numeric)': 'Both VF and Contr.',\n",
    "    'Prod(VF, CONTR)': r'VF$ \\times $Contr.',\n",
    "}\n",
    "# Filter handles and labels\n",
    "handles = [h for h, l in zip(handles, labels) if l in rename_and_filter_labels]\n",
    "labels = [rename_and_filter_labels[l] for l in labels if l in rename_and_filter_labels]\n",
    "g.figure.legend(handles, labels,\n",
    "             loc='upper center',\n",
    "             ncol=len(labels),\n",
    "             frameon=False,\n",
    "             bbox_to_anchor=(0.5, 0),\n",
    "             fontsize=16,)\n",
    "g.figure.dpi = 300\n",
    "# plt.tight_layout(rect=[0,0,1,0.96])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ── 1. create one row of three axes ────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "# map the three readable metric labels back to the list order\n",
    "metric_label_order = dict(zip(metric_names, metric_names))\n",
    "\n",
    "# ── 2. draw each metric on its own axis with a single barplot ──────\n",
    "for ax, metric_label in zip(axes, metric_names):\n",
    "    sub = df[df[\"Metric\"] == metric_label]\n",
    "\n",
    "    sns.barplot(\n",
    "        data=sub,\n",
    "        x=\"Group\", y=\"Rate\", hue=\"Condition\",\n",
    "        palette=variant_palette,\n",
    "        errorbar=None,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # overlay SE error bars and numeric labels\n",
    "    for patch, (_, row) in zip(ax.patches, sub.iterrows()):\n",
    "        x = patch.get_x() + patch.get_width() / 2\n",
    "        y = patch.get_height()\n",
    "        ax.errorbar(x, y, yerr=row.SE, capsize=5, ecolor=\"black\")\n",
    "        ax.text(x, y + 0.03, f\"{y:.3f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    # cosmetic tweaks\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Rate\" if ax is axes[0] else \"\")\n",
    "    ax.set_xticks([0, 0.76])\n",
    "    ax.set_xticklabels([\"AOKVQA\", \"VizWiz\"])\n",
    "    ax.set_title(metric_label, fontsize=14)\n",
    "\n",
    "    # drop per-subplot legends\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "# ── 3. add a single, cleaned-up legend for all subplots ────────────\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "rename_and_filter_labels = {\n",
    "    \"Control\": \"Show Explanation Only\",\n",
    "    \"VF (Numeric)\": \"Visual Fidelity\",\n",
    "    \"CONTR (Numeric)\": \"Contrastiveness\",\n",
    "    \"Show Both Metrics (Numeric)\": \"Show Both VF and CONTR\",\n",
    "    \"Prod(VF, CONTR)\": r\"VF$\\times$CONTR\",\n",
    "}\n",
    "handles = [h for h, l in zip(handles, labels) if l in rename_and_filter_labels]\n",
    "labels  = [rename_and_filter_labels[l] for l in labels if l in rename_and_filter_labels]\n",
    "\n",
    "fig.legend(handles, labels,\n",
    "           loc=\"upper center\",\n",
    "           ncol=len(labels),\n",
    "           frameon=False,\n",
    "           bbox_to_anchor=(0.5, 1.06))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_colors = {\n",
    "    'vf': sns.color_palette(\"tab10\")[0], # '#4C72B0', # blue\n",
    "    'contr': sns.color_palette(\"tab10\")[3], #'#DD8452', # orange\n",
    "    'showbothmetrics': sns.color_palette(\"tab10\")[4], #'#55A868', # green\n",
    "    'prodmetric': sns.color_palette(\"tab10\")[2], #'#C44E52', # red\n",
    "    'other': 'lightgray'\n",
    "}\n",
    "\n",
    "# use in matplotlib when we want to plot the seaborn deep palette colors\n",
    "seaborn_style_group_colors = {\n",
    "    'vf': '#3274A1', \n",
    "    'contr': '#C03D3E',\n",
    "    'showbothmetrics': '#9372B2',\n",
    "    'prodmetric': '#3B923A',\n",
    "    'other': '#C0C0C0',\n",
    "}\n",
    "# Display the colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "for i, (group, color) in enumerate(seaborn_style_group_colors.items()):\n",
    "    # Draw a rectangle with the group's color\n",
    "    rect = patches.Rectangle((i, 0), 1, 1, facecolor=color)\n",
    "    ax.add_patch(rect)\n",
    "    # Compute brightness to decide on text color\n",
    "    brightness = sum(mcolors.to_rgb(color)) / 3\n",
    "    text_color = 'white' if brightness < 0.5 else 'black'\n",
    "    ax.text(i + 0.5, 0.5, group, ha='center', va='center', color=text_color, fontsize=12)\n",
    "\n",
    "ax.set_xlim(0, len(seaborn_style_group_colors))\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Seaborn Style Group Colors\", fontsize=14, pad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# --- assume summary_df is already in your namespace ---\n",
    "\n",
    "# 1) re‑use your df_plot construction, then add a “Group” column:\n",
    "interfaces = [\n",
    "    'prod', 'prod_as_vf', 'prod_as_contr',\n",
    "    'vf', 'vf_as_prod',\n",
    "    'contr', 'contr_as_prod'\n",
    "]\n",
    "labels = [\n",
    "    'PROD', 'PROD shown as VF', 'PROD shown as CONTR',\n",
    "    'VF',   'VF shown as PROD',\n",
    "    'CONTR','CONTR shown as PROD'\n",
    "]\n",
    "# build a little df\n",
    "rows = []\n",
    "for iface, label in zip(interfaces, labels):\n",
    "    idx = (\n",
    "        \"prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0\"\n",
    "        if iface == 'prod'\n",
    "        else f\"{iface}_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0\"\n",
    "    )\n",
    "    rows.append({\n",
    "        'Interface':      label,\n",
    "        'NotUnsureAcc':   summary_df.loc[idx, 'NotUnsureAcc'],\n",
    "        'NotUnsureAcc SE':summary_df.loc[idx, 'NotUnsureAcc SE'],\n",
    "        'False Acp Rate': summary_df.loc[idx, 'False Acp Rate'],\n",
    "        'False Acp SE':   summary_df.loc[idx, 'FalseAcp SE'],\n",
    "    })\n",
    "df_plot = pd.DataFrame(rows)\n",
    "\n",
    "# map each row into one of three groups:\n",
    "group_map = {\n",
    "    'PROD': 'prod', 'PROD shown as VF':'prod', 'PROD shown as CONTR':'prod',\n",
    "    'VF':   'vf',   'VF shown as PROD':'vf',\n",
    "    'CONTR':'contr','CONTR shown as PROD':'contr'\n",
    "}\n",
    "df_plot['Group'] = df_plot['Interface'].map(group_map)\n",
    "\n",
    "# define group order and human labels\n",
    "group_order = ['prod', 'vf', 'contr']\n",
    "group_labels = {'prod':'Prod', 'vf':'VF', 'contr':'Contr'}\n",
    "\n",
    "# 2) define a hatch for each Interface‐type\n",
    "# dots for \"shown as just confidence\"\n",
    "# slashes for \"shown as VF\"\n",
    "# crosses for \"shown as CONTR\"\n",
    "hatch_map = {\n",
    "    'PROD':             '..',\n",
    "    'PROD shown as VF': '//',\n",
    "    'PROD shown as CONTR':'x',\n",
    "    'VF':               '//',\n",
    "    'VF shown as PROD': '..',\n",
    "    'CONTR':            'x',\n",
    "    'CONTR shown as PROD':'..',\n",
    "}\n",
    "\n",
    "pattern_legend = {\n",
    "    '/':  'shown as VF',\n",
    "    'x':  'shown as CONTR',\n",
    "    '.':  'shown as simple confidence'\n",
    "}\n",
    "\n",
    "# 2) compute x positions by hand\n",
    "bar_w       = 0.15\n",
    "group_gap   = 0.15   # space between groups\n",
    "x_positions = []\n",
    "xticks      = []\n",
    "x_cursor    = 0\n",
    "\n",
    "for grp in group_order:\n",
    "    sub = df_plot[df_plot['Group']==grp]\n",
    "    n = len(sub)\n",
    "    # positions for this group: [x_cursor, x_cursor+bar_w, …]\n",
    "    xs = x_cursor + np.arange(n) * bar_w\n",
    "    x_positions += list(xs)\n",
    "    # center of the group for xtick\n",
    "    xticks.append(x_cursor + (n-1)/2*bar_w)\n",
    "    # advance cursor: end of this group + gap\n",
    "    x_cursor += n*bar_w + group_gap\n",
    "\n",
    "df_plot['x'] = x_positions\n",
    "\n",
    "# # Palettes for each group\n",
    "# group_colors = {\n",
    "#     'vf': '#4C72B0', # blue\n",
    "#     'contr': '#DD8452', # orange\n",
    "#     'showbothmetrics': '#55A868', # green\n",
    "#     'prod': '#C44E52', # red\n",
    "#     'other': 'gray'\n",
    "# }\n",
    "# pick 3 “husl” hues, one per group\n",
    "base_colors = sns.color_palette(\"deep\", 4)  \n",
    "# map in your group order:\n",
    "group_colors = dict(zip(\n",
    "    ['vf','contr', _, 'prod'],\n",
    "    base_colors\n",
    "))\n",
    "\n",
    "\n",
    "# 4) plot\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5), sharey=False)\n",
    "\n",
    "for ax, (metric, errcol, title) in zip(axes, [\n",
    "    ('NotUnsureAcc', 'NotUnsureAcc SE', 'Not Unsure Accuracy'),\n",
    "    ('False Acp Rate','False Acp SE',   'False Acceptance Rate'),\n",
    "]):\n",
    "    # draw each bar in df_plot with its group color\n",
    "    for i,row in df_plot.iterrows():\n",
    "        grp = row['Group']\n",
    "        color = group_colors[grp] or 'gray'\n",
    "        h = hatch_map[row['Interface']]\n",
    "        ax.bar(row['x'], row[metric], width=bar_w, color=color, hatch=h, edgecolor='black')\n",
    "        ax.errorbar(row['x'], row[metric], \n",
    "                    yerr=row[errcol], fmt='none', capsize=4, color='k')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([group_labels[g] for g in group_order])\n",
    "    ax.set_ylim(0, .8)\n",
    "    ax.set_ylabel('Rate' if ax is axes[0] else \"\")\n",
    "\n",
    "# 7) build a combined legend\n",
    "# 7a) color patches for each group\n",
    "group_patches = [\n",
    "    Patch(facecolor=group_colors[grp],\n",
    "          edgecolor='black',\n",
    "          label=group_labels[grp])\n",
    "    for grp in group_order\n",
    "]\n",
    "# 7b) hatch patches for each “presentation”\n",
    "hatch_patches = [\n",
    "    Patch(facecolor='white',\n",
    "          edgecolor='black',\n",
    "          hatch=pat,\n",
    "          label=label)\n",
    "    for pat, label in pattern_legend.items()\n",
    "]\n",
    "# place legend\n",
    "leg = fig.legend(\n",
    "    handles=group_patches + hatch_patches,\n",
    "    loc='upper center',\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    "    title='Colors = Group      Hatches = Presentation'\n",
    ")\n",
    "plt.tight_layout(rect=[0,0,1,0.86])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- assume summary_df is already in your namespace ---\n",
    "\n",
    "# Define interfaces and their labels\n",
    "interfaces = [\n",
    "    'prod', 'prod_as_vf', 'prod_as_contr'\n",
    "]\n",
    "labels = [\n",
    "    'PROD', 'PROD shown as VF', 'PROD shown as CONTR'\n",
    "]\n",
    "\n",
    "# Map each label to a color\n",
    "colors = {\n",
    "    'PROD': sns.color_palette(\"tab10\")[2],\n",
    "    'PROD shown as VF': sns.color_palette(\"tab10\")[0],\n",
    "    'PROD shown as CONTR': sns.color_palette(\"tab10\")[3]\n",
    "}\n",
    "\n",
    "# Define metrics: (column, error column, display title)\n",
    "metrics = [\n",
    "    ('NotUnsureAcc', 'NotUnsureAcc SE', 'Not Unsure Accuracy'),\n",
    "    ('False Acp Rate', 'FalseAcp SE', 'False Acceptance Rate')\n",
    "]\n",
    "\n",
    "# Build DataFrame\n",
    "data = []\n",
    "for iface, label in zip(interfaces, labels):\n",
    "    idx = (\n",
    "        'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0'\n",
    "        if iface == 'prod'\n",
    "        else f\"{iface}_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0\"\n",
    "    )\n",
    "    row = {'Interface': label}\n",
    "    for col, errcol, _ in metrics:\n",
    "        row[col] = summary_df.loc[idx, col]\n",
    "        row[errcol] = summary_df.loc[idx, errcol]\n",
    "    data.append(row)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Spacing parameters\n",
    "bar_width = 0.2\n",
    "group_gap = 0.3\n",
    "n_ifaces = len(df)\n",
    "\n",
    "# Compute x positions for each metric group and xticks\n",
    "x_groups = []\n",
    "xticks = []\n",
    "for m in range(len(metrics)):\n",
    "    base = m * (n_ifaces * bar_width + group_gap)\n",
    "    x_m = base + np.arange(n_ifaces) * bar_width\n",
    "    x_groups.append(x_m)\n",
    "    # center of group for xtick\n",
    "    xticks.append(base + (n_ifaces - 1) / 2 * bar_width)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for m, (col, errcol, title) in enumerate(metrics):\n",
    "    x_m = x_groups[m]\n",
    "    for i, row in df.iterrows():\n",
    "        xi = x_m[i]\n",
    "        ax.bar(xi,\n",
    "               row[col],\n",
    "               width=bar_width,\n",
    "               color=colors[row['Interface']],\n",
    "               edgecolor='black')\n",
    "        ax.errorbar(xi,\n",
    "                    row[col],\n",
    "                    yerr=row[errcol],\n",
    "                    fmt='none',\n",
    "                    capsize=4,\n",
    "                    color='k')\n",
    "# Labels and legend\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels([title for _, _, title in metrics])\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_ylabel('Rate')\n",
    "# Optional x-axis label\n",
    "# ax.set_xlabel('Metric')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legends = ['Message: \"AI Confidence that the explanation is correct.\"', \n",
    "           'Message: \"AI Confidence that the explanation accurately describes the image details\"',\n",
    "           'Message: \"AI Confidence that the explanation rules out the other choices\"']\n",
    "legend_handles = [\n",
    "    Patch(facecolor=colors[label], edgecolor='black', label=text)\n",
    "    for (label, _), text in zip(colors.items(), legends)\n",
    "]\n",
    "ax.legend(handles=legend_handles,\n",
    "          loc='upper center',\n",
    "          ncol=1,\n",
    "          frameon=False,\n",
    "          bbox_to_anchor=(0.5, 1.3)\n",
    "          )\n",
    "          \n",
    "# plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── your data ──\n",
    "methods   = np.array([\"Prod\", \"Avg\", \"Random\", \"VF\", \"Contr\"])\n",
    "ece       = np.array([0.133,  0.227,  0.257,   0.270,  0.237])\n",
    "user_acc  = np.array([70.2,   66.2,    59.9,    61.3,   69.1])\n",
    "over_rev  = np.array([45.3,   52.7,    50.7,    60.0,   50.0])\n",
    "\n",
    "def fit_stats(x, y):\n",
    "    # Fit a line y = m x + b\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    y_pred = m * x + b\n",
    "    # R^2 = 1 - SS_res/SS_tot\n",
    "    ss_res = np.sum((y - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return m, b, r2\n",
    "\n",
    "# compute stats\n",
    "m1, b1, r21 = fit_stats(ece, user_acc)\n",
    "m2, b2, r22 = fit_stats(ece, over_rev)\n",
    "\n",
    "# print formulas\n",
    "print(f\"User Accuracy vs ECE:      y = {m1:.3f} x + {b1:.3f},  R² = {r21:.3f}\")\n",
    "print(f\"Over Reliance vs ECE:      y = {m2:.3f} x + {b2:.3f},  R² = {r22:.3f}\")\n",
    "\n",
    "# ── plotting ──\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "def scatter_with_line(ax, x, y, title, m, b, r2, ylabel):\n",
    "    ax.scatter(x, y, s=70, alpha=0.8)\n",
    "    # best‐fit line\n",
    "    xs = np.linspace(x.min(), x.max(), 100)\n",
    "    ys = m * xs + b\n",
    "    ax.plot(xs, ys, linewidth=2, label=f\"$R^2$={r2:.2f}\")\n",
    "    # annotate points\n",
    "    for xi, yi, label in zip(x, y, methods):\n",
    "        ax.text(xi, yi + (y.max()-y.min())*0.03, label, ha=\"center\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"ECE\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "scatter_with_line(\n",
    "    axes[0], ece, user_acc,\n",
    "    \"User Accuracy vs ECE\",\n",
    "    m1, b1, r21, \"User Accuracy\"\n",
    ")\n",
    "\n",
    "scatter_with_line(\n",
    "    axes[1], ece, over_rev,\n",
    "    \"Over Reliance vs ECE\",\n",
    "    m2, b2, r22, \"Over Reliance\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# def plot_numeric_vs_descriptive(\n",
    "#     metric_name: str,\n",
    "#     metric_se_name: str = None,\n",
    "#     y_lim_max: float = None,\n",
    "#     summary_df=None,\n",
    "#     group_colors=None,\n",
    "#     figsize: tuple = (6, 4),\n",
    "#     n_per_group: int = 300  # <-- how many annotations in each numeric / descriptive arm\n",
    "# ):\n",
    "#     sns.set_theme(style=\"whitegrid\")\n",
    "#     suffixes = {\n",
    "#         'AOKVQA': '_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "#         'VizWiz': '_llava1.5_vizwiz_better_sampled_q10_i10_s0'\n",
    "#     }\n",
    "#     # groups = ['control', 'vf', 'contr', 'showbothmetrics', 'control', 'vf']\n",
    "#     groups_to_labels_dict = {\n",
    "#         'control': 'Show Explanation Only', \n",
    "#         'vf': 'Visual Fidelity', \n",
    "#         'contr': 'Contrastiveness', \n",
    "#         'showbothmetrics': 'Show Both VF and CONTR'\n",
    "#     }\n",
    "    \n",
    "#     n_groups = 0\n",
    "#     bar_width = 0.4\n",
    "#     num_variants = []\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "#     x_so_far = 0\n",
    "    \n",
    "#     # build variant names\n",
    "#     for key, suffix in suffixes.items():\n",
    "#         if key == 'AOKVQA':\n",
    "#             groups = ['vf', 'contr', 'showbothmetrics']\n",
    "#             group_labels = [groups_to_labels_dict[g] for g in groups]\n",
    "#         elif key == 'VizWiz':\n",
    "#             groups = ['vf']\n",
    "#             group_labels = [groups_to_labels_dict[g] for g in groups]\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown key: {key}\")    \n",
    "    \n",
    "#         n_groups += len(groups)\n",
    "#         num_variants += [f\"{g}_numeric{suffix}\" for g in groups]\n",
    "        \n",
    "#         # special-case rename\n",
    "#         num_variants = [\n",
    "#             (f\"showbothmetrics{suffix}\" if name == f\"showbothmetrics_numeric{suffix}\" else name)\n",
    "#             for name in num_variants\n",
    "#         ]\n",
    "#         desc_variants = [f\"{g}_descriptive{suffix}\" for g in groups]\n",
    "#         control_variant = f\"combined12{suffix}\"\n",
    "    \n",
    "#         x_all      = np.arange(x_so_far, x_so_far + len(groups) * 2, 2)\n",
    "#         x_so_far   += len(groups) + 1\n",
    "        \n",
    "#         x_ctrl     = x_all[0]\n",
    "#         x_groups   = x_all[1:]\n",
    "#         all_labels = ['Control'] + group_labels\n",
    "        \n",
    "#         r_ctrl = summary_df.loc[control_variant, metric_name]\n",
    "#         e_ctrl = (summary_df.loc[control_variant, metric_se_name]\n",
    "#                 if metric_se_name else None)\n",
    "        \n",
    "#         for i, grp in enumerate(groups):\n",
    "#             xpos = x_groups[i]\n",
    "#             # rates\n",
    "#             r_num  = summary_df.loc[num_variants[i], metric_name]\n",
    "#             r_desc = summary_df.loc[desc_variants[i], metric_name]\n",
    "#             # errors (or zeros if none)\n",
    "#             e_num  = summary_df.loc[num_variants[i], metric_se_name] if metric_se_name else None\n",
    "#             e_desc = summary_df.loc[desc_variants[i], metric_se_name] if metric_se_name else None\n",
    "\n",
    "#             color = group_colors.get(grp, 'gray')\n",
    "            \n",
    "#             # numeric\n",
    "#             ax.bar(\n",
    "#                 xpos - bar_width/2, r_num,\n",
    "#                 width=bar_width,\n",
    "#                 yerr=e_num,\n",
    "#                 color=color,\n",
    "#                 alpha=1.0,\n",
    "#                 error_kw={'capsize':5, 'elinewidth':1},\n",
    "#                 label='Numeric' if i == 0 else \"\"\n",
    "#             )\n",
    "#             # descriptive\n",
    "#             ax.bar(\n",
    "#                 xpos + bar_width/2, r_desc,\n",
    "#                 width=bar_width,\n",
    "#                 yerr=e_desc,\n",
    "#                 color=color,\n",
    "#                 alpha=0.5,\n",
    "#                 error_kw={'capsize':5, 'elinewidth':1},\n",
    "#                 label='Descriptive' if i == 0 else \"\"\n",
    "#             )\n",
    "#         # control bar\n",
    "#         # plot the single control bar at the far left\n",
    "#         ax.bar(\n",
    "#             x_ctrl, r_ctrl,\n",
    "#             width=bar_width,\n",
    "#             yerr=e_ctrl,\n",
    "#             color='gray',\n",
    "#             alpha=1.0,\n",
    "#             error_kw={'capsize':5, 'elinewidth':1},\n",
    "#             label='_nolegend_'\n",
    "#         )\n",
    "        \n",
    "#         # annotate each bar with its height\n",
    "#         for container in ax.containers:\n",
    "#             if hasattr(container, \"patches\"):\n",
    "#                 ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "    \n",
    "#     # formatting\n",
    "#     ax.set_xticks(x_all)\n",
    "#     ax.set_xticklabels(all_labels)\n",
    "#     ax.set_ylabel(metric_name)\n",
    "#     if y_lim_max is not None:\n",
    "#         ax.set_ylim(0, y_lim_max)\n",
    "#     ax.set_title(f\"{metric_name}: Numeric vs Descriptive\")\n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # --- significance testing ---\n",
    "#     print(\"\\nTwo‐proportion z‐tests (H₀: numeric = descriptive):\")\n",
    "#     for i, grp in enumerate(groups):\n",
    "#         r_num  = summary_df.loc[num_variants[i], metric_name]\n",
    "#         r_desc = summary_df.loc[desc_variants[i], metric_name]\n",
    "#         count_num  = int(round(r_num  * n_per_group))\n",
    "#         count_desc = int(round(r_desc * n_per_group))\n",
    "#         counts = np.array([count_num, count_desc])\n",
    "#         nobs   = np.array([n_per_group, n_per_group])\n",
    "        \n",
    "#         z_stat, p_val = proportions_ztest(counts, nobs)\n",
    "#         print(f\"  {group_labels[i]:<16} Δ={r_num-r_desc: .3f}   z={z_stat: .2f}   p={p_val: .3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "def plot_numeric_vs_descriptive(\n",
    "    metric_name: list[str],\n",
    "    metric_se_name: list[str],\n",
    "    y_lim_max: float = None,\n",
    "    summary_df=None,\n",
    "    group_colors=None,\n",
    "    figsize: tuple = (9.5, 6),\n",
    "    n_per_group: int = 300\n",
    "):\n",
    "    \n",
    "    if summary_df is None:\n",
    "        raise ValueError(\"`summary_df` must be provided.\")\n",
    "    group_colors = group_colors or {}\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    suffixes = {\n",
    "        'AOKVQA': '_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "        'VizWiz': '_llava1.5_vizwiz_better_sampled_q10_i10_s0'\n",
    "    }\n",
    "    dataset_groups = {\n",
    "        'AOKVQA': ['vf', 'contr', 'showbothmetrics'],\n",
    "        'VizWiz': ['vf']\n",
    "    }\n",
    "    groups_to_labels = {\n",
    "        'control':           'Show Explanation Only',\n",
    "        'vf':                'Visual Fidelity',\n",
    "        'contr':             'Contrastiveness',\n",
    "        'showbothmetrics':   'Show Both VF and CONTR'\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    bar_width = 0.15\n",
    "\n",
    "    # two x‐ticks: AOKVQA at 0, VizWiz at 1\n",
    "    cluster_centers = np.arange(len(suffixes))\n",
    "    ax.set_xticks(cluster_centers)\n",
    "    ax.set_xticklabels(list(suffixes.keys()))\n",
    "\n",
    "    # track which legend entries we’ve already created\n",
    "    legended = set()\n",
    "\n",
    "    for idx, (dataset, suffix) in enumerate(suffixes.items()):\n",
    "        groups = dataset_groups[dataset]\n",
    "        center = cluster_centers[idx]\n",
    "\n",
    "        # prepare variant names\n",
    "        num_vars  = []\n",
    "        desc_vars = []\n",
    "        for g in groups:\n",
    "            num_name  = f\"{g}{suffix}\" if g=='showbothmetrics' else f\"{g}_numeric{suffix}\"\n",
    "            desc_name = f\"{g}_descriptive{suffix}\"\n",
    "            num_vars.append(num_name)\n",
    "            desc_vars.append(desc_name)\n",
    "\n",
    "        # compute x‑positions: 1 control + 2 per group, flush‐together\n",
    "        n_bars = 1 + 2*len(groups)\n",
    "        offsets = (np.arange(n_bars) - (n_bars-1)/2) * bar_width\n",
    "        xs = center + offsets\n",
    "\n",
    "        # — control —\n",
    "        ctrl_var = f\"combined12{suffix}\"\n",
    "        r_ctrl   = summary_df.loc[ctrl_var, metric_name]\n",
    "        e_ctrl   = summary_df.loc[ctrl_var, metric_se_name] if metric_se_name else None\n",
    "        if 'control' not in legended:\n",
    "            lbl_ctrl = groups_to_labels['control']\n",
    "            legended.add('control')\n",
    "        else:\n",
    "            lbl_ctrl = \"_nolegend_\"\n",
    "        ax.bar(\n",
    "            xs[0], r_ctrl,\n",
    "            width=bar_width, yerr=e_ctrl,\n",
    "            color=group_colors.get('control','gray'),\n",
    "            alpha=1.0,\n",
    "            label=lbl_ctrl,\n",
    "            error_kw={'capsize':5, 'elinewidth':1}\n",
    "        )\n",
    "\n",
    "        # — numeric + descriptive for each group —\n",
    "        for j, g in enumerate(groups):\n",
    "            # numeric\n",
    "            r_n   = summary_df.loc[num_vars[j], metric_name]\n",
    "            e_n   = summary_df.loc[num_vars[j], metric_se_name] if metric_se_name else None\n",
    "            key_n = (g, 'num')\n",
    "            if key_n not in legended:\n",
    "                lbl_n = f\"{groups_to_labels[g]} Numeric\"\n",
    "                legended.add(key_n)\n",
    "            else:\n",
    "                lbl_n = \"_nolegend_\"\n",
    "            ax.bar(\n",
    "                xs[1+2*j], r_n,\n",
    "                width=bar_width, yerr=e_n,\n",
    "                color=group_colors.get(g,'gray'),\n",
    "                alpha=1.0,\n",
    "                label=lbl_n,\n",
    "                error_kw={'capsize':5, 'elinewidth':1}\n",
    "            )\n",
    "\n",
    "            # descriptive (immediately to the right)\n",
    "            r_d   = summary_df.loc[desc_vars[j], metric_name]\n",
    "            e_d   = summary_df.loc[desc_vars[j], metric_se_name] if metric_se_name else None\n",
    "            key_d = (g, 'desc')\n",
    "            if key_d not in legended:\n",
    "                lbl_d = f\"{groups_to_labels[g]} Descriptive\"\n",
    "                legended.add(key_d)\n",
    "            else:\n",
    "                lbl_d = \"_nolegend_\"\n",
    "            ax.bar(\n",
    "                xs[1+2*j+1], r_d,\n",
    "                width=bar_width, yerr=e_d,\n",
    "                color=group_colors.get(g,'gray'),\n",
    "                alpha=0.5,\n",
    "                label=lbl_d,\n",
    "                error_kw={'capsize':5, 'elinewidth':1}\n",
    "            )\n",
    "\n",
    "        # — significance tests —\n",
    "        print(f\"\\nTwo‐proportion z‐tests for {dataset}:\")\n",
    "        for j, g in enumerate(groups):\n",
    "            r_num  = summary_df.loc[num_vars[j], metric_name]\n",
    "            r_desc = summary_df.loc[desc_vars[j], metric_name]\n",
    "            cnt_n  = int(round(r_num  * n_per_group))\n",
    "            cnt_d  = int(round(r_desc * n_per_group))\n",
    "            z, p   = proportions_ztest(\n",
    "                count=np.array([cnt_n, cnt_d]),\n",
    "                nobs = np.array([n_per_group, n_per_group])\n",
    "            )\n",
    "            delta = r_num - r_desc\n",
    "            print(f\"  {groups_to_labels[g]:<20} Δ={delta: .3f}   z={z: .2f}   p={p: .3f}\")\n",
    "\n",
    "    # final touches\n",
    "    ax.set_ylabel(metric_name)\n",
    "    if y_lim_max is not None:\n",
    "        ax.set_ylim(0, y_lim_max)\n",
    "    ax.set_title(f\"{metric_name}: Numeric vs Descriptive\")\n",
    "    # ax.legend(loc='upper right')\n",
    "    # Make the legend outside\n",
    "    ax.legend(\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.25),\n",
    "        ncol=len(legended)//2,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    # annotate only the bar patches\n",
    "    for container in ax.containers:\n",
    "        if isinstance(container, BarContainer):\n",
    "            ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from matplotlib.container import BarContainer\n",
    "\n",
    "def plot_numeric_vs_descriptive(\n",
    "    metric_names: list[str],\n",
    "    metric_se_names: list[str] | None = None,\n",
    "    y_lim_max: float = None,\n",
    "    summary_df=None,\n",
    "    group_colors=None,\n",
    "    figsize: tuple = (12, 5),\n",
    "    n_per_group: int = 300\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots multiple metrics comparing numeric vs descriptive variants for each group across datasets.\n",
    "\n",
    "    Args:\n",
    "        metric_names: List of column names for the metrics to plot.\n",
    "        metric_se_names: List of column names for standard errors; same length as metric_names or None.\n",
    "        y_lim_max: Optional maximum y-axis limit.\n",
    "        summary_df: DataFrame indexed by variant names, columns include metrics and their SEs.\n",
    "        group_colors: Optional dict mapping group keys to colors.\n",
    "        figsize: Figure size for the entire grid.\n",
    "        n_per_group: Sample size per group for z-test.\n",
    "    \"\"\"\n",
    "    if summary_df is None:\n",
    "        raise ValueError(\"`summary_df` must be provided.\")\n",
    "    if metric_se_names and len(metric_se_names) != len(metric_names):\n",
    "        raise ValueError(\"`metric_se_names` must match length of `metric_names`.\")\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Define datasets and groups\n",
    "    suffixes = {\n",
    "        'AOKVQA': '_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "        'VizWiz': '_llava1.5_vizwiz_better_sampled_q10_i10_s0'\n",
    "    }\n",
    "    dataset_groups = {\n",
    "        'AOKVQA': ['vf', 'contr', 'showbothmetrics'],\n",
    "        'VizWiz': ['vf']\n",
    "    }\n",
    "    groups_to_labels = {\n",
    "        'control':           'Show Explanation Only',\n",
    "        'vf':                'Visual Fidelity',\n",
    "        'contr':             'Contrastiveness',\n",
    "        'showbothmetrics':   'Both VF and Contr.'\n",
    "    }\n",
    "    group_colors = group_colors or {}\n",
    "\n",
    "    # Prepare unified legend entries in desired group order\n",
    "    group_order = ['vf', 'contr', 'showbothmetrics', 'control']\n",
    "    legend_items = []\n",
    "    for g in group_order:\n",
    "        if g == 'control':\n",
    "            legend_items.append((groups_to_labels[g], group_colors.get('control', 'silver'), 1.0))\n",
    "        else:\n",
    "            legend_items.append((f\"{groups_to_labels[g]} Numeric\", group_colors.get(g, 'silver'), 1.0))\n",
    "            legend_items.append((f\"{groups_to_labels[g]} Descriptive\", group_colors.get(g, 'silver'), 0.5))\n",
    "\n",
    "    # Subplot grid setup\n",
    "    n_metrics = len(metric_names)\n",
    "    ncols = int(np.ceil(np.sqrt(n_metrics)))\n",
    "    nrows = int(np.ceil(n_metrics / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        ax = axes[i]\n",
    "        se_col = metric_se_names[i] if metric_se_names else None\n",
    "        bar_width = 0.15\n",
    "        centers = np.arange(len(suffixes))\n",
    "        ax.set_xticks(centers)\n",
    "        ax.set_xticklabels(list(suffixes.keys()))\n",
    "\n",
    "        for idx, (dataset, suffix) in enumerate(suffixes.items()):\n",
    "            groups = dataset_groups[dataset]\n",
    "            center = centers[idx]\n",
    "            # build names\n",
    "            num_vars = [f\"{g if g=='showbothmetrics' else g + '_numeric'}{suffix}\" for g in groups]\n",
    "            desc_vars = [f\"{g}_descriptive{suffix}\" for g in groups]\n",
    "            # positions\n",
    "            n_bars = 1 + 2*len(groups)\n",
    "            offsets = (np.arange(n_bars) - (n_bars-1)/2) * bar_width\n",
    "            xs = center + offsets\n",
    "            # control\n",
    "            ctrl_var = f\"combined12{suffix}\"\n",
    "            r_ctrl = summary_df.loc[ctrl_var, metric]\n",
    "            e_ctrl = summary_df.loc[ctrl_var, se_col] if se_col else None\n",
    "            ax.bar(xs[0], r_ctrl, width=bar_width, yerr=e_ctrl,\n",
    "                   color=group_colors.get('control','silver'), alpha=1.0,\n",
    "                   error_kw={'capsize':5, 'elinewidth':1})\n",
    "            # numeric & descriptive\n",
    "            for j, g in enumerate(groups):\n",
    "                # numeric\n",
    "                r_n = summary_df.loc[num_vars[j], metric]\n",
    "                e_n = summary_df.loc[num_vars[j], se_col] if se_col else None\n",
    "                ax.bar(xs[1+2*j], r_n, width=bar_width, yerr=e_n,\n",
    "                       color=group_colors.get(g,'silver'), alpha=1.0,\n",
    "                       error_kw={'capsize':5, 'elinewidth':1})\n",
    "                # descriptive\n",
    "                r_d = summary_df.loc[desc_vars[j], metric]\n",
    "                e_d = summary_df.loc[desc_vars[j], se_col] if se_col else None\n",
    "                ax.bar(xs[2+2*j], r_d, width=bar_width, yerr=e_d,\n",
    "                       color=group_colors.get(g,'silver'), alpha=0.5,\n",
    "                       error_kw={'capsize':5, 'elinewidth':1})\n",
    "            # stats\n",
    "            print(f\"\\nTwo-proportion z-tests for {dataset}, metric={metric}:\")\n",
    "            for j, g in enumerate(groups):\n",
    "                cnt_n = int(round(summary_df.loc[num_vars[j], metric] * n_per_group))\n",
    "                cnt_d = int(round(summary_df.loc[desc_vars[j], metric] * n_per_group))\n",
    "                z, p = proportions_ztest(\n",
    "                    count=np.array([cnt_n, cnt_d]),\n",
    "                    nobs = np.array([n_per_group, n_per_group])\n",
    "                )\n",
    "                delta = summary_df.loc[num_vars[j], metric] - summary_df.loc[desc_vars[j], metric]\n",
    "                print(f\"  {groups_to_labels[g]:<20} Δ={delta: .3f}   z={z: .2f}   p={p: .3f}\")\n",
    "\n",
    "        ax.set_ylabel(metric)\n",
    "        if y_lim_max is not None:\n",
    "            ax.set_ylim(0, y_lim_max)\n",
    "        ax.set_title(f\"{metric}\")#: Numeric vs Descriptive\")\n",
    "        for container in ax.containers:\n",
    "            if isinstance(container, BarContainer):\n",
    "                ax.bar_label(container, fmt=\"%.3f\", padding=7, fontsize=10,)\n",
    "\n",
    "    # Hide extra axes\n",
    "    for ax in axes[n_metrics:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Draw shared legend with group ordering\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=c, alpha=a) for _,c,a in legend_items]\n",
    "    labels = [l for l,_,_ in legend_items]\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(group_order), frameon=False)\n",
    "    fig.dpi = 300\n",
    "    axes[0].set_title(\"User Accuracy $\\\\uparrow$\")\n",
    "    axes[1].set_title(\"Over-Reliance $\\\\downarrow$\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_numeric_vs_descriptive(\n",
    "#     metric_name='Not Unsure Accept Rate',\n",
    "#     metric_se_name='NotUnsureAccept SE',\n",
    "#     y_lim_max=1,\n",
    "#     summary_df=summary_df,\n",
    "#     group_colors=group_colors\n",
    "# )\n",
    "\n",
    "plot_numeric_vs_descriptive(\n",
    "    metric_names=['NotUnsureAcc', 'False Acp Rate'],\n",
    "    metric_se_names=['NotUnsureAcc SE', 'FalseAcp SE'],\n",
    "    y_lim_max=0.8,\n",
    "    summary_df=summary_df,\n",
    "    group_colors=group_colors\n",
    ")\n",
    "\n",
    "# plot_numeric_vs_descriptive(\n",
    "#     metric_name='False Acp Rate',\n",
    "#     metric_se_name='FalseAcp SE',\n",
    "#     y_lim_max=0.7,\n",
    "#     summary_df=summary_df,\n",
    "#     group_colors=group_colors\n",
    "# )\n",
    "\n",
    "# plot_numeric_vs_descriptive(\n",
    "#     metric_name='False Rej Rate',\n",
    "#     metric_se_name='FalseRej SE',\n",
    "#     y_lim_max=0.3,\n",
    "#     summary_df=summary_df,\n",
    "#     group_colors=group_colors\n",
    "# )\n",
    "# plot_numeric_vs_descriptive(\n",
    "#     metric_name='True Rej Rate',\n",
    "#     metric_se_name='TrueRej SE',\n",
    "#     y_lim_max=0.55,\n",
    "#     summary_df=summary_df,\n",
    "#     group_colors=group_colors\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Settings to compare ---\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'vf_descriptive_qwen2.5_vizwiz_q10_i10_s0',\n",
    "]\n",
    "\n",
    "records = []\n",
    "for setting in settings:\n",
    "    df = df_map[setting]\n",
    "    for idx, row in df.iterrows():\n",
    "        queue_name = \"qwen2.5_vizwiz_q10_i10_s0\" if 'vizwiz' in setting else \"llava1.5_with_image_q20_i10_s0\"\n",
    "        # get the batch ID (e.g. '000' from '000_3users')\n",
    "        batch_id = row['batch'].split('_')[0]\n",
    "        path = f'../web/baked_queues/{queue_name}/{batch_id}.json'\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        # load the questions for this batch\n",
    "        with open(path) as f:\n",
    "            questions = json.load(f)\n",
    "        entry = questions[row['question_i']]\n",
    "\n",
    "        vf   = entry.get('visual_fidelity')\n",
    "        ctr  = entry.get('contrastiveness')\n",
    "        avg  = None if vf is None or ctr is None else (vf + ctr) / 2\n",
    "\n",
    "        # pick which score was shown\n",
    "        if 'avg_vf_contr' in setting:\n",
    "            confidence = avg\n",
    "        elif setting.startswith('vf'):\n",
    "            confidence = vf\n",
    "        elif setting.startswith('contr'):\n",
    "            confidence = ctr\n",
    "        else:\n",
    "            condidence = None\n",
    "        # add to df\n",
    "        df.loc[idx, 'confidence'] = confidence\n",
    "    \n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# mapping from your raw column names to nice labels\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "stage_order = list(stage_map.values())\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    # 1) melt into long form\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id', 'batch', 'question_i', 'ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage',\n",
    "        value_name='Selection'\n",
    "    )\n",
    "    # 2) map raw to pretty\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "\n",
    "    # 3) group + count\n",
    "    grouped = (\n",
    "        df_long\n",
    "        .groupby('Stage')['Selection']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=stage_order, columns=selection_order)\n",
    "    )\n",
    "\n",
    "    # 4) plot stacked bar\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    x = np.arange(len(stage_order))\n",
    "    bottom = np.zeros(len(stage_order))\n",
    "\n",
    "    for sel, color in zip(selection_order, colors):\n",
    "        counts = grouped[sel].values\n",
    "        legend_label = f\"Correct\" if sel == 0 else f\"Incorrect\" if sel == 1 else \"Unsure\"\n",
    "        bars = ax.bar(x, counts, bottom=bottom,\n",
    "                      label=legend_label,\n",
    "                      color=color, edgecolor='black')\n",
    "        # annotate counts\n",
    "        for idx, bar in enumerate(bars):\n",
    "            h = bar.get_height()\n",
    "            if h:\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width()/2,\n",
    "                    bottom[idx] + h/2,\n",
    "                    f\"{int(h)}\",\n",
    "                    ha='center', va='center'\n",
    "                )\n",
    "        bottom += counts\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stage_order, fontsize=12, rotation=30)\n",
    "    ax.set_xlabel(\"Stage\", fontsize=14)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax.set_title(f\"Selection Counts by Stage ({setting.split('_llava')[0].split('_vizwiz')[0]})\",\n",
    "                 fontsize=16)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# mapping from your raw column names to nice labels\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "stage_order = list(stage_map.values())\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    # 1) melt into long form\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id', 'batch', 'question_i', 'ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage',\n",
    "        value_name='Selection'\n",
    "    )\n",
    "    # 2) map raw to pretty\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "\n",
    "    # 3) compute u_i\n",
    "    #   u_i = 0 if Selection == 2\n",
    "    #   u_i = 1 if (sel=1 & gt=0) or (sel=0 & gt=1)\n",
    "    #   u_i = -1 otherwise (i.e. sel matches gt)\n",
    "    df_long['u_i'] = np.where(\n",
    "        df_long['Selection'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df_long['Selection'] == 1) & (df_long['ground_truth'] == 1)) |\n",
    "            ((df_long['Selection'] == 0) & (df_long['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # inspect the new column\n",
    "    print(f\"\\n—— {setting} ——\")\n",
    "    # randomly sample 5 rows\n",
    "    print(df_long[['Stage', 'Selection', 'ground_truth', 'u_i']].sample(5))\n",
    "\n",
    "    # now group & plot u_i\n",
    "    u_order  = [1, -1, 0]                     # mismatch, unsure, match\n",
    "    u_colors = ['tab:green', 'tab:red', 'tab:orange',]\n",
    "    u_labels = ['Match (+1)', 'Mismatch (-1)', 'Unsure (0)',]\n",
    "\n",
    "    grouped = (\n",
    "        df_long\n",
    "        .groupby('Stage')['u_i']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=stage_order, columns=u_order)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    x = np.arange(len(stage_order))\n",
    "    bottom = np.zeros(len(stage_order))\n",
    "\n",
    "    for u, color, label in zip(u_order, u_colors, u_labels):\n",
    "        counts = grouped[u].values\n",
    "        bars = ax.bar(x, counts, bottom=bottom, label=label,\n",
    "                    color=color, edgecolor='black')\n",
    "        for idx, bar in enumerate(bars):\n",
    "            h = bar.get_height()\n",
    "            if h:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                        bottom[idx] + h/2,\n",
    "                        f\"{int(h)}\",\n",
    "                        ha='center', va='center')\n",
    "        bottom += counts\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stage_order, fontsize=12, rotation=30)\n",
    "    ax.set_xlabel(\"Stage\", fontsize=14)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax.set_title(f\"Answer Correctness by Stage ({setting.split('_llava')[0].split('_vizwiz')[0]})\",\n",
    "                fontsize=16)\n",
    "    ax.legend(fontsize=12, title=\"Correctness\", loc='upper left')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# --- your existing mappings & lists ---\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "labels = {\n",
    "    0: 'Correct',\n",
    "    1: 'Incorrect',\n",
    "    2: 'Unsure'\n",
    "}\n",
    "applied_settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "applied_setting_names = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF Numeric',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0': 'CONTR Numeric',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Average VF/CONTR',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0': 'Both Metrics',\n",
    "}\n",
    "\n",
    "# 1) Melt & compute u_i for all settings (same as before) …\n",
    "all_dfs = []\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in applied_settings:\n",
    "        continue\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id','batch','question_i','ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage', value_name='Selection'\n",
    "    )\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "    df_long['Setting'] = setting\n",
    "\n",
    "    all_dfs.append(df_long)\n",
    "big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# 2a) Overall Stage 2 (“With Explanation”)\n",
    "agg2 = (big.loc[big['Stage']=='With Explanation','Selection']\n",
    "          .value_counts()\n",
    "          .reindex(selection_order, fill_value=0))\n",
    "agg2_prop = agg2 / agg2.sum()\n",
    "\n",
    "# 2b) Stage 3 (“With Explanation + Quality”) per setting\n",
    "stage3 = big[big['Stage']=='With Explanation + Quality']\n",
    "by_set = (stage3.groupby('Setting')['Selection']\n",
    "               .value_counts()\n",
    "               .unstack(fill_value=0)\n",
    "               .reindex(columns=selection_order, fill_value=0))\n",
    "by_set_prop = by_set.div(by_set.sum(axis=1), axis=0)\n",
    "\n",
    "# 3) Build combined_prop in the exact order: Overall, then each applied_setting\n",
    "rows = [agg2_prop] + [by_set_prop.loc[s] for s in applied_settings]\n",
    "index = ['Overall'] + applied_settings\n",
    "combined_prop = pd.DataFrame(rows, index=index, columns=selection_order)\n",
    "\n",
    "# 4) Plot them with tighter bars\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x = np.arange(len(combined_prop))\n",
    "width = 0.8   \n",
    "bottom = np.zeros(len(combined_prop))\n",
    "\n",
    "for u, color in zip(selection_order, colors):\n",
    "    vals = combined_prop[u]\n",
    "    bars = ax.bar(x, vals, width, bottom=bottom,\n",
    "                  color=color, label=labels[u], edgecolor='k')\n",
    "    for xi, (bar, v) in enumerate(zip(bars, vals)):\n",
    "        if v > 0:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bottom[xi] + v/2,\n",
    "                f\"{v:.1%}\",\n",
    "                ha='center', va='center'\n",
    "            )\n",
    "    bottom += vals\n",
    "\n",
    "# 5) Force x‑labels in the same exact order\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    ['With Explanation'] +\n",
    "    [applied_setting_names[s] for s in applied_settings],\n",
    "    rotation=30, ha='right'\n",
    ")\n",
    "\n",
    "ax.margins(x=0.02)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylabel('Percentage', fontsize=14)\n",
    "ax.set_title('User Selection Across Settings', fontsize=16)\n",
    "ax.legend(title='Selection', loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# --- your existing mappings & lists ---\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "selection_order = [1, -1, 0]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "labels = {\n",
    "    1: 'Match (+1)',\n",
    "   -1: 'Mismatch (-1)',\n",
    "    0: 'Unsure (0)'\n",
    "}\n",
    "applied_settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "applied_setting_names = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF Numeric',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0': 'CONTR Numeric',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Average VF/CONTR',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0': 'Both Metrics',\n",
    "}\n",
    "\n",
    "# 1) Melt & compute u_i for all settings (same as before) …\n",
    "all_dfs = []\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in applied_settings:\n",
    "        continue\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id','batch','question_i','ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage', value_name='Selection'\n",
    "    )\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "    df_long['Setting'] = setting\n",
    "    df_long['u_i'] = np.where(\n",
    "        df_long['Selection']==2, 0,\n",
    "        np.where(\n",
    "            ((df_long['Selection']==1)&(df_long['ground_truth']==1)) |\n",
    "            ((df_long['Selection']==0)&(df_long['ground_truth']==0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "    all_dfs.append(df_long)\n",
    "big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# 2a) Overall Stage 2 (“With Explanation”)\n",
    "agg2 = (big.loc[big['Stage']=='With Explanation','u_i']\n",
    "          .value_counts()\n",
    "          .reindex(selection_order, fill_value=0))\n",
    "agg2_prop = agg2 / agg2.sum()\n",
    "\n",
    "# 2b) Stage 3 (“With Explanation + Quality”) per setting\n",
    "stage3 = big[big['Stage']=='With Explanation + Quality']\n",
    "by_set = (stage3.groupby('Setting')['u_i']\n",
    "               .value_counts()\n",
    "               .unstack(fill_value=0)\n",
    "               .reindex(columns=selection_order, fill_value=0))\n",
    "by_set_prop = by_set.div(by_set.sum(axis=1), axis=0)\n",
    "\n",
    "# 3) Build combined_prop in the exact order: Overall, then each applied_setting\n",
    "rows = [agg2_prop] + [by_set_prop.loc[s] for s in applied_settings]\n",
    "index = ['Overall'] + applied_settings\n",
    "combined_prop = pd.DataFrame(rows, index=index, columns=selection_order)\n",
    "\n",
    "# 4) Plot them with tighter bars\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x = np.arange(len(combined_prop))\n",
    "width = 0.8   \n",
    "bottom = np.zeros(len(combined_prop))\n",
    "\n",
    "for u, color in zip(selection_order, colors):\n",
    "    vals = combined_prop[u]\n",
    "    bars = ax.bar(x, vals, width, bottom=bottom,\n",
    "                  color=color, label=labels[u], edgecolor='k')\n",
    "    for xi, (bar, v) in enumerate(zip(bars, vals)):\n",
    "        if v > 0:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bottom[xi] + v/2,\n",
    "                f\"{v:.1%}\",\n",
    "                ha='center', va='center'\n",
    "            )\n",
    "    bottom += vals\n",
    "\n",
    "# 5) Force x‑labels in the same exact order\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    ['With Explanation'] +\n",
    "    [applied_setting_names[s] for s in applied_settings],\n",
    "    rotation=30, ha='right'\n",
    ")\n",
    "\n",
    "ax.margins(x=0.02)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylabel('Percentage', fontsize=14)\n",
    "ax.set_title('Answer Correctness by Setting (in %)', fontsize=16)\n",
    "ax.legend(title='Correctness', loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting, df in df_map.items():\n",
    "    df['utility_2'] = np.where(\n",
    "        df['withexplanation'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df['withexplanation'] == 1) & (df['ground_truth'] == 1)) |\n",
    "            ((df['withexplanation'] == 0) & (df['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "    df['utility_3'] = np.where(\n",
    "        df['withexplanationquality'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df['withexplanationquality'] == 1) & (df['ground_truth'] == 1)) |\n",
    "            ((df['withexplanationquality'] == 0) & (df['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0'].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Compute M2–M4 using utility, for each applied setting\n",
    "all_scores = []\n",
    "for setting in applied_settings:\n",
    "    df = df_map[setting]\n",
    "    \n",
    "    # M2: Positive conversion rate\n",
    "    #    P(u3 = +1 | u2 = 0, u3 != 0)\n",
    "    mask2 = (df['utility_2'] == 0) & (df['utility_3'] != 0)\n",
    "    m2 = (df.loc[mask2, 'utility_3'] == 1).sum() / mask2.sum()\n",
    "    \n",
    "    # M3: Error recovery rate\n",
    "    #    P(u3 >= 0 | u2 < 0)\n",
    "    mask3 = df['utility_2'] < 0\n",
    "    m3 = (df.loc[mask3, 'utility_3'] >= 0).sum() / mask3.sum()\n",
    "    \n",
    "    # M4: Correct loss rate\n",
    "    #    P(u3 <= 0 | u2 > 0)\n",
    "    mask4 = df['utility_2'] > 0\n",
    "    m4 = (df.loc[mask4, 'utility_3'] <= 0).sum() / mask4.sum()\n",
    "    \n",
    "    all_scores.append({\n",
    "        'Setting': applied_setting_names[setting],\n",
    "        'Positive Conversion': m2,\n",
    "        'Error Recovery': m3,\n",
    "        'Correct Loss': m4\n",
    "    })\n",
    "\n",
    "# 3) Build DataFrame and plot grouped bar chart\n",
    "df_scores = pd.DataFrame(all_scores).set_index('Setting')\n",
    "\n",
    "ax = df_scores.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "ax.set_xlabel('Setting', fontsize=16)\n",
    "ax.set_ylabel('Rate', fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Correctness Change Measures Across Settings', fontsize=16)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=16)\n",
    "ax.legend(title='Metric', loc='upper right', fontsize=16, title_fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Group by metric on the x‑axis instead of by setting\n",
    "#    i.e. transpose so rows become metrics and columns become settings\n",
    "df_scores_T = df_scores.transpose()\n",
    "\n",
    "ax = df_scores_T.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "ax.set_xlabel('Metric', fontsize=16)\n",
    "ax.set_ylabel('Rate', fontsize=18)\n",
    "ax.set_title('Correctness Change Measures', fontsize=16)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=16)\n",
    "ax.legend(title='Setting', loc='upper right', fontsize=16, title_fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Plot M3 (Error Recovery) for VF Numeric vs VF Descriptive\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "numeric_descriptive_settings_pair = [\n",
    "    ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'vf_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "    ['contr_numeric_llava1.5_with_image_q20_i10_s0', 'contr_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "    ['showbothmetrics_llava1.5_with_image_q20_i10_s0', 'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "]\n",
    "\n",
    "# Define group labels\n",
    "group_labels = ['VF', 'CONTR', 'Both']\n",
    "\n",
    "# Compute M3 for each group\n",
    "m3_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    mask_num = df_num['utility_2'] < 0\n",
    "    mask_des = df_des['utility_2'] < 0\n",
    "    \n",
    "    m3_num = (df_num.loc[mask_num, 'utility_3'] >= 0).sum() / mask_num.sum()\n",
    "    m3_des = (df_des.loc[mask_des, 'utility_3'] >= 0).sum() / mask_des.sum()\n",
    "    \n",
    "    m3_dict[label] = {'Numeric': m3_num, 'Descriptive': m3_des}\n",
    "\n",
    "# Build DataFrame\n",
    "m3_df = pd.DataFrame.from_dict(m3_dict, orient='index')\n",
    "\n",
    "# Plot grouped bars\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "m3_df.plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Tighten spacing\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Error Recovery Rate', fontsize=16)\n",
    "# ax.set_ylim(0, 1)\n",
    "ax.set_title('Error Recovery Rate by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "\n",
    "# Annotate bar values\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric', 'Descriptive']):\n",
    "        v = m3_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Legend\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute M4 for each group\n",
    "m4_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    mask_num = df_num['utility_2'] > 0\n",
    "    mask_des = df_des['utility_2'] > 0\n",
    "    \n",
    "    m4_num = (df_num.loc[mask_num, 'utility_3'] <= 0).sum() / mask_num.sum()\n",
    "    m4_des = (df_des.loc[mask_des, 'utility_3'] <= 0).sum() / mask_des.sum()\n",
    "    \n",
    "    m4_dict[label] = {'Numeric': m4_num, 'Descriptive': m4_des}\n",
    "# Build DataFrame\n",
    "m4_df = pd.DataFrame.from_dict(m4_dict, orient='index')\n",
    "# Plot grouped bars\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "m4_df.plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "# Tighten spacing\n",
    "ax.margins(x=0.02)\n",
    "# Labels and title\n",
    "ax.set_ylabel('Correct Loss Rate', fontsize=16)\n",
    "# ax.set_ylim(0, 1)\n",
    "ax.set_title('Correct Loss Rate by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "# Annotate bar values\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric', 'Descriptive']):\n",
    "        v = m4_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "# Legend\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Compute M5 (Recovery Gap) correctly for each group\n",
    "m5_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    # Mask for error cases (u2 < 0) and for correct cases (u2 > 0)\n",
    "    mask_num_neg = df_num['utility_2'] < 0\n",
    "    mask_num_pos = df_num['utility_2'] > 0\n",
    "    mask_des_neg = df_des['utility_2'] < 0\n",
    "    mask_des_pos = df_des['utility_2'] > 0\n",
    "    \n",
    "    # M3_num on negative mask, M4_num on positive mask\n",
    "    m3_num = (df_num.loc[mask_num_neg, 'utility_3'] >= 0).sum()   / mask_num_neg.sum()\n",
    "    m4_num = (df_num.loc[mask_num_pos, 'utility_3'] <= 0).sum()   / mask_num_pos.sum()\n",
    "    \n",
    "    # same for descriptive\n",
    "    m3_des = (df_des.loc[mask_des_neg, 'utility_3'] >= 0).sum()   / mask_des_neg.sum()\n",
    "    m4_des = (df_des.loc[mask_des_pos, 'utility_3'] <= 0).sum()   / mask_des_pos.sum()\n",
    "    \n",
    "    m5_dict[label] = {\n",
    "        'Numeric': m3_num - m4_num,\n",
    "        'Descriptive': m3_des - m4_des\n",
    "    }\n",
    "\n",
    "# Build and plot as before\n",
    "m5_df = pd.DataFrame.from_dict(m5_dict, orient='index')\n",
    "ax = m5_df.plot(kind='bar', figsize=(8,5), width=0.6, edgecolor='k')\n",
    "ax.margins(x=0.02)\n",
    "ax.set_ylabel('Recovery Gap', fontsize=16)\n",
    "ax.set_title('Recovery Gap by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric','Descriptive']):\n",
    "        v = m5_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\", ha='center', va='bottom', fontsize=10)\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confidence distributions for each setting\n",
    "for setting in settings:\n",
    "    df = df_map[setting]\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.hist(df['confidence'], bins=20, alpha=0.7, label=setting)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Confidence Distribution for {setting}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bin_stats(df, not_wanted_value):\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return pd.Series({'p': 0, 'se': 0, 'n': 0})\n",
    "    k = (df['withexplanationquality'] != not_wanted_value).sum()\n",
    "    p = k / n\n",
    "    # Laplace-smoothed se\n",
    "    p_ = (k + 1) / (n + 2)\n",
    "    se_ = sqrt(p_ * (1 - p_) / (n + 2))\n",
    "    return pd.Series({'p': p, 'se': se_, 'n': n})\n",
    "\n",
    "# colors for each selection\n",
    "color_map = {0: 'C0', 1: 'tab:green', 2: 'tab:orange'}\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in settings:\n",
    "        continue\n",
    "\n",
    "    # choose metric label\n",
    "    if 'avg_vf_contr' in setting:\n",
    "        confidence_metric = 'AVERAGE(VF, CONTR)'\n",
    "    elif setting.startswith('vf'):\n",
    "        confidence_metric = 'VF'\n",
    "    elif setting.startswith('contr'):\n",
    "        confidence_metric = 'CONTR'\n",
    "    else:\n",
    "        confidence_metric = ''\n",
    "\n",
    "    # bin edges\n",
    "    n_bins = 5\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    width = (bin_edges[1] - bin_edges[0]) * 0.9\n",
    "    \n",
    "    df['conf_bin'] = pd.cut(\n",
    "        df['confidence'],\n",
    "        bins=bin_edges,\n",
    "        include_lowest=True,  # makes 0.0 go into the first bin\n",
    "        right=True            # makes 1.0 go into the last bin\n",
    "    )\n",
    "\n",
    "    # make 3 subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        1, 3,\n",
    "        figsize=(12, 4),\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    for ax, sel in zip(axes, [0, 1, 2]):\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        grouped = (\n",
    "            df0\n",
    "            .groupby('conf_bin', observed=False)\n",
    "            .apply(lambda g: bin_stats(g, sel))\n",
    "            .reset_index()\n",
    "        )\n",
    "        # to percentages\n",
    "        grouped['p']  *= 100\n",
    "        grouped['se'] *= 100\n",
    "\n",
    "        ax.bar(\n",
    "            bin_centers,\n",
    "            grouped['p'],\n",
    "            width=width,\n",
    "            yerr=grouped['se'],\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=color_map[sel]\n",
    "        )\n",
    "        # annotate counts on top of each bar\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            ax.text(\n",
    "                x, \n",
    "                height + 1,              # small vertical offset\n",
    "                f'n={int(count)}',\n",
    "                ha='center', \n",
    "                va='bottom',\n",
    "                fontsize='small'\n",
    "            )\n",
    "                \n",
    "        # ---------------------------------------------------------------\n",
    "        # Weighted Linear Regression using WLS (weights = counts, n)\n",
    "        # ---------------------------------------------------------------\n",
    "        x_data = bin_centers\n",
    "        y_data = grouped['p']\n",
    "        weights = grouped['n']\n",
    "        \n",
    "        # Remove potential bins with zero counts, which could be problematic\n",
    "        valid = weights > 0\n",
    "        x_valid = x_data[valid]\n",
    "        y_valid = y_data[valid]\n",
    "        weights_valid = weights[valid]\n",
    "        \n",
    "        # Only regress if we have 2+ points\n",
    "        if len(x_valid) > 1:\n",
    "            X = sm.add_constant(x_valid)\n",
    "            wls_model = sm.WLS(y_valid, X, weights=weights_valid)\n",
    "            results = wls_model.fit()\n",
    "            intercept, slope = results.params\n",
    "            slope_se = results.bse[1]  # standard error of slope\n",
    "            # Create line for plotting\n",
    "            x_line = np.linspace(min(x_valid), max(x_valid), 50)\n",
    "            y_line = slope * x_line + intercept\n",
    "            ax.plot(\n",
    "                x_line, y_line,\n",
    "                color='red', linestyle='--',\n",
    "                label=(f\"y = {slope:.2f}x + {intercept:.2f}\\n\"\n",
    "                    f\"Slope SE = {slope_se:.2f}\")\n",
    "            )\n",
    "\n",
    "            ax.legend(fontsize='small')\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "\n",
    "        ax.set_title(f's₂ == {sel}')\n",
    "        ax.set_xlabel(f'Confidence ({confidence_metric})')\n",
    "        ax.set_xlim(0, 1)\n",
    "\n",
    "\n",
    "        \n",
    "    axes[0].set_ylabel(f'P(s₃ ≠ s₂ ∣ s₂ == {{0,1,2}}) [%]')\n",
    "    axes[0].set_ylim(0, 100)\n",
    "    plt.suptitle(f\"Setting: {setting}\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bin_stats(df, not_wanted_value):\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return pd.Series({'p': 0, 'se': 0, 'n': 0})\n",
    "    k = (df['withexplanationquality'] != not_wanted_value).sum()\n",
    "    p = k / n\n",
    "    # Laplace-smoothed standard error\n",
    "    p_ = (k + 1) / (n + 2)\n",
    "    se_ = sqrt(p_ * (1 - p_) / (n + 2))\n",
    "    return pd.Series({'p': p, 'se': se_, 'n': n})\n",
    "\n",
    "metrics_to_plot = ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'vf_descriptive_llava1.5_with_image_q20_i10_s0']\n",
    "metric_colors = {'VF_numeric': 'C0', 'VF_descriptive': 'C1'}\n",
    "# a simple mapping from your metric‐IDs to pretty legend labels\n",
    "label_map = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF numeric',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0': 'VF descriptive',\n",
    "}\n",
    "\n",
    "n_bins = 5\n",
    "bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "# Loop over the s₂ values – here using withexplanation == 0, 1, 2\n",
    "for ax, sel in zip(axes, [0, 1, 2]):\n",
    "    # For each s₂ subplot, plot both metrics on the same axes\n",
    "    for metric in metrics_to_plot:\n",
    "        # Get the corresponding DataFrame from df_map.\n",
    "        # (For safety, we use a copy since we will add a new column.)\n",
    "        df = df_map[metric].copy()\n",
    "        # Bin the confidence values (you can adjust include_lowest/right as appropriate)\n",
    "        df['conf_bin'] = pd.cut(\n",
    "            df['confidence'],\n",
    "            bins=bin_edges,\n",
    "            include_lowest=True,\n",
    "            right=True\n",
    "        )\n",
    "        \n",
    "        # Select the data for the current s₂ value\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        \n",
    "        # Group by confidence bin and calculate proportion (p), laplace-smoothed se, and count (n)\n",
    "        grouped = (df0.groupby('conf_bin', observed=False)\n",
    "                    .apply(lambda g: bin_stats(g, sel))\n",
    "                    .reset_index())\n",
    "        # (Optional) make sure that the results are ordered by bin\n",
    "        grouped = grouped.sort_values(by='conf_bin')\n",
    "        \n",
    "        # Convert proportions to percentages\n",
    "        grouped['p'] *= 100\n",
    "        grouped['se'] *= 100\n",
    "        \n",
    "        # Plot a line with markers and error bars using errorbar()\n",
    "        ax.errorbar(\n",
    "            bin_centers,\n",
    "            grouped['p'],\n",
    "            yerr=grouped['se'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color=metric_colors.get(metric, None),\n",
    "            capsize=5,\n",
    "            label=label_map[metric]\n",
    "        )\n",
    "        \n",
    "        # annotate counts on top of each bar\n",
    "        height_adjust = -1 if metric == 'vf_numeric_llava1.5_with_image_q20_i10_s0' else 4\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            ax.text(\n",
    "                x + 0.08,              # small horizontal offset\n",
    "                height + height_adjust if height < 99 else height - 10 + height_adjust,              # small vertical offset\n",
    "                f'n={int(count)}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize='small',\n",
    "                color=metric_colors['VF_numeric'] if 'numeric' in metric else metric_colors['VF_descriptive']\n",
    "            )\n",
    "        \n",
    "\n",
    "    title = f\"s{sel}: \"\n",
    "    if sel == 0:\n",
    "        title += \"User thinks AI is Correct\"\n",
    "    elif sel == 1:\n",
    "        title += \"User thinks AI is Incorrect\"\n",
    "    elif sel == 2:\n",
    "        title += \"User is Unsure\"\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_xlim(0, 1)\n",
    "    # Set legend name\n",
    "    # ax.legend(fontsize='small')\n",
    "    \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=3, fontsize='small')\n",
    "axes[0].set_ylabel('Switch Rate at Stage 3 [%]', fontsize=14)\n",
    "axes[0].set_ylim(0, 100)\n",
    "plt.suptitle(\"After seeing explanation...\", y=0.96)\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional color mapping for compared metrics\n",
    "compared_metric_colors = {\n",
    "'vf_numeric_llava1.5_with_image_q20_i10_s0': 'C0',\n",
    "'contr_numeric_llava1.5_with_image_q20_i10_s0': 'C2',\n",
    "'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'C3'\n",
    "}\n",
    "\n",
    "compared_metrics2 = ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'contr_numeric_llava1.5_with_image_q20_i10_s0', 'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0']\n",
    "\n",
    "\n",
    "# Update label_map if needed\n",
    "label_map.update({\n",
    "'contr_numeric_llava1.5_with_image_q20_i10_s0': 'Contr. numeric',\n",
    "'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Avg VF Contr. numeric'\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "LABEL_PAD   = 2      # vertical step (points) you’re willing to move the label\n",
    "MIN_SEP_Y   = 3      # how close two labels can be before we call it a collision\n",
    "MIN_SEP_X   = 0.15   # so labels on different bars at same x don’t count\n",
    "placed_xy   = []     # keeps track of every label we’ve already drawn\n",
    "\n",
    "def find_free_y(x, y0):\n",
    "    \"\"\"\n",
    "    Return the nearest y‑coordinate to y0 that is not 'too close' to any\n",
    "    existing label.  We look up, then down, in LABEL_PAD‑point steps.\n",
    "    \"\"\"\n",
    "    up_y   = y0\n",
    "    down_y = y0\n",
    "    # keep expanding the search ring until we find an empty slot\n",
    "    while True:\n",
    "        # try above first\n",
    "        if not any(abs(up_y   - py) < MIN_SEP_Y and abs(x-px) < MIN_SEP_X for px, py in placed_xy):\n",
    "            placed_xy.append((x, up_y))\n",
    "            return up_y\n",
    "        # then one step below\n",
    "        if not any(abs(down_y - py) < MIN_SEP_Y and abs(x-px) < MIN_SEP_X for px, py in placed_xy):\n",
    "            placed_xy.append((x, down_y))\n",
    "            return down_y\n",
    "        up_y   += LABEL_PAD\n",
    "        down_y -= LABEL_PAD\n",
    "\n",
    "\n",
    "# Loop over the s₂ values – here using withexplanation values 0, 1, 2\n",
    "for ax, sel in zip(axes, [0, 1, 2]):\n",
    "    # First, plot the original metrics\n",
    "    for metric in compared_metrics2:\n",
    "        df = df_map[metric].copy()\n",
    "        df['conf_bin'] = pd.cut(\n",
    "        df['confidence'],\n",
    "        bins=bin_edges,\n",
    "        include_lowest=True,\n",
    "        right=True\n",
    "        )\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        grouped = (df0.groupby('conf_bin', observed=False)\n",
    "            .apply(lambda g: bin_stats(g, sel))\n",
    "            .reset_index())\n",
    "        grouped = grouped.sort_values(by='conf_bin')\n",
    "        grouped['p'] *= 100\n",
    "        grouped['se'] *= 100\n",
    "\n",
    "        valid = grouped['n'] > 0\n",
    "        ax.errorbar(\n",
    "            bin_centers[valid],\n",
    "            grouped.loc[valid, 'p'],\n",
    "            yerr=grouped.loc[valid, 'se'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color=compared_metric_colors.get(metric, None),\n",
    "            capsize=5,\n",
    "            label=label_map[metric]\n",
    "        )\n",
    "        \n",
    "        # annotate counts\n",
    "        # ── 2. inside your loop that writes the labels ─────────────────────────────────\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            if count == 0:\n",
    "                continue\n",
    "            # first decide the nominal y you would have used\n",
    "            desired_y = height\n",
    "            if height > 95:\n",
    "                desired_y = height - 5\n",
    "            elif height < 5:\n",
    "                desired_y = height + 5\n",
    "\n",
    "            \n",
    "            # ask the helper for a collision‑free y position\n",
    "            y_for_label = find_free_y(x + 0.08, desired_y)\n",
    "            \n",
    "            ax.text(\n",
    "                x + 0.08,\n",
    "                y_for_label,\n",
    "                f'n={int(count)}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize='small',\n",
    "                color=compared_metric_colors.get(metric, 'black')\n",
    "            )\n",
    "\n",
    "            \n",
    "    # Set subplot titles and labels\n",
    "    title = f\"s{sel}: \"\n",
    "    if sel == 0:\n",
    "        title += \"User thinks AI is Correct\"\n",
    "    elif sel == 1:\n",
    "        title += \"User thinks AI is Incorrect\"\n",
    "    elif sel == 2:\n",
    "        title += \"User is Unsure\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_xlim(0, 1)\n",
    "    # ax.legend(fontsize='small')\n",
    "    \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=3, fontsize='small')\n",
    "axes[0].set_ylabel('Switch Rate at Stage 3 [%]', fontsize=14)\n",
    "axes[0].set_ylim(0, 100) \n",
    "plt.suptitle(\"After seeing explanation...\") \n",
    "plt.tight_layout(rect=[0, 0, 1, 1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
