{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "METRIC_FUNCTIONS = {\n",
    "    \"VisualFidelity\": lambda vf, cs: vf,\n",
    "    \"Contrastiveness\": lambda vf, cs: cs,\n",
    "    \"VF*Contrastiveness\": lambda vf, cs: vf * cs if vf is not None else None,\n",
    "    \"Min(VF,Contrast)\": lambda vf, cs: np.minimum(vf, cs) if vf is not None else None,\n",
    "    \"Avg(VF,Contrast)\": lambda vf, cs: (vf + cs) / 2 if vf is not None else None,\n",
    "}\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, ax, title):\n",
    "    \"\"\"\n",
    "    Plots a calibration curve (using 10 bins) on the provided axes.\n",
    "    Returns the computed Expected Calibration Error (ECE).\n",
    "    \"\"\"\n",
    "    n_bins = 10\n",
    "    bin_num_positives = []\n",
    "    bin_mean_probs = []\n",
    "    bin_num_instances = []\n",
    "    bin_centers = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_lower = i / n_bins\n",
    "        bin_upper = (i+1) / n_bins if i != n_bins - 1 else 1.01  # capture 1.0 values\n",
    "        bin_indices = [j for j in range(len(y_prob)) if bin_lower <= y_prob[j] < bin_upper]\n",
    "        if len(bin_indices) > 0:\n",
    "            bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "            bin_num_instances.append(len(bin_indices))\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "        bin_centers.append((i+0.5))\n",
    "            \n",
    "    # Calculate ECE\n",
    "    total_instances = np.sum(bin_num_instances)\n",
    "    ece = 0\n",
    "    for pos, mean_prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances):\n",
    "        \n",
    "        ece += count * np.abs(pos - mean_prob)\n",
    "    ece = ece / total_instances if total_instances > 0 else None\n",
    "    \n",
    "    # Prepare DataFrame for plotting\n",
    "    df = pd.DataFrame({\n",
    "        'bin_num_positives': bin_num_positives,\n",
    "        'bin_centers': bin_centers,\n",
    "        'bin_num_instances': bin_num_instances\n",
    "    })\n",
    "    \n",
    "    # Define a fixed normalization: values 0 to 500 map consistently across plots.\n",
    "    norm_obj = mpl.colors.Normalize(vmin=0, vmax=500)\n",
    "    cmap = plt.get_cmap(\"crest\")\n",
    "    # Compute a color for each bar based on the number of instances\n",
    "    colors = [cmap(norm_obj(count)) for count in df[\"bin_num_instances\"]]\n",
    "    \n",
    "    # Create a barplot; note the hue now encodes number of instances\n",
    "    sns.barplot(x='bin_centers', y='bin_num_positives', data=df, ax=ax,\n",
    "                hue='bin_num_instances', edgecolor='black',\n",
    "                linewidth=1.2, width=1, hue_norm=(0,500), palette='crest')\n",
    "    ax.grid(axis='y')\n",
    "    ax.plot([-0.5, n_bins-0.5], [0, 1], \"k--\")\n",
    "    ax.set_xlim(-0.5, n_bins-0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"Metric Score\", fontsize='x-large')\n",
    "    ax.set_ylabel(\"Prediction Accuracy\", fontsize='x-large')\n",
    "    if ece is not None:\n",
    "        ax.set_title(f\"{title}\\nCalibration Error = {ece:.4f}\", fontsize='large')\n",
    "    else:\n",
    "        ax.set_title(title, fontsize='large')\n",
    "    ax.set_xticks(np.arange(-0.5, n_bins+0.5, 1))\n",
    "    ax.set_xticklabels([f\"{i/n_bins:.1f}\" for i in range(n_bins+1)])\n",
    "    ax.legend().remove()\n",
    "    norm = plt.Normalize(0, 500)\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"crest\", norm=norm)\n",
    "    # colorbar\n",
    "    cbar = ax.figure.colorbar(sm, ax=ax, orientation='vertical', pad=0.02)\n",
    "    cbar.set_label(\"Number of Instances\", fontsize='x-large')\n",
    "    return ece\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Computes Expected Calibration Error (ECE) for given true labels and predicted scores.\"\"\"\n",
    "    bin_num_positives = []\n",
    "    bin_mean_probs = []\n",
    "    bin_num_instances = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_lower = i / n_bins\n",
    "        bin_upper = (i + 1) / n_bins if i != n_bins - 1 else 1.01\n",
    "        bin_indices = np.where((y_prob >= bin_lower) & (y_prob < bin_upper))[0]\n",
    "        if len(bin_indices) > 0:\n",
    "            bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "            bin_num_instances.append(len(bin_indices))\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "    total_instances = np.sum(bin_num_instances)\n",
    "    ece = 0\n",
    "    for pos, mean_prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances):\n",
    "        ece += count * np.abs(pos - mean_prob)\n",
    "    return ece / total_instances if total_instances > 0 else None\n",
    "\n",
    "def save_individual_plots(data, dataset_type, model):\n",
    "    \"\"\"\n",
    "    For the given data (DataFrame), plots calibration curves for each metric (VisualFidelity,\n",
    "    Contrastiveness, and their product) and saves them in a folder with structure:\n",
    "    ece_analysis/dataset_type/model/PROPERTY.png\n",
    "    \"\"\"\n",
    "    save_dir = os.path.join(\"ece_analysis\", dataset_type, model)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    y_true = data[\"is_correct\"].astype(float).values\n",
    "    \n",
    "    vf = data[\"visual_fidelity\"].astype(float).values\n",
    "    cs = data[\"contrastiveness_score\"].astype(float).values\n",
    "    \n",
    "    # Generate metrics based on the global dictionary.\n",
    "    metrics = {}\n",
    "    for metric_name, func in METRIC_FUNCTIONS.items():\n",
    "        if vf is not None:\n",
    "            metrics[metric_name] = func(vf, cs)\n",
    "        else:\n",
    "            # If visual fidelity data is missing and the metric depends on it,\n",
    "            # you can set it to None or skip it.\n",
    "            metrics[metric_name] = None\n",
    "    \n",
    "    ece_values = {}\n",
    "    for metric_name, scores in metrics.items():\n",
    "        if scores is None:\n",
    "            print(f\"Skipping {metric_name} for {dataset_type} {model} due to missing data.\")\n",
    "            continue\n",
    "        fig, ax = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "        ece = plot_calibration_curve(y_true, scores, ax, metric_name)\n",
    "        ece_values[metric_name] = ece\n",
    "        # Replace any problematic characters in the filename if needed.\n",
    "        save_path = os.path.join(save_dir, f\"{metric_name.replace('*','x')}.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "        \n",
    "    # Plot additional metrics if they exist\n",
    "    additional_metrics = {\n",
    "        \"Support\": data[\"entail_prob\"].astype(float).values if \"support\" in data.columns else None,\n",
    "        \"informative\": data[\"informative\"].astype(float).values if \"informative\" in data.columns else None,\n",
    "        \"Commonsense_Plausibility\": data[\"commonsense_plausibility\"].astype(float).values if \"commonsense_plausibility\" in data.columns else None\n",
    "    }\n",
    "\n",
    "    for metric_name, values in additional_metrics.items():\n",
    "        if values is not None:\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "            ece = plot_calibration_curve(y_true, values, ax, metric_name)\n",
    "            ece_values[metric_name] = ece\n",
    "            save_path = os.path.join(save_dir, f\"{metric_name}.png\")\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            print(f\"{metric_name} column not available for {dataset_type} {model}.\")\n",
    "\n",
    "    return ece_values\n",
    "\n",
    "# def create_large_combined_plot(results_list):\n",
    "#     \"\"\"\n",
    "#     Creates one large figure (6 rows x 3 columns) for all dataset–model combinations.\n",
    "#     Each row corresponds to one dataset–model pair and each column to one metric:\n",
    "#       - Column 1: VisualFidelity\n",
    "#       - Column 2: Contrastiveness\n",
    "#       - Column 3: VF*Contrastiveness\n",
    "#     The large figure is saved in ece_analysis/.\n",
    "#     \"\"\"\n",
    "#     num_combos = len(results_list)\n",
    "#     metric_names = list(METRIC_FUNCTIONS.keys())\n",
    "#     num_metrics = len(metric_names)\n",
    "    \n",
    "#     fig, axs = plt.subplots(num_combos, num_metrics, figsize=(5*num_metrics, 4*num_combos), dpi=200)\n",
    "    \n",
    "#     # Ensure axs is 2D (in case of only one row)\n",
    "#     if num_combos == 1:\n",
    "#         axs = np.array([axs])\n",
    "    \n",
    "#     # For each dataset-model combination:\n",
    "#     for idx, item in enumerate(results_list):\n",
    "#         dataset_type = item[\"Dataset\"]\n",
    "#         model = item[\"Model\"]\n",
    "#         data = item[\"data\"]\n",
    "#         y_true = data[\"is_correct\"].astype(float).values\n",
    "        \n",
    "#         vf = data[\"visual_fidelity\"].astype(float).values\n",
    "#         cs = data[\"contrastiveness_score\"].astype(float).values\n",
    "        \n",
    "#         # Compute each metric using the functions from the dictionary.\n",
    "#         computed_metrics = {}\n",
    "#         for metric_name, func in METRIC_FUNCTIONS.items():\n",
    "#             if vf is not None:\n",
    "#                 computed_metrics[metric_name] = func(vf, cs)\n",
    "#             else:\n",
    "#                 computed_metrics[metric_name] = None\n",
    "                \n",
    "        \n",
    "#         # Plot each metric in the corresponding column\n",
    "#         for col, metric_name in enumerate(metric_names):\n",
    "#             ax = axs[idx, col]\n",
    "#             scores = computed_metrics[metric_name]\n",
    "#             title = f\"{dataset_type} | {model}\\n{metric_name}\"\n",
    "#             if scores is not None:\n",
    "#                 plot_calibration_curve(y_true, scores, ax, title)\n",
    "#             else:\n",
    "#                 ax.text(0.5, 0.5, f\"{metric_name} data not available\", horizontalalignment='center', verticalalignment='center')\n",
    "#                 ax.set_title(title)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     os.makedirs(\"ece_analysis\", exist_ok=True)\n",
    "#     save_path = os.path.join(\"ece_analysis\", \"large_combined_plot.png\")\n",
    "#     fig.savefig(save_path)\n",
    "#     plt.close(fig)\n",
    "\n",
    "def create_large_combined_plot(results_list):\n",
    "    \"\"\"\n",
    "    Creates a large figure for all dataset–model combinations.\n",
    "    For each combination, we include both the original metrics (from METRIC_FUNCTIONS) and additional ones if available.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    # We’ll first determine the maximum number of metrics available across all items.\n",
    "    for item in results_list:\n",
    "        data = item[\"data\"]\n",
    "        metrics = {}\n",
    "        if \"visual_fidelity\" in data.columns and \"contrastiveness_score\" in data.columns:\n",
    "            vf = data[\"visual_fidelity\"].astype(float).values\n",
    "            cs = data[\"contrastiveness_score\"].astype(float).values\n",
    "            for metric_name, func in METRIC_FUNCTIONS.items():\n",
    "                metrics[metric_name] = func(vf, cs)\n",
    "        # Add additional metrics if available\n",
    "        if \"support\" in data.columns:\n",
    "            metrics[\"Support\"] = data[\"entail_prob\"].astype(float).values\n",
    "        if \"informative\" in data.columns:\n",
    "            metrics[\"Informative\"] = data[\"informative\"].astype(float).values\n",
    "        if \"commonsense_plausibility\" in data.columns:\n",
    "            metrics[\"Commonsense_Plausibility\"] = data[\"commonsense_plausibility\"].astype(float).values\n",
    "        item[\"metrics\"] = metrics  # store for later use\n",
    "        item[\"metric_names\"] = list(metrics.keys())\n",
    "\n",
    "    # Assume that all items have the same set of metrics (or take the union)\n",
    "    all_metric_names = sorted({name for item in results_list for name in item[\"metric_names\"]})\n",
    "    num_metrics = len(all_metric_names)\n",
    "    num_combos = len(results_list)\n",
    "\n",
    "    fig, axs = plt.subplots(num_combos, num_metrics, figsize=(5*num_metrics, 4*num_combos), dpi=200)\n",
    "    if num_combos == 1:\n",
    "        axs = np.array([axs])\n",
    "\n",
    "    for idx, item in enumerate(results_list):\n",
    "        dataset_type = item[\"Dataset\"]\n",
    "        model = item[\"Model\"]\n",
    "        data = item[\"data\"]\n",
    "        y_true = data[\"is_correct\"].astype(float).values\n",
    "        metrics = item[\"metrics\"]\n",
    "        for col, metric_name in enumerate(all_metric_names):\n",
    "            ax = axs[idx, col]\n",
    "            title = f\"{dataset_type} | {model}\\n{metric_name}\"\n",
    "            if metric_name in metrics and metrics[metric_name] is not None:\n",
    "                plot_calibration_curve(y_true, metrics[metric_name], ax, title)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f\"{metric_name} data not available\", horizontalalignment='center', verticalalignment='center')\n",
    "                ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    os.makedirs(\"ece_analysis\", exist_ok=True)\n",
    "    save_path = os.path.join(\"ece_analysis\", \"large_combined_plot.png\")\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    datasets_folder = \"model_outputs\"\n",
    "    # List your dataset identifiers and models here\n",
    "    dataset_types = [\"AOKVQA\", \"VizWiz\"]\n",
    "    model_list = [\"llava-v1.5-7b\", \"qwen2.5-vl-7b-instruct\", \"gpt-4o-2024-05-13\"]\n",
    "    \n",
    "    summary_results = []\n",
    "    combined_results = []\n",
    "    \n",
    "    for dataset_type in dataset_types:\n",
    "        for model in model_list:\n",
    "            csv_path = os.path.join(datasets_folder, dataset_type, f\"{model}.csv\")\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"CSV file not found: {csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            data = pd.read_csv(csv_path)\n",
    "            # Check necessary columns\n",
    "            if \"is_correct\" not in data.columns:\n",
    "                print(f\"'is_correct' column not found in {csv_path}.\")\n",
    "                continue\n",
    "            if \"contrastiveness_score\" not in data.columns:\n",
    "                print(f\"'contrastiveness_score' column not found in {csv_path}.\")\n",
    "                continue\n",
    "            \n",
    "            correctnesses = data[\"is_correct\"].astype(float).values\n",
    "            if \"visual_fidelity\" in data.columns:\n",
    "                visual_fidelities = data[\"visual_fidelity\"].astype(float).values\n",
    "            else:\n",
    "                visual_fidelities = None\n",
    "            contrastiveness_scores = data[\"contrastiveness_score\"].astype(float).values\n",
    "            \n",
    "            \n",
    "            support = data[\"entail_prob\"].astype(float).values if \"support\" in data.columns else None\n",
    "            informative = data[\"informative\"].astype(float).values if \"informative\" in data.columns else None\n",
    "            commonsense_plausibility = data[\"commonsense_plausibility\"].astype(float).values if \"commonsense_plausibility\" in data.columns else None\n",
    "            \n",
    "            # Compute overall accuracy and ECE values using the helper function\n",
    "            accuracy = np.mean(correctnesses)\n",
    "            \n",
    "            ece_visual = compute_ece(correctnesses, visual_fidelities)\n",
    "            ece_product = compute_ece(correctnesses, visual_fidelities * contrastiveness_scores)\n",
    "            ece_contrast = compute_ece(correctnesses, contrastiveness_scores)\n",
    "            \n",
    "            ece_min_vf_contr = compute_ece(correctnesses, np.minimum(visual_fidelities, contrastiveness_scores))\n",
    "            ece_avg_vf_contr = compute_ece(correctnesses, (visual_fidelities + contrastiveness_scores) / 2)\n",
    "            \n",
    "            ece_support = compute_ece(correctnesses, support) if support is not None else None\n",
    "            ece_informative = compute_ece(correctnesses, informative) if informative is not None else None\n",
    "            ece_commonsense_plausibility = compute_ece(correctnesses, commonsense_plausibility) if commonsense_plausibility is not None else None\n",
    "            \n",
    "            summary_results.append({\n",
    "                \"Dataset\": dataset_type,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"ECE VisualFidelity\": ece_visual,\n",
    "                \"ECE Contrastiveness\": ece_contrast,\n",
    "                \"ECE VF*Contrastiveness\": ece_product,\n",
    "                \"ECE Min(VF,Contrast)\": ece_min_vf_contr,\n",
    "                \"ECE Avg(VF,Contrast)\": ece_avg_vf_contr,\n",
    "                \"ECE Support\": ece_support,\n",
    "                \"ECE informative\": ece_informative,\n",
    "                \"ECE Commonsense_Plausibility\": ece_commonsense_plausibility\n",
    "            })\n",
    "            \n",
    "            # Save individual calibration plots for this dataset-model combination\n",
    "            save_individual_plots(data, dataset_type, model)\n",
    "            \n",
    "            # Save info for the combined large plot\n",
    "            combined_results.append({\n",
    "                \"Dataset\": dataset_type,\n",
    "                \"Model\": model,\n",
    "                \"data\": data\n",
    "            })\n",
    "    \n",
    "    # Print summary table\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    print(summary_df)\n",
    "    summary_df.to_csv(\"ece_analysis/summary_results.csv\", index=False)\n",
    "    \n",
    "    # Create and save the large combined plot (6 rows x 3 columns)\n",
    "    if combined_results:\n",
    "        create_large_combined_plot(combined_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "def collect_human_study_ids(folder: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    Reads every *.json file in `folder` (expects 000.json … 009.json) and\n",
    "    returns the set of `question_id` strings found inside.\n",
    "    \"\"\"\n",
    "    ids = set()\n",
    "    for fp in sorted(glob.glob(os.path.join(folder, \"*.json\"))):\n",
    "        with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            for entry in json.load(f):\n",
    "                ids.add(str(entry[\"question_id\"]))        # keep as str\n",
    "    return ids\n",
    "\n",
    "\n",
    "def compute_subset_ece(csv_path: str, id_subset: set[str],\n",
    "                       model_name: str, dataset_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads `csv_path`, filters rows whose index is in `id_subset`,\n",
    "    then calculates all ECE variants defined in METRIC_FUNCTIONS.\n",
    "    Returns a dict {metric_name: ece_value}.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    df = pd.read_csv(csv_path, dtype={\"index\": str})   # keep ids as str\n",
    "    df = df[df[\"index\"].isin(id_subset)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No rows from {csv_path} match the provided ids.\")\n",
    "    \n",
    "    print(f\"Loaded {len(df)} rows from {csv_path} for model {model_name}.\")\n",
    "\n",
    "    y_true = df[\"is_correct\"].astype(float).values\n",
    "    vf     = df[\"visual_fidelity\"].astype(float).values\n",
    "    cs     = df[\"contrastiveness_score\"].astype(float).values\n",
    "\n",
    "    results = {}\n",
    "    results[\"Accuracy\"] = float(np.mean(y_true))\n",
    "    for name, fn in METRIC_FUNCTIONS.items():\n",
    "        scores = fn(vf, cs)\n",
    "        results[name] = compute_ece(y_true, scores)\n",
    "\n",
    "    # optional extra metrics, if you stored them\n",
    "    if \"entail_prob\" in df.columns:\n",
    "        results[\"Support\"] = compute_ece(y_true, df[\"entail_prob\"].values)\n",
    "    if \"informative\" in df.columns:\n",
    "        results[\"Informative\"] = compute_ece(y_true, df[\"informative\"].values)\n",
    "    if \"commonsense_plausibility\" in df.columns:\n",
    "        results[\"Commonsense_Plausibility\"] = compute_ece(\n",
    "            y_true, df[\"commonsense_plausibility\"].values\n",
    "        )\n",
    "\n",
    "    # === save per-metric calibration plots just like before ===\n",
    "    save_individual_plots(df, f\"{dataset_name}_HUMAN\", model_name)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main_human_study():\n",
    "    # ---------- 1. collect the two id sets ----------\n",
    "    llava_ids = collect_human_study_ids(\n",
    "        \"human_study_questions/llava1.5_with_image_q20_i10_s0\"\n",
    "    )\n",
    "    qwen_ids = collect_human_study_ids(\n",
    "        \"human_study_questions/qwen2.5_vizwiz_q10_i10_s0\"\n",
    "    )\n",
    "\n",
    "    # ---------- 2. compute ECE on each slice ----------\n",
    "    llava_res = compute_subset_ece(\n",
    "        \"model_outputs/AOKVQA/llava-v1.5-7b.csv\",\n",
    "        llava_ids,\n",
    "        model_name=\"llava-v1.5-7b\",\n",
    "        dataset_name=\"AOKVQA\"\n",
    "    )\n",
    "    qwen_res = compute_subset_ece(\n",
    "        \"model_outputs/VizWiz/qwen2.5-vl-7b-instruct.csv\",\n",
    "        qwen_ids,\n",
    "        model_name=\"qwen2.5-vl-7b-instruct\",\n",
    "        dataset_name=\"VizWiz\"\n",
    "    )\n",
    "\n",
    "    # ---------- 3. print / save a summary ----------\n",
    "    summary = (\n",
    "        pd.DataFrame([{\"Model\": \"llava-v1.5-7b\", **llava_res},\n",
    "                      {\"Model\": \"qwen2.5-vl-7b-instruct\", **qwen_res}])\n",
    "        .set_index(\"Model\")\n",
    "          # metrics down the rows\n",
    "        .round(4)\n",
    "    )\n",
    "    print(\"\\n=== Human-study ECE (10-bin) ===\")\n",
    "    print(summary)\n",
    "    summary.to_csv(\"ece_analysis/human_study_ECE.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run your existing full-dataset analysis first (optional)\n",
    "    # main()\n",
    "\n",
    "    # then run the human-study slice\n",
    "    main_human_study()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECE for a specific JSON file\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the JSON file\n",
    "file_path = 'human_study_questions/llava-v1.5-7b_aokvqa_sampled.json'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}. Please ensure the JSON file is located at this path.\")\n",
    "else:\n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Prepare fields\n",
    "    df['is_correct'] = df['prediction_is_correct'].astype(float)\n",
    "    df['visual_fidelity'] = df['visual_fidelity'].astype(float)\n",
    "    df['contrastiveness'] = df['contrastiveness'].astype(float)\n",
    "    df['uniform_random_score'] = df['uniform_random_score'].astype(float)\n",
    "\n",
    "    # Define ECE computation\n",
    "    def compute_ece(y_true, y_prob, n_bins=10):\n",
    "        y_true = np.array(y_true)\n",
    "        y_prob = np.array(y_prob)\n",
    "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        ece = 0.0\n",
    "        total = len(y_prob)\n",
    "\n",
    "        for i in range(n_bins):\n",
    "            lower, upper = bin_edges[i], bin_edges[i + 1]\n",
    "            if i == n_bins - 1:\n",
    "                mask = (y_prob >= lower) & (y_prob <= upper)\n",
    "            else:\n",
    "                mask = (y_prob >= lower) & (y_prob < upper)\n",
    "            if mask.sum() > 0:\n",
    "                bin_acc = y_true[mask].mean()\n",
    "                bin_conf = y_prob[mask].mean()\n",
    "                ece += mask.sum() * abs(bin_acc - bin_conf)\n",
    "        return ece / total\n",
    "\n",
    "    # Metric definitions\n",
    "    METRIC_FUNCTIONS = {\n",
    "        \"VisualFidelity\": lambda vf, cs: vf,\n",
    "        \"Contrastiveness\": lambda vf, cs: cs,\n",
    "        \"VF*Contrastiveness\": lambda vf, cs: vf * cs,\n",
    "        \"Min(VF,Contrast)\": lambda vf, cs: np.minimum(vf, cs),\n",
    "        \"Avg(VF,Contrast)\": lambda vf, cs: (vf + cs) / 2\n",
    "    }\n",
    "\n",
    "    # Compute ECE for each metric\n",
    "    results = []\n",
    "    y_true = df['is_correct']\n",
    "    vf = df['visual_fidelity']\n",
    "    cs = df['contrastiveness']\n",
    "    rand = df['uniform_random_score']\n",
    "\n",
    "    for name, func in METRIC_FUNCTIONS.items():\n",
    "        scores = func(vf, cs)\n",
    "        ece = compute_ece(y_true, scores)\n",
    "        results.append({'Metric': name, 'ECE': ece})\n",
    "        \n",
    "    # Compute ECE for random score\n",
    "    ece_random = compute_ece(y_true, rand)\n",
    "    results.append({'Metric': 'Random Score', 'ECE': ece_random})\n",
    "\n",
    "    # Display results\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Calibration helpers\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, ax, title):\n",
    "    n_bins = 10\n",
    "    bin_num_positives = []\n",
    "    bin_mean_probs = []\n",
    "    bin_num_instances = []\n",
    "    bin_centers = []\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lower = i / n_bins\n",
    "        upper = (i + 1) / n_bins if i != n_bins - 1 else 1.01\n",
    "        idx = np.where((y_prob >= lower) & (y_prob < upper))[0]\n",
    "        if len(idx) > 0:\n",
    "            bin_num_positives.append(np.mean(y_true[idx]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[idx]))\n",
    "            bin_num_instances.append(len(idx))\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "        bin_centers.append(i + 0.5)\n",
    "\n",
    "    total_instances = np.sum(bin_num_instances)\n",
    "    ece = sum(count * abs(pos - prob)\n",
    "              for pos, prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances))\n",
    "    ece = ece / total_instances if total_instances > 0 else None\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'bin_centers': bin_centers,\n",
    "        'accuracy': bin_num_positives,\n",
    "        'counts': bin_num_instances\n",
    "    })\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=100)\n",
    "    cmap = plt.get_cmap('crest')\n",
    "    sns.barplot(x='bin_centers', y='accuracy', data=df, ax=ax,\n",
    "                hue='counts', palette='crest', hue_norm=(0,100),\n",
    "                edgecolor='black', linewidth=1.2, width=1)\n",
    "    ax.plot([-0.5, n_bins-0.5], [0, 1], 'k--')\n",
    "    ax.set_xlim(-0.5, n_bins-0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Metric Score')\n",
    "    ax.set_ylabel('Prediction Accuracy')\n",
    "    ax.set_xticks(np.arange(n_bins))\n",
    "    ax.set_xticklabels([f\"{i/n_bins:.1f}\" for i in range(n_bins)], rotation=45)\n",
    "    ax.set_title(f\"{title}\\nECE = {ece:.4f}\" if ece is not None else title)\n",
    "    ax.legend().remove()\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.02).set_label('Number of Instances')\n",
    "    return ece\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    bin_num_positives = []\n",
    "    bin_mean_probs = []\n",
    "    bin_num_instances = []\n",
    "    for i in range(n_bins):\n",
    "        lower = i / n_bins\n",
    "        upper = (i + 1) / n_bins if i != n_bins - 1 else 1.01\n",
    "        idx = np.where((y_prob >= lower) & (y_prob < upper))[0]\n",
    "        if len(idx) > 0:\n",
    "            bin_num_positives.append(np.mean(y_true[idx]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[idx]))\n",
    "            bin_num_instances.append(len(idx))\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "    total = np.sum(bin_num_instances)\n",
    "    ece = sum(count * abs(pos - prob)\n",
    "              for pos, prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances))\n",
    "    return ece / total if total > 0 else None\n",
    "\n",
    "\n",
    "def random_sample_ece_analysis(dataset, csv_path, output_base, n_samples=30, sample_size=100):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    pos_df = data[data.is_correct == 1]\n",
    "    neg_df = data[data.is_correct == 0]\n",
    "    per_class = sample_size // 2\n",
    "    records = []\n",
    "\n",
    "    for i in range(1, n_samples + 1):\n",
    "    # for i in range(30, 31):\n",
    "        rng = np.random.RandomState(i)\n",
    "        pos_samp = pos_df.sample(per_class, random_state=i)\n",
    "        neg_samp = neg_df.sample(per_class, random_state=i)\n",
    "        sample_df = pd.concat([pos_samp, neg_samp]).reset_index(drop=True)\n",
    "\n",
    "        y_true = sample_df['is_correct'].astype(float).values\n",
    "        vf = sample_df['visual_fidelity'].astype(float).values\n",
    "        cs = sample_df['contrastiveness_score'].astype(float).values\n",
    "\n",
    "        metrics = {\n",
    "            'VisualFidelity': vf,\n",
    "            'Contrastiveness': cs,\n",
    "            'VFxContrastiveness': vf * cs,\n",
    "            'Min(VF,Contrast)': np.minimum(vf, cs),\n",
    "            'Avg(VF,Contrast)': (vf + cs) / 2,\n",
    "            'UniformRandom': rng.rand(len(vf)),         # uniform [0,1]\n",
    "            'FlippedProd': 1.0 - (vf * cs),             # 1 − product\n",
    "            'FlippedMin': 1.0 - np.minimum(vf, cs),     # 1 − min\n",
    "            'FlippedAvg': 1.0 - (vf + cs) / 2,          # 1 − avg\n",
    "            'Support': sample_df['entail_prob'].astype(float).values if 'entail_prob' in sample_df else None,\n",
    "            'Informative': sample_df['informative'].astype(float).values if 'informative' in sample_df else None,\n",
    "            'Commonsense_Plausibility': sample_df['commonsense_plausibility'].astype(float).values if 'commonsense_plausibility' in sample_df else None\n",
    "        }\n",
    "\n",
    "        sample_dir = os.path.join(output_base, f'sample_{i}')\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "        rec = {'sample': i}\n",
    "        # Compute, print & save\n",
    "        ece_vf = compute_ece(y_true, vf)\n",
    "        ece_contr = compute_ece(y_true, cs)\n",
    "        ece_avg = compute_ece(y_true, metrics['Avg(VF,Contrast)'])\n",
    "        print(f\"Run {i:02}: Acc={np.mean(y_true):.4f}, ECE VF={ece_vf:.4f}, CR={ece_contr:.4f}, Avg={ece_avg:.4f}\", end=', ')\n",
    "        print(f\"Min={compute_ece(y_true, metrics['Min(VF,Contrast)']):.4f}, PROD={compute_ece(y_true, metrics['VFxContrastiveness']):.4f}\", end=', ')\n",
    "        print(f\"SPT={compute_ece(y_true, metrics['Support']):.4f}, INFO={compute_ece(y_true, metrics['Informative']):.4f}, COMSP={compute_ece(y_true, metrics['Commonsense_Plausibility']):.4f}\")\n",
    "        print(f\"UR={compute_ece(y_true, metrics['UniformRandom']):.4f}, FLIPPROD={compute_ece(y_true, metrics['FlippedProd']):.4f}, FLIPMIN={compute_ece(y_true, metrics['FlippedMin']):.4f}, FLIPAVG={compute_ece(y_true, metrics['FlippedAvg']):.4f}\")\n",
    "\n",
    "        for name, vals in metrics.items():\n",
    "            if vals is None:\n",
    "                continue\n",
    "            ece_val = compute_ece(y_true, vals)\n",
    "            rec[name] = ece_val\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "            plot_calibration_curve(y_true, vals, ax, name)\n",
    "            fig.tight_layout()\n",
    "            out_path = os.path.join(sample_dir, f\"{name.replace('*','x')}.png\")\n",
    "            fig.savefig(out_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # define 20 equal-width bins over [0,1]\n",
    "            bin_edges = np.linspace(0, 1, 21)\n",
    "            \n",
    "            # Distributions\n",
    "            sns.histplot(vals, bins=bin_edges, kde=True)\n",
    "            plt.title(f\"Distribution of {name} scores\")\n",
    "            plt.xlabel(name)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(os.path.join(sample_dir, f\"{name}_distribution.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            plt.figure(figsize=(6, 4), dpi=200)\n",
    "            plt.title(f\"Distribution of {name} scores for correct and incorrect answers\")\n",
    "            sns.histplot(vals[y_true == 1], bins=bin_edges, kde=False, label='Correct', color='tab:green')\n",
    "            sns.histplot(vals[y_true == 0], bins=bin_edges, kde=False, label='Incorrect', color='tab:red')\n",
    "            plt.xlabel(name)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(sample_dir, f\"{name}_correct_incorrect_distribution.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    # Save summary and identify best run\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    if dataset == 'VizWiz':\n",
    "        cols = ['VisualFidelity']\n",
    "    else:    \n",
    "        cols = ['VisualFidelity', 'Contrastiveness', 'VFxContrastiveness']\n",
    "        \n",
    "    summary_df['mean_ece'] = summary_df[cols].mean(axis=1)\n",
    "    best = summary_df.loc[summary_df['mean_ece'].idxmin()]\n",
    "    print(f\"\\nBest run: sample_{int(best['sample'])} with mean ECE among {cols} = {best['mean_ece']:.4f}\\n\")\n",
    "\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    summary_df.to_csv(os.path.join(output_base, 'ece_summary.csv'), index=False)\n",
    "\n",
    "    print(f\"Completed runs. Summary saved to {output_base}/ece_summary.csv\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = 'llava-v1.5-7b'\n",
    "    # dataset = 'AOKVQA'\n",
    "    # model = 'qwen2.5-vl-7b-instruct'\n",
    "    dataset = 'VizWiz'\n",
    "    csv_path = os.path.join('model_outputs', dataset, f'{model}.csv')\n",
    "    out_base = os.path.join('ece_analysis', f'{dataset}_random_sampled', model)\n",
    "    random_sample_ece_analysis(dataset, csv_path, out_base, n_samples=50, sample_size=100)\n",
    "    # model = 'qwen2.5-vl-7b-instruct'\n",
    "    # dataset = 'AOKVQA'\n",
    "    # csv_path = os.path.join('model_outputs', dataset, f'{model}.csv')\n",
    "    # out_base = os.path.join('ece_analysis', f'{dataset}_random_sampled', model)\n",
    "    # random_sample_ece_analysis(csv_path, out_base, n_samples=50, sample_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the summary results CSV file (ensure that main() from your earlier code has been executed)\n",
    "summary_csv_path = \"ece_analysis/summary_results.csv\"\n",
    "summary_df = pd.read_csv(summary_csv_path)\n",
    "\n",
    "# List the columns corresponding to the ECE metrics\n",
    "metric_columns = [\n",
    "    \"ECE VisualFidelity\", \n",
    "    \"ECE Contrastiveness\", \n",
    "    \"ECE VF*Contrastiveness\", \n",
    "    \"ECE Min(VF,Contrast)\", \n",
    "    \"ECE Avg(VF,Contrast)\", \n",
    "    \"ECE Support\", \n",
    "    \"ECE informative\", \n",
    "    \"ECE Commonsense_Plausibility\"\n",
    "]\n",
    "\n",
    "# Convert metric columns to numeric values in case of non-numeric entries\n",
    "for col in metric_columns:\n",
    "    summary_df[col] = pd.to_numeric(summary_df[col], errors='coerce')\n",
    "\n",
    "# Reshape the DataFrame to long-format\n",
    "melted_df = summary_df.melt(id_vars=[\"Dataset\", \"Model\"], \n",
    "                            value_vars=metric_columns,\n",
    "                            var_name=\"Metric\", \n",
    "                            value_name=\"ECE\")\n",
    "# Remove any rows with missing ECE values\n",
    "melted_df = melted_df.dropna(subset=[\"ECE\"])\n",
    "\n",
    "# Create a KDE plot for each metric's ECE distribution on the same figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = melted_df[\"Metric\"].unique()\n",
    "for metric in metrics:\n",
    "    subset = melted_df[melted_df[\"Metric\"] == metric]\n",
    "    sns.kdeplot(data=subset, x=\"ECE\", label=metric, fill=True, common_norm=False)\n",
    "    \n",
    "plt.title(\"KDE Plot for ECE Distributions for Each Metric\")\n",
    "plt.xlabel(\"Expected Calibration Error (ECE)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ece_analysis/ece_distribution_kde.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joypy\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a Seaborn style for enhanced aesthetics\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n",
    "\n",
    "# Load the summary results CSV file\n",
    "summary_df = pd.read_csv(\"ece_analysis/summary_results.csv\")\n",
    "\n",
    "# Rename the columns immediately\n",
    "summary_df.rename(columns={\n",
    "    \"ECE VisualFidelity\": \"VF\",\n",
    "    \"ECE Contrastiveness\": \"CONTR\",\n",
    "    \"ECE Avg(VF,Contrast)\": \"Avg(VF,CONTR)\",\n",
    "    \"ECE Support\": \"SUPT\",\n",
    "    \"ECE informative\": \"INFO\",\n",
    "    \"ECE Commonsense_Plausibility\": \"COMMONSENSE\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Update the metric_columns list based on the new names\n",
    "metric_columns = [\"VF\", \"CONTR\", \"Avg(VF,CONTR)\", \"SUPT\", \"INFO\", \"COMMONSENSE\"]\n",
    "\n",
    "# Ensure the metric columns are numeric\n",
    "for col in metric_columns:\n",
    "    summary_df[col] = pd.to_numeric(summary_df[col], errors='coerce')\n",
    "\n",
    "\n",
    "# Reshape the DataFrame from long to wide format because joypy expects a wide format\n",
    "# We pivot on the metrics so each column is one metric's ECE values.\n",
    "melted_df = summary_df.melt(id_vars=[\"Dataset\", \"Model\"], \n",
    "                             value_vars=metric_columns,\n",
    "                             var_name=\"Metric\", \n",
    "                             value_name=\"ECE\")\n",
    "melted_df = melted_df.dropna(subset=[\"ECE\"])  # remove any missing ECE values\n",
    "\n",
    "# Pivot the data so that each metric becomes a column\n",
    "wide_data = melted_df.pivot_table(index=[\"Dataset\", \"Model\"], columns=\"Metric\", values=\"ECE\")\n",
    "wide_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create the ridgeline plot with joypy\n",
    "# Customize the colormap, overlap, and additional styling parameters to improve aesthetics.\n",
    "fig, axes = joypy.joyplot(\n",
    "    wide_data[metric_columns],  # only include metric columns\n",
    "    colormap=plt.cm.viridis,     # choose a vibrant, appealing colormap\n",
    "    linewidth=1.2,               # slightly thicker lines for clarity\n",
    "    overlap=1,                   # adjust overlap between curves\n",
    "    grid=True,                   # display background grid lines\n",
    "    figsize=(10, 8),             # adjust figure size as needed\n",
    "    kind=\"kde\"                  # plot density curves\n",
    ")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "\n",
    "# Enhance the plot with titles and axis labels.\n",
    "plt.title(\"Plot of ECE Distributions\", fontsize=22, pad=20)\n",
    "plt.xlabel(\"Expected Calibration Error (ECE)\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(\"ece_analysis/ece_distribution_ridgeline_colorful.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_df = pd.read_csv(\"ece_analysis/summary_results.csv\")\n",
    "\n",
    "metric_columns = [\n",
    "    \"ECE VisualFidelity\", \n",
    "    \"ECE Contrastiveness\", \n",
    "    \"ECE VF*Contrastiveness\", \n",
    "    \"ECE Min(VF,Contrast)\", \n",
    "    \"ECE Avg(VF,Contrast)\", \n",
    "    \"ECE Support\", \n",
    "    \"ECE informative\", \n",
    "    \"ECE Commonsense_Plausibility\"\n",
    "]\n",
    "\n",
    "for col in metric_columns:\n",
    "    summary_df[col] = pd.to_numeric(summary_df[col], errors='coerce')\n",
    "melted_df = summary_df.melt(id_vars=[\"Dataset\", \"Model\"], \n",
    "                            value_vars=metric_columns,\n",
    "                            var_name=\"Metric\", \n",
    "                            value_name=\"ECE\")\n",
    "melted_df = melted_df.dropna(subset=[\"ECE\"])\n",
    "\n",
    "# Violin Plot\n",
    "sns.violinplot(data=melted_df, x=\"Metric\", y=\"ECE\", ax=None)\n",
    "plt.title(\"ECE Distribution by Metric and Dataset\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Expected Calibration Error (ECE)\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ece_analysis/ece_distribution_box_violin.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "\n",
    "# def plot_calibration_curve(y_true, y_prob, ax, title):\n",
    "#     \"\"\"\n",
    "#     Plots a calibration curve (using 10 bins) on the provided axes.\n",
    "#     Returns the computed Expected Calibration Error (ECE).\n",
    "#     \"\"\"\n",
    "#     n_bins = 10\n",
    "#     bin_num_positives = []\n",
    "#     bin_mean_probs = []\n",
    "#     bin_num_instances = []\n",
    "#     bin_centers = []\n",
    "    \n",
    "#     for i in range(n_bins):\n",
    "#         bin_lower = i / n_bins\n",
    "#         bin_upper = (i+1) / n_bins if i != n_bins - 1 else 1.01  # capture 1.0 values\n",
    "#         bin_indices = [j for j in range(len(y_prob)) if bin_lower <= y_prob[j] < bin_upper]\n",
    "#         if len(bin_indices) > 0:\n",
    "#             bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "#             bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "#             bin_num_instances.append(len(bin_indices))\n",
    "#         else:\n",
    "#             bin_num_positives.append(0)\n",
    "#             bin_mean_probs.append(0)\n",
    "#             bin_num_instances.append(0)\n",
    "#         bin_centers.append((i+0.5))\n",
    "            \n",
    "#     # Calculate ECE\n",
    "#     total_instances = np.sum(bin_num_instances)\n",
    "#     ece = 0\n",
    "#     for pos, mean_prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances):\n",
    "        \n",
    "#         ece += count * np.abs(pos - mean_prob)\n",
    "#     ece = ece / total_instances if total_instances > 0 else None\n",
    "    \n",
    "#     # Prepare DataFrame for plotting\n",
    "#     df = pd.DataFrame({\n",
    "#         'bin_num_positives': bin_num_positives,\n",
    "#         'bin_centers': bin_centers,\n",
    "#         'bin_num_instances': bin_num_instances\n",
    "#     })\n",
    "    \n",
    "#     # Define a fixed normalization: values 0 to 500 map consistently across plots.\n",
    "#     norm_obj = mpl.colors.Normalize(vmin=0, vmax=500)\n",
    "#     cmap = plt.get_cmap(\"crest\")\n",
    "#     # Compute a color for each bar based on the number of instances\n",
    "#     colors = [cmap(norm_obj(count)) for count in df[\"bin_num_instances\"]]\n",
    "    \n",
    "#     # Create a barplot; note the hue now encodes number of instances\n",
    "#     sns.barplot(x='bin_centers', y='bin_num_positives', data=df, ax=ax,\n",
    "#                 hue='bin_num_instances', edgecolor='black',\n",
    "#                 linewidth=1.2, width=1, hue_norm=(0,500), palette='crest')\n",
    "#     ax.grid(axis='y')\n",
    "#     ax.plot([-0.5, n_bins-0.5], [0, 1], \"k--\")\n",
    "#     ax.set_xlim(-0.5, n_bins-0.5)\n",
    "#     ax.set_ylim(0, 1)\n",
    "#     ax.set_xlabel(\"Metric Score\", fontsize='x-large')\n",
    "#     ax.set_ylabel(\"Prediction Accuracy\", fontsize='x-large')\n",
    "#     if ece is not None:\n",
    "#         ax.set_title(f\"{title}\\nCalibration Error = {ece:.4f}\", fontsize='large')\n",
    "#     else:\n",
    "#         ax.set_title(title, fontsize='large')\n",
    "#     ax.set_xticks(np.arange(-0.5, n_bins+0.5, 1))\n",
    "#     ax.set_xticklabels([f\"{i/n_bins:.1f}\" for i in range(n_bins+1)])\n",
    "#     ax.legend().remove()\n",
    "#     norm = plt.Normalize(0, 500)\n",
    "#     sm = plt.cm.ScalarMappable(cmap=\"crest\", norm=norm)\n",
    "#     # colorbar\n",
    "#     cbar = ax.figure.colorbar(sm, ax=ax, orientation='vertical', pad=0.02)\n",
    "#     cbar.set_label(\"Number of Instances\", fontsize='x-large')\n",
    "#     return ece\n",
    "\n",
    "\n",
    "# def compute_ece(y_true, y_prob, n_bins=10):\n",
    "#     \"\"\"Computes Expected Calibration Error (ECE) for given true labels and predicted scores.\"\"\"\n",
    "#     bin_num_positives = []\n",
    "#     bin_mean_probs = []\n",
    "#     bin_num_instances = []\n",
    "    \n",
    "#     for i in range(n_bins):\n",
    "#         bin_lower = i / n_bins\n",
    "#         bin_upper = (i + 1) / n_bins if i != n_bins - 1 else 1.01\n",
    "#         bin_indices = np.where((y_prob >= bin_lower) & (y_prob < bin_upper))[0]\n",
    "#         if len(bin_indices) > 0:\n",
    "#             bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "#             bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "#             bin_num_instances.append(len(bin_indices))\n",
    "#         else:\n",
    "#             bin_num_positives.append(0)\n",
    "#             bin_mean_probs.append(0)\n",
    "#             bin_num_instances.append(0)\n",
    "#     total_instances = np.sum(bin_num_instances)\n",
    "#     ece = 0\n",
    "#     for pos, mean_prob, count in zip(bin_num_positives, bin_mean_probs, bin_num_instances):\n",
    "#         ece += count * np.abs(pos - mean_prob)\n",
    "#     return ece / total_instances if total_instances > 0 else None\n",
    "\n",
    "# def save_individual_plots(data, dataset_type, model):\n",
    "#     \"\"\"\n",
    "#     For the given data (DataFrame), plots calibration curves for each metric (VisualFidelity,\n",
    "#     Contrastiveness, and their product) and saves them in a folder with structure:\n",
    "#     ece_analysis/dataset_type/model/PROPERTY.png\n",
    "#     \"\"\"\n",
    "#     save_dir = os.path.join(\"ece_analysis\", dataset_type, model)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     y_true = data[\"is_correct\"].astype(float).values\n",
    "#     # Get arrays (if visual_fidelity is missing, handle accordingly)\n",
    "#     support_values = data[\"entail_prob\"].astype(float).values\n",
    "#     if \"visual_fidelity\" in data.columns:\n",
    "#         visual_fidelities = data[\"visual_fidelity\"].astype(float).values\n",
    "#     else:\n",
    "#         visual_fidelities = None\n",
    "#     contrastiveness_scores = data[\"contrastiveness_score\"].astype(float).values\n",
    "    \n",
    "#     metrics = {}\n",
    "#     metrics['Support'] = support_values\n",
    "#     metrics[\"VisualFidelity\"] = visual_fidelities\n",
    "#     metrics[\"Contrastiveness\"] = contrastiveness_scores\n",
    "#     # Only compute product if visual fidelity exists\n",
    "#     metrics[\"VF*Contrastiveness\"] = visual_fidelities * contrastiveness_scores if visual_fidelities is not None else None\n",
    "    \n",
    "#     ece_values = {}\n",
    "#     for metric_name, scores in metrics.items():\n",
    "#         if scores is None:\n",
    "#             print(f\"Skipping {metric_name} for {dataset_type} {model} due to missing data.\")\n",
    "#             continue\n",
    "#         fig, ax = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "#         ece = plot_calibration_curve(y_true, scores, ax, metric_name)\n",
    "#         ece_values[metric_name] = ece\n",
    "#         # Save plot with filename based on property (replace * with x for filename-safety)\n",
    "#         save_path = os.path.join(save_dir, f\"{metric_name.replace('*','x')}.png\")\n",
    "#         fig.tight_layout()\n",
    "#         fig.savefig(save_path)\n",
    "#         plt.close(fig)\n",
    "#     return ece_values\n",
    "\n",
    "# def create_large_combined_plot(results_list):\n",
    "#     \"\"\"\n",
    "#     Creates one large figure (6 rows x 3 columns) for all dataset–model combinations.\n",
    "#     Each row corresponds to one dataset–model pair and each column to one metric:\n",
    "#       - Column 1: VisualFidelity\n",
    "#       - Column 2: Contrastiveness\n",
    "#       - Column 3: VF*Contrastiveness\n",
    "#     The large figure is saved in ece_analysis/.\n",
    "#     \"\"\"\n",
    "#     num_combos = len(results_list)  # e.g., 6 (2 datasets x 3 models)\n",
    "#     num_metrics = 4  # three metrics per combo\n",
    "    \n",
    "#     # Adjust figsize as needed; here, for example, 18 inches wide and 6 inches tall per row\n",
    "#     fig, axs = plt.subplots(num_combos, num_metrics, figsize=(18, 6*num_combos), dpi=200)\n",
    "    \n",
    "#     # Ensure axs is 2D (in case of only one row)\n",
    "#     if num_combos == 1:\n",
    "#         axs = np.array([axs])\n",
    "    \n",
    "#     # For each dataset-model combination:\n",
    "#     for idx, item in enumerate(results_list):\n",
    "#         dataset_type = item[\"Dataset\"]\n",
    "#         model = item[\"Model\"]\n",
    "#         data = item[\"data\"]\n",
    "#         y_true = data[\"is_correct\"].astype(float).values\n",
    "        \n",
    "#         support_values = data[\"entail_prob\"].astype(float).values\n",
    "        \n",
    "        \n",
    "#         if \"visual_fidelity\" in data.columns:\n",
    "#             visual_fidelities = data[\"visual_fidelity\"].astype(float).values\n",
    "#         else:\n",
    "#             visual_fidelities = None\n",
    "#         contrastiveness_scores = data[\"contrastiveness_score\"].astype(float).values\n",
    "        \n",
    "#         metrics = {}\n",
    "#         metrics['Support'] = support_values\n",
    "#         metrics[\"VisualFidelity\"] = visual_fidelities\n",
    "#         metrics[\"Contrastiveness\"] = contrastiveness_scores\n",
    "#         metrics[\"VF*Contrastiveness\"] = visual_fidelities * contrastiveness_scores if visual_fidelities is not None else None\n",
    "        \n",
    "#         # Plot each metric in the corresponding column\n",
    "#         for col, metric_name in enumerate([\"Support\",\"VisualFidelity\", \"Contrastiveness\", \"VF*Contrastiveness\"]):\n",
    "#             ax = axs[idx, col]\n",
    "#             scores = metrics[metric_name]\n",
    "#             title = f\"{dataset_type} | {model}\\n{metric_name}\"\n",
    "#             if scores is not None:\n",
    "#                 plot_calibration_curve(y_true, scores, ax, title)\n",
    "#             else:\n",
    "#                 ax.text(0.5, 0.5, f\"{metric_name} data not available\", horizontalalignment='center', verticalalignment='center')\n",
    "#                 ax.set_title(title)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     os.makedirs(\"ece_analysis\", exist_ok=True)\n",
    "#     save_path = os.path.join(\"ece_analysis\", \"large_combined_plot.png\")\n",
    "#     fig.savefig(save_path)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def main():\n",
    "#     datasets_folder = \"model_outputs\"\n",
    "#     # List your dataset identifiers and models here\n",
    "#     dataset_types = [\"AOKVQA\", \"VizWiz\"]\n",
    "#     model_list = [\"llava-v1.5-7b\", \"qwen2.5-vl-7b-instruct\", \"gpt-4o-2024-05-13\"]\n",
    "    \n",
    "#     summary_results = []\n",
    "#     combined_results = []\n",
    "    \n",
    "#     for dataset_type in dataset_types:\n",
    "#         for model in model_list:\n",
    "#             csv_path = os.path.join(datasets_folder, dataset_type, f\"{model}.csv\")\n",
    "#             if not os.path.exists(csv_path):\n",
    "#                 print(f\"CSV file not found: {csv_path}\")\n",
    "#                 continue\n",
    "            \n",
    "#             data = pd.read_csv(csv_path)\n",
    "#             # Check necessary columns\n",
    "#             if \"is_correct\" not in data.columns:\n",
    "#                 print(f\"'is_correct' column not found in {csv_path}.\")\n",
    "#                 continue\n",
    "#             if \"contrastiveness_score\" not in data.columns:\n",
    "#                 print(f\"'contrastiveness_score' column not found in {csv_path}.\")\n",
    "#                 continue\n",
    "#             if \"entail_prob\" not in data.columns:\n",
    "#                 print(f\"'entail_prob' column not found in {csv_path}.\")\n",
    "#                 continue\n",
    "            \n",
    "#             correctnesses = data[\"is_correct\"].astype(float).values\n",
    "#             support_values = data[\"entail_prob\"].astype(float).values\n",
    "            \n",
    "#             if \"visual_fidelity\" in data.columns:\n",
    "#                 visual_fidelities = data[\"visual_fidelity\"].astype(float).values\n",
    "#             else:\n",
    "#                 visual_fidelities = None\n",
    "#             contrastiveness_scores = data[\"contrastiveness_score\"].astype(float).values\n",
    "            \n",
    "#             # Compute overall accuracy and ECE values using the helper function\n",
    "#             accuracy = np.mean(correctnesses)\n",
    "#             ece_support = compute_ece(correctnesses, support_values)\n",
    "#             if visual_fidelities is not None:\n",
    "#                 ece_visual = compute_ece(correctnesses, visual_fidelities)\n",
    "#                 ece_product = compute_ece(correctnesses, visual_fidelities * contrastiveness_scores)\n",
    "#             else:\n",
    "#                 ece_visual = \"n/a\"\n",
    "#                 ece_product = \"n/a\"\n",
    "#             ece_contrast = compute_ece(correctnesses, contrastiveness_scores)\n",
    "            \n",
    "            \n",
    "#             summary_results.append({\n",
    "#                 \"Dataset\": dataset_type,\n",
    "#                 \"Model\": model,\n",
    "#                 \"Accuracy\": accuracy,\n",
    "#                 \"ECE Support\": ece_support,\n",
    "#                 \"ECE VisualFidelity\": ece_visual,\n",
    "#                 \"ECE Contrastiveness\": ece_contrast,\n",
    "#                 \"ECE VF*Contrastiveness\": ece_product\n",
    "#             })\n",
    "            \n",
    "#             # Save individual calibration plots for this dataset-model combination\n",
    "#             save_individual_plots(data, dataset_type, model)\n",
    "            \n",
    "#             # Save info for the combined large plot\n",
    "#             combined_results.append({\n",
    "#                 \"Dataset\": dataset_type,\n",
    "#                 \"Model\": model,\n",
    "#                 \"data\": data\n",
    "#             })\n",
    "    \n",
    "#     # Print summary table\n",
    "#     summary_df = pd.DataFrame(summary_results)\n",
    "#     print(summary_df)\n",
    "    \n",
    "#     # Create and save the large combined plot (6 rows x 3 columns)\n",
    "#     if combined_results:\n",
    "#         create_large_combined_plot(combined_results)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Calculate classifier calibration\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_filename = \"model_outputs/VizWiz/gpt-4o-2024-05-13.csv\"\n",
    "data = pd.read_csv(data_filename)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_fidelities = data['visual_fidelity']\n",
    "contrastiveness_scores = data['contrastiveness_score']\n",
    "correctnesses = data['is_correct']\n",
    "\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob, ax, title):\n",
    "    bin_num_positives, bin_mean_probs, bin_num_instances, bin_centers = [], [], [], []\n",
    "    for i in range(10):\n",
    "        bin_lower = i/10\n",
    "        bin_upper = (i+1)/10 if i != 9 else 1.01\n",
    "        bin_indices = [j for j in range(len(y_prob)) if bin_lower <= y_prob[j] < bin_upper]\n",
    "        if len(bin_indices) != 0:\n",
    "            bin_num_positives.append(np.mean(y_true[bin_indices]))\n",
    "            bin_mean_probs.append(np.mean(y_prob[bin_indices]))\n",
    "            bin_num_instances.append(len(bin_indices))\n",
    "            bin_centers.append((i+0.5)/10)\n",
    "        else:\n",
    "            bin_num_positives.append(0)\n",
    "            bin_mean_probs.append(0)\n",
    "            bin_num_instances.append(0)\n",
    "            bin_centers.append((i+0.5)/10)\n",
    "\n",
    "    # Calculate expected calibration error\n",
    "    ece = 0\n",
    "    for i in range(len(bin_num_positives)):\n",
    "        ece += bin_num_instances[i] * np.abs(bin_num_positives[i] - bin_mean_probs[i])\n",
    "    ece /= sum(bin_num_instances)\n",
    "\n",
    "    #print(bin_num_positives, bin_mean_probs, bin_num_instances)\n",
    "    df = pd.DataFrame({'bin_num_positives': bin_num_positives, 'bin_centers': bin_centers, 'bin_num_instances': bin_num_instances})\n",
    "    sns.barplot(x='bin_centers', y='bin_num_positives', data=df, ax=ax, hue='bin_num_instances', palette='crest', edgecolor='black', linewidth=1.2, width=1, hue_norm=(0, 200))\n",
    "    ax.grid(axis='y')\n",
    "    ax.plot([-0.5, 9.5], [0, 1], \"k--\")\n",
    "    #ax.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "    ax.set_xlim(-0.5, 9.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"Metric Score\", fontsize='x-large')\n",
    "    ax.set_ylabel(\"Prediction Accuracy\", fontsize='x-large')\n",
    "    ax.set_title(f\"{title}\\nCalibration Error = {ece:.4f}\", fontsize='large')\n",
    "    ax.set_xticks(np.arange(-0.5, 10.5, 1))\n",
    "    ax.set_xticklabels([i/10 for i in range(11)])\n",
    "    \n",
    "    ax.legend().remove()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(18, 4), dpi=200)\n",
    "fig.suptitle(\"Calibration of VisualFidelity and Contrastiveness metrics\", fontsize='xx-large')\n",
    "plot_calibration_curve(correctnesses, visual_fidelities, ax[0], \"VisualFidelity\")\n",
    "plot_calibration_curve(correctnesses, contrastiveness_scores, ax[1], \"Contrastiveness\")\n",
    "plot_calibration_curve(correctnesses, np.array([(x+y)/2 for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[2], \"(VisualFidelity + Contrastiveness)/2\")\n",
    "plot_calibration_curve(correctnesses, np.array([min(x, y) for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[3], \"min(VisualFidelity, Contrastiveness)\")\n",
    "plot_calibration_curve(correctnesses, np.array([x*y for x, y in zip(visual_fidelities, contrastiveness_scores)]), ax[4], \"VisualFidelity * Contrastiveness\")\n",
    "\n",
    "norm = plt.Normalize(0, 500)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"crest\", norm=norm)\n",
    "ax[4].figure.colorbar(sm, ax=ax[4])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
